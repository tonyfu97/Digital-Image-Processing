{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Digital Image Processing with C++ : Study Notes This website includes my study notes for the book Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, and Barra. Please note that this website does not include the book's content, except for some of the code snippets used. If you're interested in more details, you can purchase the book here . I am not affiliated with the authors or the publisher in any way. I simply enjoy the book and want to share my notes with others. While I have some background in image processing, I have attempted to make my notes beginner-friendly. I have included things that I personally found to need more explanation or clarification. I hope you find them useful. Disclaimer All intellectual property rights related to the book, including content, images, and related materials, are owned by the authors and publisher. The information provided on this website is intended for educational purposes and personal use only, and should not be considered a substitute for purchasing the book. Introduction I used to rely on OpenCV for image processing tasks. It's powerful, but getting it set up on a new computer was always a bit of a headache. Plus, it has so many features that it was hard to keep track of everything. Then I found CImg. What's cool about it is that it's all in one header file. That means you can just drop it into your project and get going. This repository will document my journey and the insights I gain along the way. References Primary reference : Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra CImg Library Principles of Digital Image Processing series by Burger & Burge (2009, 2013)","title":"Home"},{"location":"#digital-image-processing-with-c-study-notes","text":"This website includes my study notes for the book Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, and Barra. Please note that this website does not include the book's content, except for some of the code snippets used. If you're interested in more details, you can purchase the book here . I am not affiliated with the authors or the publisher in any way. I simply enjoy the book and want to share my notes with others. While I have some background in image processing, I have attempted to make my notes beginner-friendly. I have included things that I personally found to need more explanation or clarification. I hope you find them useful.","title":"Digital Image Processing with C++ : Study Notes"},{"location":"#disclaimer","text":"All intellectual property rights related to the book, including content, images, and related materials, are owned by the authors and publisher. The information provided on this website is intended for educational purposes and personal use only, and should not be considered a substitute for purchasing the book.","title":"Disclaimer"},{"location":"#introduction","text":"I used to rely on OpenCV for image processing tasks. It's powerful, but getting it set up on a new computer was always a bit of a headache. Plus, it has so many features that it was hard to keep track of everything. Then I found CImg. What's cool about it is that it's all in one header file. That means you can just drop it into your project and get going. This repository will document my journey and the insights I gain along the way.","title":"Introduction"},{"location":"#references","text":"Primary reference : Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra CImg Library Principles of Digital Image Processing series by Burger & Burge (2009, 2013)","title":"References"},{"location":"01_getting_started/","text":"Getting Started with the CImg Library - Learning Reflection Author : Tony Fu Date : August 18, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 2.1 - 2.2 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. Installing XQuartz (X11 on macOS) Yes, I lied. I said you only need to include the CImg.h header file to get started. But that's not entirely true. You also need to install the X11 library, which is used to display images. The X11 library is used to display images. It is an open-source effort to develop a version of the X.Org X Window System that runs on macOS. You can download the latest version of XQuartz here . Follow the instructions to install it on your machine. Because X11 is often installs in a non-standard location (/opt/X11). This means that the compiler and linker may not automatically look in this location when trying to find the X11 libraries and include files. So, we need to modified the ~/.zshrc file (or whatever shell you're using) to add the following lines: # XQuartz (for CImg) export LIBRARY_PATH=/opt/X11/lib:$LIBRARY_PATH export CPATH=/opt/X11/include:$CPATH This effectively includes the X11 library in the compiler's search path. Don't forget to restart the shell or execute source ~/.zshrc to apply the changes to the current shell. 2. Installing the PNG Library If you want to read and save images in .png format, as demonstrated in the upcoming example, you'll need to install the libpng library. You can install it using Homebrew with the following command: brew install libpng You won't need to modify any environment variables for libpng . When you install libpng using Homebrew (or another standard package manager), it places the library and include files in standard locations that the compiler and linker already recognize. 3. First Program To display an image using the CImg library, include the CImg.h header file and use the CImgDisplay class. Here's a simple example first_code.cpp : #define cimg_use_png #include \"CImg.h\" using namespace cimg_library; int main() { CImg<unsigned char> img(\"../images/lighthouse.png\"); img.display(\"Lighthouse\"); return 0; } The CImg class is a template class that represents an image. The template parameter specifies the type of the image pixels. In this case, the image is a color image with unsigned 8-bit integer pixels. The CImg class has a constructor that takes a filename as input and loads the image from the file. The display() method of the CImg class displays the image in a window. The first argument is the title of the window. Here the macro cimg_use_png is used to specify that the libpng library should be used to read and write images in .png format. This gives us a peak into how we can customize the CImg library to suit our needs. To compile the program, you need to link the X11 and png library. Here is the command I used: g++ -o first_code first_code.cpp -lX11 -lpthread -lpng The -lpthread option links the pthread library. The pthread library is used to create threads, which is needed by the CImg library. To run the program, do the following: ./first_code 4. Visual Studio Code Configuration If you're coding inside Visual Studio Code like I am, you might see a red squiggly line under #include \"CImg.h\" , accompanied by the complaint Cannot open source file \"X11/Xlib.h\" (dependency of \"CImg.h\") . Here's how to fix this issue: Open the Command Palette by pressing Cmd+Shift+P on a Mac or Ctrl+Shift+P on Windows/Linux. Type \"C/C++\" in the Command Palette, and then select \"Edit Configurations (JSON)\" from the dropdown list. This action will open the c_cpp_properties.json file directly. Add the following line to the includePath array: \"includePath\": [ \"${workspaceFolder}/**\", \"/opt/X11/include/**\" // Add this line ], Save the file. The red squiggly line should now disappear.","title":"1. Getting Started"},{"location":"01_getting_started/#getting-started-with-the-cimg-library-learning-reflection","text":"Author : Tony Fu Date : August 18, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 2.1 - 2.2 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Getting Started with the CImg Library - Learning Reflection"},{"location":"01_getting_started/#1-installing-xquartz-x11-on-macos","text":"Yes, I lied. I said you only need to include the CImg.h header file to get started. But that's not entirely true. You also need to install the X11 library, which is used to display images. The X11 library is used to display images. It is an open-source effort to develop a version of the X.Org X Window System that runs on macOS. You can download the latest version of XQuartz here . Follow the instructions to install it on your machine. Because X11 is often installs in a non-standard location (/opt/X11). This means that the compiler and linker may not automatically look in this location when trying to find the X11 libraries and include files. So, we need to modified the ~/.zshrc file (or whatever shell you're using) to add the following lines: # XQuartz (for CImg) export LIBRARY_PATH=/opt/X11/lib:$LIBRARY_PATH export CPATH=/opt/X11/include:$CPATH This effectively includes the X11 library in the compiler's search path. Don't forget to restart the shell or execute source ~/.zshrc to apply the changes to the current shell.","title":"1. Installing XQuartz (X11 on macOS)"},{"location":"01_getting_started/#2-installing-the-png-library","text":"If you want to read and save images in .png format, as demonstrated in the upcoming example, you'll need to install the libpng library. You can install it using Homebrew with the following command: brew install libpng You won't need to modify any environment variables for libpng . When you install libpng using Homebrew (or another standard package manager), it places the library and include files in standard locations that the compiler and linker already recognize.","title":"2. Installing the PNG Library"},{"location":"01_getting_started/#3-first-program","text":"To display an image using the CImg library, include the CImg.h header file and use the CImgDisplay class. Here's a simple example first_code.cpp : #define cimg_use_png #include \"CImg.h\" using namespace cimg_library; int main() { CImg<unsigned char> img(\"../images/lighthouse.png\"); img.display(\"Lighthouse\"); return 0; } The CImg class is a template class that represents an image. The template parameter specifies the type of the image pixels. In this case, the image is a color image with unsigned 8-bit integer pixels. The CImg class has a constructor that takes a filename as input and loads the image from the file. The display() method of the CImg class displays the image in a window. The first argument is the title of the window. Here the macro cimg_use_png is used to specify that the libpng library should be used to read and write images in .png format. This gives us a peak into how we can customize the CImg library to suit our needs. To compile the program, you need to link the X11 and png library. Here is the command I used: g++ -o first_code first_code.cpp -lX11 -lpthread -lpng The -lpthread option links the pthread library. The pthread library is used to create threads, which is needed by the CImg library. To run the program, do the following: ./first_code","title":"3. First Program"},{"location":"01_getting_started/#4-visual-studio-code-configuration","text":"If you're coding inside Visual Studio Code like I am, you might see a red squiggly line under #include \"CImg.h\" , accompanied by the complaint Cannot open source file \"X11/Xlib.h\" (dependency of \"CImg.h\") . Here's how to fix this issue: Open the Command Palette by pressing Cmd+Shift+P on a Mac or Ctrl+Shift+P on Windows/Linux. Type \"C/C++\" in the Command Palette, and then select \"Edit Configurations (JSON)\" from the dropdown list. This action will open the c_cpp_properties.json file directly. Add the following line to the includePath array: \"includePath\": [ \"${workspaceFolder}/**\", \"/opt/X11/include/**\" // Add this line ], Save the file. The red squiggly line should now disappear.","title":"4. Visual Studio Code Configuration"},{"location":"02_block_decomposition/","text":"Block Decomposition - Learning Reflection Author : Tony Fu Date : August 18, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : Github Reference : Chapter 2.3 - 2.7 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. CImg Template Class The CImg library is a template-based image manipulation library, and its template argument specifies the pixel type. By default, if you don't specify the template argument, it's instantiated with float . So when you declare an image like this: CImg<> img(\"image.png\"); It is equivalent to: CImg<float> img(\"image.png\"); You can specify a different type if you want, such as unsigned char , int , etc. But if you simply use CImg<> , then it defaults to using float . Below are some commonly used constructors for CImg<T> : Default Constructor : CImg<T>(); This constructs an empty image. Constructor with Dimensions : CImg<T>(const unsigned int width, const unsigned int height, const unsigned int depth = 1, const unsigned int spectrum = 1, const T& value = 0); * `width`: Width of the image. * `height`: Height of the image. * `depth`: Depth of the image (default is 1 for 2D images). * `spectrum`: Number of channels (e.g., 3 for RGB image). * `value`: Initial value for all pixels. Copy Constructor : CImg<T>(const CImg<T>& img); Constructs a copy of the given image img . Constructor from File : CImg<T>(const char* filename); Constructs an image by reading from a file specified by filename . Constructor from Data : CImg<T>(const T* data, const unsigned int width, const unsigned int height, const unsigned int depth = 1, const unsigned int spectrum = 1, const bool shared = false); * `data`: Pointer to pixel data. * `width`, `height`, `depth`, `spectrum`: Same as above. * `shared`: If `true`, the data is shared with the original pointer without making a separate copy. Constructor from Expression : CImg<T>(const char* expression, const char* variable_name = 0, const T& variable_value = 0, const char* variable_name1 = 0, const T& variable_value1 = 0); This constructor creates an image from a mathematical expression, allowing for variable substitutions. 2. Reading Command-Line Parameters The cimg_usage() and cimg_option() functions are used to handle command-line arguments. Here's a brief description of each function: cimg_usage(const char *const format, ...) : This function is typically used to print a description of your program when it's invoked from the command line. cimg_option(const char *const opt, type_def variable, const char *const format, ...) : This function is a command-line option parser. It's used to handle options passed to your program when it's invoked from the command line. Here's a breakdown of the parameters: opt : the name of the command-line option. variable : the default value that will be assigned to the variable if the corresponding command-line option is not provided. format : a string that may contain a description of what the option does (this will be printed if a specific help option is invoked, like --help ). Here's a simple example showing how you might use these functions: #include \"CImg.h\" int main(int argc, char **argv) { cimg_usage(\"My simple program that does XYZ.\"); int my_option1 = cimg_option(\"-o1\", 0, \"An optional parameter that affects behavior.\"); int my_option2 = cimg_option(\"-o2\", 99, \"Another optional parameter that affects behavior.\"); // Rest of the program } If the user runs the program with the options, like ./my_program -o1 5 , the my_option1 variable will be set to 5 , and the my_option2 variable will be set to the default value of 99 . If they run the program with the --help option, they will see the usage string followed by the options descriptions. 3. Get vs. Non-Get Methods in Image Processing with CImg In image processing using CImg, it's really helpful to know whether a method is going to give you a new object (a get method) or change the one you already have (a non-get method). CImg has both types for most of its methods: Get Methods : These create a new object with the changes you want, leaving the one you started with the same. Like CImg<float>::get_blur() , which makes a new blurred image but doesn't touch the original. Non-get Methods : These change the object you call them on and usually give you back a reference to that changed object. For example, CImg<float>::blur() changes the image and gives you back a reference to it. Here's an example to show how this works: CImg<> lum = img.get_norm().blur(sigma).normalize(0, 255); In this code, get_norm makes a new image (make it gray-scale by taking the L2-norm of the RGB channel), and blur and normalize change it and give you back references. This way of doing things makes it easy to chain operations together and save memory. Resutls: Original Image Luminance Image 4. Gradient Magnitude Computation Gradient is computed using the get_gradient() method. The gradient is computed using the centered finite differences by default. We will discuss the spatial filtering in Chapter 5. The method returns a CImgList object, which is a list of images in the order you specify. In this case, we want the gradient in the x and y directions, so we specify \"xy\" as the argument. CImgList<> grad = lum.get_gradient(\"xy\"); Then we compute the gradient magnitude using the following formula: \\|\\nabla I\\| = \\sqrt{G_x^2 + G_y^2} CImg<> normGrad = (grad[0].get_sqr() += grad[1].get_sqr()).sqrt(); Notice the use of the \"non-get\" += operator, which prevents the creation of an unnecessary temporary image. Results: Gradient Magnitude Image 5. Block Decomposition See Algorithm 1 in the book for the pseudocode. Here's my breakdown: Accessing the Current Block : The loop iterates through a list of blocks ( blocks ), where each block is represented by a CImg<int> object containing four integers representing the coordinates of the top-left and bottom-right corners of the block (x0, y0) and (x1, y1). (Yes, CImg can be used as 1D vectors using CImg<int>::vector() . Checking Conditions : For each block, the code checks two conditions: (a) If the maximum value of the normGrad image, when cropped to the current block, is greater than a given threshold. (b) If both the width and height of the block are greater than 8. Splitting the Block : If both conditions are met, the block is divided into four equal parts. The new blocks are created by calculating the midpoint of the original block (xc, yc) and using these coordinates to define the four new blocks. Updating the List of Blocks : The four new blocks are added to the blocks list using the move_to() method to avoid creating unnecessary copies. The original block is then removed from the list using the remove() method. Continuing Iteration : If the conditions are not met, the loop simply moves on to the next block by incrementing the index l . 6. Loop Iteration in CImg The following macros greatly simplify writing loops that iterate over various parts of an image: cimg_for(img,ptr,T) : Iterates over all pixels of an image. cimg_for(img, ptr, T) { // Do something with ptr, a pointer to the pixel value. } cimg_forX(img,x) : Iterates over the width of an image. cimg_forX(img, x) { // x is the x-coordinate, ranging from 0 to img.width() - 1. } For macro cimg_forX(img,x) , you do not need to declare x as an integer before using it in the loop. The macro itself takes care of that. If there is already a variable named x in the same scope where you're using this macro, you can place the code inside a different scope. cimg_forY(img,y) : Iterates over the height of an image. cimg_forY(img, y) { // y is the y-coordinate, ranging from 0 to img.height() - 1. } cimg_forZ(img,z) : Iterates over the depth of an image (for 3D images). cimg_forZ(img, z) { // z is the z-coordinate, ranging from 0 to img.depth() - 1. } cimg_forC(img,c) : Iterates over the channels (spectrum) of an image. cimg_forC(img, c) { // c is the channel index, ranging from 0 to img.spectrum() - 1. } cimg_forXY(img,x,y) : Iterates over both the width and height of an image. cimg_forXY(img, x, y) { // Do something with x and y coordinates. } cimg_forXYZ(img,x,y,z) : Iterates over width, height, and depth of a 3D image. cimg_forXYZ(img, x, y, z) { // Do something with x, y, and z coordinates. } cimg_forXYZC(img,x,y,z,c) : Iterates over all dimensions, including channels. cimg_forXYZC(img, x, y, z, c) { // Do something with x, y, z coordinates and channel c. } 7. Drawing Blocks blocks is a vector of (x0, y0, x1, y1) coordinates. In the following code, we iterate through each block. We use get_crop() and resize() the cropped image to 1x1 pixels, which is the average color of the block. Then we draw a rectangle using the draw_rectangle() method. // Rendering of the decomposition. CImg<unsigned char> res(img.width(), img.height(), 1, 3, 0); CImg<int> coords(img.width(), img.height(), 1, 4, 0); cimglist_for(blocks, l) { CImg<int> &block = blocks[l]; int x0 = block[0], y0 = block[1], x1 = block[2], y1 = block[3]; CImg<unsigned char> color = img.get_crop(x0, y0, x1, y1).resize(1, 1, 1, 3, 2); res.draw_rectangle(x0, y0, x1, y1, color.data(), 1); coords.draw_rectangle(x0, y0, x1, y1, block.data()); } There are two CImg objects here: res : This is the final image that will be displayed. It is initialized to all black pixels. coords : This maps each pixel to the block it belongs to. It will be used later for user interaction. // Adding black borders. res.mul(1 - (res.get_shift(1, 1, 0, 0, 0) - res).norm().cut(0, 1)); This above is a clever way to add black borders: Part 1: Edge Detection The first part of the expression is a kind of high-pass edge detection filter that calculates the difference between adjacent pixels in the image, thereby emphasizing sharp changes or edges. The expression for this part is: \\text{mask}(x, y) = \\left(1 - \\min\\left(1, \\max\\left(0, \\|res(x, y) - res(x - 1, y - 1)\\|\\right)\\right)\\right) Here, \\|res(x, y) - res(x - 1, y - 1)\\| calculates the difference between adjacent pixels. By using the min and max functions, this difference is clipped to the range [0, 1] , with 0 representing no change (no edge) and 1 representing a large change (an edge). res mask Part 2: Multiplication The second part of the expression involves multiplying the original pixel value with the edge value obtained in Part 1. This has the effect of enhancing the detected edges in the image. The expression for this part is: \\text{result}(x, y) = res(x, y) \\cdot \\text{mask}(x, y) Block Decomposition The book also propose two other ways to render the borders: CImg<unsigned char>::fill() and cimg_for3x3 . 8. GUI The CImgDisplay class is used to create a window and display an image. It has the following constructor: CImgDisplay disp(const CImg<T>& img, const char* title = 0, const int normalization_type = 3); The normalization type is used to specify how the image is normalized when displayed. The following table shows the different options: Normalization Value Description 0 No normalization applied. 1 Automatic linear normalization to the [0, 255] range. 2 A one-time linear normalization with parameters calculated at the first display. These are then reused for subsequent images in the same window. Ideal for preserving consistent gray levels. 3 Default automatic mode, with behavior depending on the type. The following code creates a window and displays the image res : // Start the interactive viewer. CImgDisplay disp(res, \"CImg Tutorial: Block Decomposition\", 0); unsigned char white[] = {255, 255, 255}, black[] = {0, 0, 0}; while (!disp.is_closed() && !disp.is_keyESC()) { int x = disp.mouse_x(), y = disp.mouse_y(); if (x >= 0 && y >= 0) { // Get the coordinates of the block under the mouse position. int x0 = coords(x, y, 0), y0 = coords(x, y, 1), x1 = coords(x, y, 2), y1 = coords(x, y, 3), xc = (x0 + x1) / 2, yc = (y0 + y1) / 2; // Get the block and its gradient. CImg<unsigned char> pImg = img.get_crop(x0, y0, x1, y1).resize(128, 128, 1, 3, 1), pGrad = normGrad.get_crop(x0, y0, x1, y1).resize(128, 128, 1, 3, 1).normalize(0, 255).map(CImg<unsigned char>::hot_LUT256()); // Display the block and its gradient. (+res). draw_text(10, 3, \"X, Y = %d, %d\", white, 0, 1, 24, x, y). draw_rectangle(x0, y0, x1, y1, black, 0.25f). draw_line(74, 109, xc, yc, white, 0.75, 0xCCCCCCCC). draw_line(74, 264, xc, yc, white, 0.75, 0xCCCCCCCC). draw_rectangle(7, 32, 140, 165, white). draw_rectangle(7, 197, 140, 330, white). draw_image(10, 35, pImg). draw_image(10, 200, pGrad). display(disp); } disp.wait(); // Wait for an user event if (disp.is_resized()) disp.resize(disp); } Read a book for more details. Note that (+res) is a trick used to make a copy of the image res without creating a new object.","title":"2. Block Decomposition"},{"location":"02_block_decomposition/#block-decomposition-learning-reflection","text":"Author : Tony Fu Date : August 18, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : Github Reference : Chapter 2.3 - 2.7 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Block Decomposition - Learning Reflection"},{"location":"02_block_decomposition/#1-cimg-template-class","text":"The CImg library is a template-based image manipulation library, and its template argument specifies the pixel type. By default, if you don't specify the template argument, it's instantiated with float . So when you declare an image like this: CImg<> img(\"image.png\"); It is equivalent to: CImg<float> img(\"image.png\"); You can specify a different type if you want, such as unsigned char , int , etc. But if you simply use CImg<> , then it defaults to using float . Below are some commonly used constructors for CImg<T> : Default Constructor : CImg<T>(); This constructs an empty image. Constructor with Dimensions : CImg<T>(const unsigned int width, const unsigned int height, const unsigned int depth = 1, const unsigned int spectrum = 1, const T& value = 0); * `width`: Width of the image. * `height`: Height of the image. * `depth`: Depth of the image (default is 1 for 2D images). * `spectrum`: Number of channels (e.g., 3 for RGB image). * `value`: Initial value for all pixels. Copy Constructor : CImg<T>(const CImg<T>& img); Constructs a copy of the given image img . Constructor from File : CImg<T>(const char* filename); Constructs an image by reading from a file specified by filename . Constructor from Data : CImg<T>(const T* data, const unsigned int width, const unsigned int height, const unsigned int depth = 1, const unsigned int spectrum = 1, const bool shared = false); * `data`: Pointer to pixel data. * `width`, `height`, `depth`, `spectrum`: Same as above. * `shared`: If `true`, the data is shared with the original pointer without making a separate copy. Constructor from Expression : CImg<T>(const char* expression, const char* variable_name = 0, const T& variable_value = 0, const char* variable_name1 = 0, const T& variable_value1 = 0); This constructor creates an image from a mathematical expression, allowing for variable substitutions.","title":"1. CImg Template Class"},{"location":"02_block_decomposition/#2-reading-command-line-parameters","text":"The cimg_usage() and cimg_option() functions are used to handle command-line arguments. Here's a brief description of each function: cimg_usage(const char *const format, ...) : This function is typically used to print a description of your program when it's invoked from the command line. cimg_option(const char *const opt, type_def variable, const char *const format, ...) : This function is a command-line option parser. It's used to handle options passed to your program when it's invoked from the command line. Here's a breakdown of the parameters: opt : the name of the command-line option. variable : the default value that will be assigned to the variable if the corresponding command-line option is not provided. format : a string that may contain a description of what the option does (this will be printed if a specific help option is invoked, like --help ). Here's a simple example showing how you might use these functions: #include \"CImg.h\" int main(int argc, char **argv) { cimg_usage(\"My simple program that does XYZ.\"); int my_option1 = cimg_option(\"-o1\", 0, \"An optional parameter that affects behavior.\"); int my_option2 = cimg_option(\"-o2\", 99, \"Another optional parameter that affects behavior.\"); // Rest of the program } If the user runs the program with the options, like ./my_program -o1 5 , the my_option1 variable will be set to 5 , and the my_option2 variable will be set to the default value of 99 . If they run the program with the --help option, they will see the usage string followed by the options descriptions.","title":"2. Reading Command-Line Parameters"},{"location":"02_block_decomposition/#3-get-vs-non-get-methods-in-image-processing-with-cimg","text":"In image processing using CImg, it's really helpful to know whether a method is going to give you a new object (a get method) or change the one you already have (a non-get method). CImg has both types for most of its methods: Get Methods : These create a new object with the changes you want, leaving the one you started with the same. Like CImg<float>::get_blur() , which makes a new blurred image but doesn't touch the original. Non-get Methods : These change the object you call them on and usually give you back a reference to that changed object. For example, CImg<float>::blur() changes the image and gives you back a reference to it. Here's an example to show how this works: CImg<> lum = img.get_norm().blur(sigma).normalize(0, 255); In this code, get_norm makes a new image (make it gray-scale by taking the L2-norm of the RGB channel), and blur and normalize change it and give you back references. This way of doing things makes it easy to chain operations together and save memory. Resutls: Original Image Luminance Image","title":"3. Get vs. Non-Get Methods in Image Processing with CImg"},{"location":"02_block_decomposition/#4-gradient-magnitude-computation","text":"Gradient is computed using the get_gradient() method. The gradient is computed using the centered finite differences by default. We will discuss the spatial filtering in Chapter 5. The method returns a CImgList object, which is a list of images in the order you specify. In this case, we want the gradient in the x and y directions, so we specify \"xy\" as the argument. CImgList<> grad = lum.get_gradient(\"xy\"); Then we compute the gradient magnitude using the following formula: \\|\\nabla I\\| = \\sqrt{G_x^2 + G_y^2} CImg<> normGrad = (grad[0].get_sqr() += grad[1].get_sqr()).sqrt(); Notice the use of the \"non-get\" += operator, which prevents the creation of an unnecessary temporary image. Results: Gradient Magnitude Image","title":"4. Gradient Magnitude Computation"},{"location":"02_block_decomposition/#5-block-decomposition","text":"See Algorithm 1 in the book for the pseudocode. Here's my breakdown: Accessing the Current Block : The loop iterates through a list of blocks ( blocks ), where each block is represented by a CImg<int> object containing four integers representing the coordinates of the top-left and bottom-right corners of the block (x0, y0) and (x1, y1). (Yes, CImg can be used as 1D vectors using CImg<int>::vector() . Checking Conditions : For each block, the code checks two conditions: (a) If the maximum value of the normGrad image, when cropped to the current block, is greater than a given threshold. (b) If both the width and height of the block are greater than 8. Splitting the Block : If both conditions are met, the block is divided into four equal parts. The new blocks are created by calculating the midpoint of the original block (xc, yc) and using these coordinates to define the four new blocks. Updating the List of Blocks : The four new blocks are added to the blocks list using the move_to() method to avoid creating unnecessary copies. The original block is then removed from the list using the remove() method. Continuing Iteration : If the conditions are not met, the loop simply moves on to the next block by incrementing the index l .","title":"5. Block Decomposition"},{"location":"02_block_decomposition/#6-loop-iteration-in-cimg","text":"The following macros greatly simplify writing loops that iterate over various parts of an image: cimg_for(img,ptr,T) : Iterates over all pixels of an image. cimg_for(img, ptr, T) { // Do something with ptr, a pointer to the pixel value. } cimg_forX(img,x) : Iterates over the width of an image. cimg_forX(img, x) { // x is the x-coordinate, ranging from 0 to img.width() - 1. } For macro cimg_forX(img,x) , you do not need to declare x as an integer before using it in the loop. The macro itself takes care of that. If there is already a variable named x in the same scope where you're using this macro, you can place the code inside a different scope. cimg_forY(img,y) : Iterates over the height of an image. cimg_forY(img, y) { // y is the y-coordinate, ranging from 0 to img.height() - 1. } cimg_forZ(img,z) : Iterates over the depth of an image (for 3D images). cimg_forZ(img, z) { // z is the z-coordinate, ranging from 0 to img.depth() - 1. } cimg_forC(img,c) : Iterates over the channels (spectrum) of an image. cimg_forC(img, c) { // c is the channel index, ranging from 0 to img.spectrum() - 1. } cimg_forXY(img,x,y) : Iterates over both the width and height of an image. cimg_forXY(img, x, y) { // Do something with x and y coordinates. } cimg_forXYZ(img,x,y,z) : Iterates over width, height, and depth of a 3D image. cimg_forXYZ(img, x, y, z) { // Do something with x, y, and z coordinates. } cimg_forXYZC(img,x,y,z,c) : Iterates over all dimensions, including channels. cimg_forXYZC(img, x, y, z, c) { // Do something with x, y, z coordinates and channel c. }","title":"6. Loop Iteration in CImg"},{"location":"02_block_decomposition/#7-drawing-blocks","text":"blocks is a vector of (x0, y0, x1, y1) coordinates. In the following code, we iterate through each block. We use get_crop() and resize() the cropped image to 1x1 pixels, which is the average color of the block. Then we draw a rectangle using the draw_rectangle() method. // Rendering of the decomposition. CImg<unsigned char> res(img.width(), img.height(), 1, 3, 0); CImg<int> coords(img.width(), img.height(), 1, 4, 0); cimglist_for(blocks, l) { CImg<int> &block = blocks[l]; int x0 = block[0], y0 = block[1], x1 = block[2], y1 = block[3]; CImg<unsigned char> color = img.get_crop(x0, y0, x1, y1).resize(1, 1, 1, 3, 2); res.draw_rectangle(x0, y0, x1, y1, color.data(), 1); coords.draw_rectangle(x0, y0, x1, y1, block.data()); } There are two CImg objects here: res : This is the final image that will be displayed. It is initialized to all black pixels. coords : This maps each pixel to the block it belongs to. It will be used later for user interaction. // Adding black borders. res.mul(1 - (res.get_shift(1, 1, 0, 0, 0) - res).norm().cut(0, 1)); This above is a clever way to add black borders:","title":"7. Drawing Blocks"},{"location":"02_block_decomposition/#part-1-edge-detection","text":"The first part of the expression is a kind of high-pass edge detection filter that calculates the difference between adjacent pixels in the image, thereby emphasizing sharp changes or edges. The expression for this part is: \\text{mask}(x, y) = \\left(1 - \\min\\left(1, \\max\\left(0, \\|res(x, y) - res(x - 1, y - 1)\\|\\right)\\right)\\right) Here, \\|res(x, y) - res(x - 1, y - 1)\\| calculates the difference between adjacent pixels. By using the min and max functions, this difference is clipped to the range [0, 1] , with 0 representing no change (no edge) and 1 representing a large change (an edge). res mask","title":"Part 1: Edge Detection"},{"location":"02_block_decomposition/#part-2-multiplication","text":"The second part of the expression involves multiplying the original pixel value with the edge value obtained in Part 1. This has the effect of enhancing the detected edges in the image. The expression for this part is: \\text{result}(x, y) = res(x, y) \\cdot \\text{mask}(x, y) Block Decomposition The book also propose two other ways to render the borders: CImg<unsigned char>::fill() and cimg_for3x3 .","title":"Part 2: Multiplication"},{"location":"02_block_decomposition/#8-gui","text":"The CImgDisplay class is used to create a window and display an image. It has the following constructor: CImgDisplay disp(const CImg<T>& img, const char* title = 0, const int normalization_type = 3); The normalization type is used to specify how the image is normalized when displayed. The following table shows the different options: Normalization Value Description 0 No normalization applied. 1 Automatic linear normalization to the [0, 255] range. 2 A one-time linear normalization with parameters calculated at the first display. These are then reused for subsequent images in the same window. Ideal for preserving consistent gray levels. 3 Default automatic mode, with behavior depending on the type. The following code creates a window and displays the image res : // Start the interactive viewer. CImgDisplay disp(res, \"CImg Tutorial: Block Decomposition\", 0); unsigned char white[] = {255, 255, 255}, black[] = {0, 0, 0}; while (!disp.is_closed() && !disp.is_keyESC()) { int x = disp.mouse_x(), y = disp.mouse_y(); if (x >= 0 && y >= 0) { // Get the coordinates of the block under the mouse position. int x0 = coords(x, y, 0), y0 = coords(x, y, 1), x1 = coords(x, y, 2), y1 = coords(x, y, 3), xc = (x0 + x1) / 2, yc = (y0 + y1) / 2; // Get the block and its gradient. CImg<unsigned char> pImg = img.get_crop(x0, y0, x1, y1).resize(128, 128, 1, 3, 1), pGrad = normGrad.get_crop(x0, y0, x1, y1).resize(128, 128, 1, 3, 1).normalize(0, 255).map(CImg<unsigned char>::hot_LUT256()); // Display the block and its gradient. (+res). draw_text(10, 3, \"X, Y = %d, %d\", white, 0, 1, 24, x, y). draw_rectangle(x0, y0, x1, y1, black, 0.25f). draw_line(74, 109, xc, yc, white, 0.75, 0xCCCCCCCC). draw_line(74, 264, xc, yc, white, 0.75, 0xCCCCCCCC). draw_rectangle(7, 32, 140, 165, white). draw_rectangle(7, 197, 140, 330, white). draw_image(10, 35, pImg). draw_image(10, 200, pGrad). display(disp); } disp.wait(); // Wait for an user event if (disp.is_resized()) disp.resize(disp); } Read a book for more details. Note that (+res) is a trick used to make a copy of the image res without creating a new object.","title":"8. GUI"},{"location":"03_point_processing/","text":"Point Processing Transformations - Learning Reflection Author : Tony Fu Date : August 19, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 3 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. Mathematical Transformations 1.0. Orignal Image 1.1. Exponential Transformations CImg<> expImg = lum.get_exp()/50; 1.2. Square Root Transformations CImg<> sqrtImg = lum.get_sqrt()*10; 1.3. Logarithmic Transformations CImg<> logImg = (2 + lum.get_abs()).log(); 1.4 Cube Transformations CImg<> cubeImg = lum.get_pow(3); 2. Bitwise Transformations 2.0. Orignal Images 2.1. Bitwise AND CImg<unsigned char> img_and = img1 & img2; If you have two 8-bit pixels, one with the value 0101 (5 in decimal) and the other with 1010 (10 in decimal), then the bitwise AND operation between these two pixels would give you 0000 (0 in decimal). 2.2. Bitwise OR CImg<unsigned char> img_or = img1 | img2; 2.3. Bitwise XOR CImg<unsigned char> img_xor = img1 ^ img2; As we seen in the above example, bitwise operations might seem somewhat abstract when applied to images (5 & 10 = 0. What?), but they are actually quite useful in in specific contexts: Masking : If you have a binary mask where certain pixels are set to 1 and others are set to 0, you can use the bitwise AND operation with an image to \"mask\" or isolate those areas. Steganography or Watermarking : Bitwise operations might be used to embed or extract information within an image. By operating at the bit level, you can hide data within the least significant bits of an image in a way that's almost visually imperceptible. Refer to Code 3.3 in the book for an example. Thresholding : If you have two binary images representing different thresholded features, the bitwise AND can be used to find the overlapping areas between those features. 3. Histogram Equalization 3.0. Orignal Image And the corresponding histogram: 3.1. Histogram Equalization CImg<> equalizeHisto(CImg<> &imgIn, unsigned int nb) { CImg<> imgOut(imgIn); // Create a copy of the input image for the output float vmin, vmax = imgIn.max_min(vmin); // Find the minimum and maximum pixel values int size = imgIn.size(); // Get the total number of pixels in the image int vdiff = vmax - vmin; // Compute the difference between max and min values CImg<> hist = imgIn.get_histogram(nb, vmin, vmax); // Calculate the histogram with nb bins long int cumul = 0; // Compute the cumulative histogram cimg_forX(hist, pos) { cumul += hist[pos]; hist[pos] = cumul; } if (cumul == 0) // Check for a special case where the image has no non-zero pixels cumul = 1; // Avoid division by zero later in the code // Equalize the image by adjusting pixel values according to the cumulative histogram cimg_foroff(imgIn, off) // Iterate through all offsets (positions) in the input image { int pos = (int)((imgIn[off] - vmin) * (nb - 1) / vdiff); if (pos >= 0 && pos < (int)nb) imgOut[off] = vmin + vdiff * hist[pos] / size; } return imgOut; } In the example code above, cimg_foroff is a macro provided by the CImg library that creates a loop to iterate over all the pixels in the image. Inside this loop, we first calculate the position of the pixel in the histogram ( pos ). \\text{{pos}} = \\left\\lfloor \\frac{{(I_{\\text{{off}}} - \\text{{vmin}}) \\cdot (\\text{{nb}} - 1)}}{{\\text{{vdiff}}}} \\right\\rfloor where I_{\\text{{off}}} is the intensity of the pixel at offset off , \\text{{vmin}} and \\text{{vmax}} are the minimum and maximum intensity values in the image, \\text{{vdiff}} = \\text{{vmax}} - \\text{{vmin}} , and \\text{{nb}} is the number of bins in the histogram. Then, we check if the position is within the valid range of the histogram: \\text{if } \\text{{pos}} \\geq 0 \\text{ and } \\text{{pos}} < \\text{{nb}} If so, we calculate the new pixel value using the following expression: I_{\\text{{out, off}}} = \\text{{vmin}} + \\frac{{\\text{{vdiff}} \\cdot H_{\\text{{pos}}}}}{{\\text{{size}}}} where I_{\\text{{out, off}}} is the new intensity value at offset off , H_{\\text{{pos}}} is the value at the pos position in the cumulative histogram, and \\text{{size}} is the total number of pixels in the image. And the corresponding histogram:","title":"3. Point Processing Transformations"},{"location":"03_point_processing/#point-processing-transformations-learning-reflection","text":"Author : Tony Fu Date : August 19, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 3 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Point Processing Transformations - Learning Reflection"},{"location":"03_point_processing/#1-mathematical-transformations","text":"","title":"1. Mathematical Transformations"},{"location":"03_point_processing/#10-orignal-image","text":"","title":"1.0. Orignal Image"},{"location":"03_point_processing/#11-exponential-transformations","text":"CImg<> expImg = lum.get_exp()/50;","title":"1.1. Exponential Transformations"},{"location":"03_point_processing/#12-square-root-transformations","text":"CImg<> sqrtImg = lum.get_sqrt()*10;","title":"1.2. Square Root Transformations"},{"location":"03_point_processing/#13-logarithmic-transformations","text":"CImg<> logImg = (2 + lum.get_abs()).log();","title":"1.3. Logarithmic Transformations"},{"location":"03_point_processing/#14-cube-transformations","text":"CImg<> cubeImg = lum.get_pow(3);","title":"1.4 Cube Transformations"},{"location":"03_point_processing/#2-bitwise-transformations","text":"","title":"2. Bitwise Transformations"},{"location":"03_point_processing/#20-orignal-images","text":"","title":"2.0. Orignal Images"},{"location":"03_point_processing/#21-bitwise-and","text":"CImg<unsigned char> img_and = img1 & img2; If you have two 8-bit pixels, one with the value 0101 (5 in decimal) and the other with 1010 (10 in decimal), then the bitwise AND operation between these two pixels would give you 0000 (0 in decimal).","title":"2.1. Bitwise AND"},{"location":"03_point_processing/#22-bitwise-or","text":"CImg<unsigned char> img_or = img1 | img2;","title":"2.2. Bitwise OR"},{"location":"03_point_processing/#23-bitwise-xor","text":"CImg<unsigned char> img_xor = img1 ^ img2; As we seen in the above example, bitwise operations might seem somewhat abstract when applied to images (5 & 10 = 0. What?), but they are actually quite useful in in specific contexts: Masking : If you have a binary mask where certain pixels are set to 1 and others are set to 0, you can use the bitwise AND operation with an image to \"mask\" or isolate those areas. Steganography or Watermarking : Bitwise operations might be used to embed or extract information within an image. By operating at the bit level, you can hide data within the least significant bits of an image in a way that's almost visually imperceptible. Refer to Code 3.3 in the book for an example. Thresholding : If you have two binary images representing different thresholded features, the bitwise AND can be used to find the overlapping areas between those features.","title":"2.3. Bitwise XOR"},{"location":"03_point_processing/#3-histogram-equalization","text":"","title":"3. Histogram Equalization"},{"location":"03_point_processing/#30-orignal-image","text":"And the corresponding histogram:","title":"3.0. Orignal Image"},{"location":"03_point_processing/#31-histogram-equalization","text":"CImg<> equalizeHisto(CImg<> &imgIn, unsigned int nb) { CImg<> imgOut(imgIn); // Create a copy of the input image for the output float vmin, vmax = imgIn.max_min(vmin); // Find the minimum and maximum pixel values int size = imgIn.size(); // Get the total number of pixels in the image int vdiff = vmax - vmin; // Compute the difference between max and min values CImg<> hist = imgIn.get_histogram(nb, vmin, vmax); // Calculate the histogram with nb bins long int cumul = 0; // Compute the cumulative histogram cimg_forX(hist, pos) { cumul += hist[pos]; hist[pos] = cumul; } if (cumul == 0) // Check for a special case where the image has no non-zero pixels cumul = 1; // Avoid division by zero later in the code // Equalize the image by adjusting pixel values according to the cumulative histogram cimg_foroff(imgIn, off) // Iterate through all offsets (positions) in the input image { int pos = (int)((imgIn[off] - vmin) * (nb - 1) / vdiff); if (pos >= 0 && pos < (int)nb) imgOut[off] = vmin + vdiff * hist[pos] / size; } return imgOut; } In the example code above, cimg_foroff is a macro provided by the CImg library that creates a loop to iterate over all the pixels in the image. Inside this loop, we first calculate the position of the pixel in the histogram ( pos ). \\text{{pos}} = \\left\\lfloor \\frac{{(I_{\\text{{off}}} - \\text{{vmin}}) \\cdot (\\text{{nb}} - 1)}}{{\\text{{vdiff}}}} \\right\\rfloor where I_{\\text{{off}}} is the intensity of the pixel at offset off , \\text{{vmin}} and \\text{{vmax}} are the minimum and maximum intensity values in the image, \\text{{vdiff}} = \\text{{vmax}} - \\text{{vmin}} , and \\text{{nb}} is the number of bins in the histogram. Then, we check if the position is within the valid range of the histogram: \\text{if } \\text{{pos}} \\geq 0 \\text{ and } \\text{{pos}} < \\text{{nb}} If so, we calculate the new pixel value using the following expression: I_{\\text{{out, off}}} = \\text{{vmin}} + \\frac{{\\text{{vdiff}} \\cdot H_{\\text{{pos}}}}}{{\\text{{size}}}} where I_{\\text{{out, off}}} is the new intensity value at offset off , H_{\\text{{pos}}} is the value at the pos position in the cumulative histogram, and \\text{{size}} is the total number of pixels in the image. And the corresponding histogram:","title":"3.1. Histogram Equalization"},{"location":"04_morphology/","text":"Mathematical Morphology - Learning Reflection Author : Tony Fu Date : August 19, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 4 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. Erosion and Dilation I found the book's explanation of erosion and dilation to be a bit confusing. I would recommend watching this video for a more intuitive explanation of the concepts. Mathematical morphology operations have a straightforward definition when dealing with binary images. For grayscale images, the definitions become more complex: Binary Images For binary images A and a structuring element B , the operations can be defined as follows: Dilation : (A \\oplus B)(x,y) = \\max \\{ A(x-i, y-j) \\cdot B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Erosion : (A \\ominus B)(x,y) = \\min \\{ A(x+i, y+j) \\cdot B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Gray-Level Images For grayscale images A and a structuring element B , the definitions change slightly: Dilation : (A \\oplus B)(x,y) = \\max \\{ A(x-i, y-j) + B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Dilation causes bright regions to expand and dark regions to contract. Erosion : (A \\ominus B)(x,y) = \\min \\{ A(x+i, y+j) - B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Erosion causes bright regions to contract and dark regions to expand. In CImg, we can perform erosion and dilation using the erode() and dilate() functions. Both functions take a structuring element B as an argument. CImg<unsigned char> img(\"coins.png\"); CImg<> lum = img.get_norm().blur(0.75f); lum.threshold(lum.median()).normalize(0, 255); CImg<unsigned char> B = CImg<unsigned char>(3, 3).fill(1); CImg<unsigned char> imgErode = lum.get_erode(B), // Erosion imgDilate = lum.get_dilate(B); // Dilation\" Original : Binarized Luminance : Then Apply Erosion : ... or Dilation : 2. Opening and Closing The opening and closing operations are defined as follows (works for both binary and grayscale images): Opening : A \\circ B = (A \\ominus B) \\oplus B Closing : A \\bullet B = (A \\oplus B) \\ominus B CImg<unsigned char> B = CImg<unsigned char>(3, 3).fill(1); CImg<unsigned char> imgErode = lum.get_erode(B), // Erosion imgDilate = lum.get_dilate(B), // Dilation\" imgOpen = imgErode.get_dilate(B), // Opening imgClose = imgDilate.get_erode(B); // Closing Opening : Erode, then dilate (removes small objects and smooths larger ones) Closing : Dilate, then erode (closes small holes and joins nearby objects) 3. Kramer-Bruckner Filter The Kramer-Bruckner filter is a specific morphological filter used to enhance contrast and reduce noise in an image. It performs a combination of dilation and erosion, typically using a circular structuring element. In math, it can be expressed as the following two formulas: Compute the mid-value M for each pixel: M(x,y) = 0.5 \\times \\left( (A \\ominus B)(x,y) + (A \\oplus B)(x,y) \\right) Assign the output image values based on the input image and mid-value: A_{\\text{out}}(x,y) = \\begin{cases} (A \\ominus B)(x,y) & \\text{if } A(x,y) \\leq M(x,y) \\\\ (A \\oplus B)(x,y) & \\text{if } A(x,y) > M(x,y) \\end{cases} Here, A is the input image, B is the structuring element, \\ominus denotes erosion, and \\oplus denotes dilation. CImg<> KramerBruckner(CImg<> &imgIn, int n) { CImg<> imgOut(imgIn), mask = CImg<>(n, n).fill(1), imgErode = imgIn.get_erode(mask), imgDilate = imgIn.get_dilate(mask); // Dilation cimg_forXY(imgOut, x, y) { float M = 0.5f * (imgErode(x, y) + imgDilate(x, y)); imgOut(x, y) = (imgIn(x, y) <= M ? imgErode(x, y) : imgDilate(x, y)); } return imgOut; } 4. Alternating Sequential Filters (ASF): Alternating Sequential Filters are a series of morphological operations that are applied sequentially, often involving both erosions and dilations with increasing sizes of structuring elements. ASF (n = 1) : Erosion, then dilation ASF (n = 3) : (Erosion, then dilation) * 3 ASF (n = 11) : 5. Other Morphological Operations 1. Morphological Gradients Morphological gradients measure the difference between dilation and erosion of an image. In this code, two gradients are computed: the erosion gradient and the dilation gradient. Erosion Gradient ( \\text{gradE} ): \\text{gradE} = A - (A \\ominus B) Dilation Gradient ( \\text{gradD} ): \\text{gradD} = (A \\oplus B) - A Gradient E: Gradient D: 2. Beucher Gradient The Beucher gradient is another way of expressing the difference between dilation and erosion. It's calculated as the difference between the dilated and eroded images. Beucher Gradient: \\text{imgBeucher} = (A \\oplus B) - (A \\ominus B) 3. Top Hat Transformations Top hat transformations emphasize differences between the original image and its morphological opening or closing. White Top Hat: \\text{whiteTopHat} = A - (A \\ominus B) \\oplus B = A - A \\circ B Black Top Hat: \\text{blackTopHat} = (A \\oplus B) \\ominus B - A = A \\bullet B - A White Top Hat: Black Top Hat: 4. Edge Detector (contourMin and contourMax) These operations compute the minimum and maximum between the erosion and dilation gradients, respectively. Contour Min: \\text{contourMin} = \\min(\\text{gradE}, \\text{gradD}) Contour Max: \\text{contourMax} = \\max(\\text{gradE}, \\text{gradD}) Contour Min (this is blank because there is no overlap between gradE and gradD ) Contour Max: 5. Nonlinear Laplacian The nonlinear Laplacian is the difference between the dilation and erosion gradients. Nonlinear Laplacian: \\text{Laplician} = \\text{gradD} - \\text{gradE} 6. Skeletonization Here we implement the Zhang-Suen skeletonization algorithm using the CImg library in C++. Very Important Note : this method requires that the input image is binary (i.e., pixels are either 0 or 1). Iterative Skeletonization Function The main function for skeletonization is IterSkeleton . This function takes a binary image as input and performs two passes, removing certain edge pixels in each pass. Initializing Variables CImg<unsigned char> D(imgIn.width(), imgIn.height(), 1, 1, 0); CImg_3x3(N, unsigned char); Here, we use D to tag pixels for removal. Whenever a pixel is tagged, we set its value to 1. Note that CImg_3x3 has the macro format CImg_3x3(I,T) : #define CImg_3x3(I,T) T I[9]; \\ T& I##pp = I[0]; T& I##cp = I[1]; T& I##np = I[2]; \\ T& I##pc = I[3]; T& I##cc = I[4]; T& I##nc = I[5]; \\ T& I##pn = I[6]; T& I##cn = I[7]; T& I##nn = I[8]; \\ I##pp = I##cp = I##np = \\ I##pc = I##cc = I##nc = \\ I##pn = I##cn = I##nn = 0 So that in the subsequent code, we can use N to access the 8 neighbors of a pixel. The variables Npp, Ncp, Nnp, Npc, Nnc, Npn, Ncn, Nnn represent the 8 neighboring pixels around the central pixel (x, y) . The variable names correspond to their relative positions: Npp : Previous row, previous column. Ncp : Current row, previous column. Nnp : Next row, previous column. Npc : Previous row, current column. Nnc : Next row, current column. Npn : Previous row, next column. Ncn : Current row, next column. Nnn : Next row, next column. The central pixel itself is referred to as Ncc . Pass 1 The first pass checks each pixel and its 8 neighbors to determine if it should be removed: B = N_{pp} + N_{cp} + N_{np} + N_{pc} + N_{nc} + N_{pn} + N_{cn} + N_{nn} C = N_{nc} \\cdot (N_{nc} - N_{np}) + N_{np} \\cdot (N_{np} - N_{cp}) + \\cdots + N_{nn} \\cdot (N_{nn} - N_{nc}) Here, C is the the number of 0-1 transitions in the 8-neighborhood of the pixel. The book's description \"C(N), called connectivity number, expressing how many binary components are connected to the central pixel (x,y)\" appears to be incorrect. The conditions for removal are: R1 = \\left( B \\geq 2 \\right) \\land \\left( B \\leq 6 \\right) \\land \\left( C = 1 \\right) \\land \\left( N_{cn} \\cdot N_{nc} \\cdot N_{cp} = 0 \\right) \\land \\left( N_{pc} \\cdot N_{cn} \\cdot N_{nc} = 0 \\right) Intuitively, this means that the pixel should be removed if: It has 2-6 neighbors. (Because if B is 0 or 1, then the pixel is an end point or an isolated point.) It has exactly 1 0-1 transition in its 8-neighborhood because it is likely to be a boundary pixel. N_{cn} \\cdot N_{nc} \\cdot N_{cp} = 0 and N_{pc} \\cdot N_{cn} \\cdot N_{nc} = 0 for R1 , and N_{nc} \\cdot N_{cp} \\cdot N_{pc} = 0 and N_{cp} \\cdot N_{pc} \\cdot N_{cn} = 0 for R2 : These are specific conditions for Zhang-Suen thinning. They ensure that no spur pixels (pixels that stick out from the object) are created during the thinning process. For R1 , these conditions make sure that the pixel is not an endpoint in the south and east directions, and that removing it won't break connectivity. The conditions for R2 (see below) do a similar thing, but in the north and west directions. By separating the thinning process into two stages, the algorithm avoids the possibility of over-thinning or under-thinning. // Pass 1 int n1 = 0; cimg_for3x3(imgIn, x, y, 0, 0, N, unsigned char) { if (imgIn(x, y)) { // Compute B and C here... bool R1 = B >= 2 && B <= 6 && C == 1 && Ncn * Nnc * Ncp == 0 && Npc * Ncn * Nnc == 0; if (R1) { // Tag (x,y) D(x, y) = 1; ++n1; } } } Removing Tagged Pixels from Pass 1 cimg_forXY(imgIn, x, y) imgIn(x, y) -= (n1 > 0) * D(x, y); Pass 2 The second pass is similar to the first, but with different conditions for removal: R2 = \\left( B \\geq 2 \\right) \\land \\left( B \\leq 6 \\right) \\land \\left( C = 1 \\right) \\land \\left( N_{nc} \\cdot N_{cp} \\cdot N_{pc} = 0 \\right) \\land \\left( N_{cp} \\cdot N_{pc} \\cdot N_{cn} = 0 \\right) Removing Tagged Pixels from Pass 2 Similar to the removal in Pass 1. Return the Total Number of Removed Pixels return n1 + n2; Iterative Skeletonization int num_removed; do { num_removed = IterSkeleton(lum); } while (num_removed > 0); Here, we keep calling IterSkeleton until no pixels are removed in a pass. Results Original : Skeleton :","title":"4. Morphology"},{"location":"04_morphology/#mathematical-morphology-learning-reflection","text":"Author : Tony Fu Date : August 19, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 4 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Mathematical Morphology - Learning Reflection"},{"location":"04_morphology/#1-erosion-and-dilation","text":"I found the book's explanation of erosion and dilation to be a bit confusing. I would recommend watching this video for a more intuitive explanation of the concepts. Mathematical morphology operations have a straightforward definition when dealing with binary images. For grayscale images, the definitions become more complex:","title":"1. Erosion and Dilation"},{"location":"04_morphology/#binary-images","text":"For binary images A and a structuring element B , the operations can be defined as follows: Dilation : (A \\oplus B)(x,y) = \\max \\{ A(x-i, y-j) \\cdot B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Erosion : (A \\ominus B)(x,y) = \\min \\{ A(x+i, y+j) \\cdot B(i,j) : (i,j) \\in \\text{{domain of }} B \\}","title":"Binary Images"},{"location":"04_morphology/#gray-level-images","text":"For grayscale images A and a structuring element B , the definitions change slightly: Dilation : (A \\oplus B)(x,y) = \\max \\{ A(x-i, y-j) + B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Dilation causes bright regions to expand and dark regions to contract. Erosion : (A \\ominus B)(x,y) = \\min \\{ A(x+i, y+j) - B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Erosion causes bright regions to contract and dark regions to expand. In CImg, we can perform erosion and dilation using the erode() and dilate() functions. Both functions take a structuring element B as an argument. CImg<unsigned char> img(\"coins.png\"); CImg<> lum = img.get_norm().blur(0.75f); lum.threshold(lum.median()).normalize(0, 255); CImg<unsigned char> B = CImg<unsigned char>(3, 3).fill(1); CImg<unsigned char> imgErode = lum.get_erode(B), // Erosion imgDilate = lum.get_dilate(B); // Dilation\" Original : Binarized Luminance : Then Apply Erosion : ... or Dilation :","title":"Gray-Level Images"},{"location":"04_morphology/#2-opening-and-closing","text":"The opening and closing operations are defined as follows (works for both binary and grayscale images): Opening : A \\circ B = (A \\ominus B) \\oplus B Closing : A \\bullet B = (A \\oplus B) \\ominus B CImg<unsigned char> B = CImg<unsigned char>(3, 3).fill(1); CImg<unsigned char> imgErode = lum.get_erode(B), // Erosion imgDilate = lum.get_dilate(B), // Dilation\" imgOpen = imgErode.get_dilate(B), // Opening imgClose = imgDilate.get_erode(B); // Closing Opening : Erode, then dilate (removes small objects and smooths larger ones) Closing : Dilate, then erode (closes small holes and joins nearby objects)","title":"2. Opening and Closing"},{"location":"04_morphology/#3-kramer-bruckner-filter","text":"The Kramer-Bruckner filter is a specific morphological filter used to enhance contrast and reduce noise in an image. It performs a combination of dilation and erosion, typically using a circular structuring element. In math, it can be expressed as the following two formulas: Compute the mid-value M for each pixel: M(x,y) = 0.5 \\times \\left( (A \\ominus B)(x,y) + (A \\oplus B)(x,y) \\right) Assign the output image values based on the input image and mid-value: A_{\\text{out}}(x,y) = \\begin{cases} (A \\ominus B)(x,y) & \\text{if } A(x,y) \\leq M(x,y) \\\\ (A \\oplus B)(x,y) & \\text{if } A(x,y) > M(x,y) \\end{cases} Here, A is the input image, B is the structuring element, \\ominus denotes erosion, and \\oplus denotes dilation. CImg<> KramerBruckner(CImg<> &imgIn, int n) { CImg<> imgOut(imgIn), mask = CImg<>(n, n).fill(1), imgErode = imgIn.get_erode(mask), imgDilate = imgIn.get_dilate(mask); // Dilation cimg_forXY(imgOut, x, y) { float M = 0.5f * (imgErode(x, y) + imgDilate(x, y)); imgOut(x, y) = (imgIn(x, y) <= M ? imgErode(x, y) : imgDilate(x, y)); } return imgOut; }","title":"3. Kramer-Bruckner Filter"},{"location":"04_morphology/#4-alternating-sequential-filters-asf","text":"Alternating Sequential Filters are a series of morphological operations that are applied sequentially, often involving both erosions and dilations with increasing sizes of structuring elements. ASF (n = 1) : Erosion, then dilation ASF (n = 3) : (Erosion, then dilation) * 3 ASF (n = 11) :","title":"4. Alternating Sequential Filters (ASF):"},{"location":"04_morphology/#5-other-morphological-operations","text":"","title":"5. Other Morphological Operations"},{"location":"04_morphology/#1-morphological-gradients","text":"Morphological gradients measure the difference between dilation and erosion of an image. In this code, two gradients are computed: the erosion gradient and the dilation gradient. Erosion Gradient ( \\text{gradE} ): \\text{gradE} = A - (A \\ominus B) Dilation Gradient ( \\text{gradD} ): \\text{gradD} = (A \\oplus B) - A Gradient E: Gradient D:","title":"1. Morphological Gradients"},{"location":"04_morphology/#2-beucher-gradient","text":"The Beucher gradient is another way of expressing the difference between dilation and erosion. It's calculated as the difference between the dilated and eroded images. Beucher Gradient: \\text{imgBeucher} = (A \\oplus B) - (A \\ominus B)","title":"2. Beucher Gradient"},{"location":"04_morphology/#3-top-hat-transformations","text":"Top hat transformations emphasize differences between the original image and its morphological opening or closing. White Top Hat: \\text{whiteTopHat} = A - (A \\ominus B) \\oplus B = A - A \\circ B Black Top Hat: \\text{blackTopHat} = (A \\oplus B) \\ominus B - A = A \\bullet B - A White Top Hat: Black Top Hat:","title":"3. Top Hat Transformations"},{"location":"04_morphology/#4-edge-detector-contourmin-and-contourmax","text":"These operations compute the minimum and maximum between the erosion and dilation gradients, respectively. Contour Min: \\text{contourMin} = \\min(\\text{gradE}, \\text{gradD}) Contour Max: \\text{contourMax} = \\max(\\text{gradE}, \\text{gradD}) Contour Min (this is blank because there is no overlap between gradE and gradD ) Contour Max:","title":"4. Edge Detector (contourMin and contourMax)"},{"location":"04_morphology/#5-nonlinear-laplacian","text":"The nonlinear Laplacian is the difference between the dilation and erosion gradients. Nonlinear Laplacian: \\text{Laplician} = \\text{gradD} - \\text{gradE}","title":"5. Nonlinear Laplacian"},{"location":"04_morphology/#6-skeletonization","text":"Here we implement the Zhang-Suen skeletonization algorithm using the CImg library in C++. Very Important Note : this method requires that the input image is binary (i.e., pixels are either 0 or 1).","title":"6. Skeletonization"},{"location":"04_morphology/#iterative-skeletonization-function","text":"The main function for skeletonization is IterSkeleton . This function takes a binary image as input and performs two passes, removing certain edge pixels in each pass.","title":"Iterative Skeletonization Function"},{"location":"04_morphology/#initializing-variables","text":"CImg<unsigned char> D(imgIn.width(), imgIn.height(), 1, 1, 0); CImg_3x3(N, unsigned char); Here, we use D to tag pixels for removal. Whenever a pixel is tagged, we set its value to 1. Note that CImg_3x3 has the macro format CImg_3x3(I,T) : #define CImg_3x3(I,T) T I[9]; \\ T& I##pp = I[0]; T& I##cp = I[1]; T& I##np = I[2]; \\ T& I##pc = I[3]; T& I##cc = I[4]; T& I##nc = I[5]; \\ T& I##pn = I[6]; T& I##cn = I[7]; T& I##nn = I[8]; \\ I##pp = I##cp = I##np = \\ I##pc = I##cc = I##nc = \\ I##pn = I##cn = I##nn = 0 So that in the subsequent code, we can use N to access the 8 neighbors of a pixel. The variables Npp, Ncp, Nnp, Npc, Nnc, Npn, Ncn, Nnn represent the 8 neighboring pixels around the central pixel (x, y) . The variable names correspond to their relative positions: Npp : Previous row, previous column. Ncp : Current row, previous column. Nnp : Next row, previous column. Npc : Previous row, current column. Nnc : Next row, current column. Npn : Previous row, next column. Ncn : Current row, next column. Nnn : Next row, next column. The central pixel itself is referred to as Ncc .","title":"Initializing Variables"},{"location":"04_morphology/#pass-1","text":"The first pass checks each pixel and its 8 neighbors to determine if it should be removed: B = N_{pp} + N_{cp} + N_{np} + N_{pc} + N_{nc} + N_{pn} + N_{cn} + N_{nn} C = N_{nc} \\cdot (N_{nc} - N_{np}) + N_{np} \\cdot (N_{np} - N_{cp}) + \\cdots + N_{nn} \\cdot (N_{nn} - N_{nc}) Here, C is the the number of 0-1 transitions in the 8-neighborhood of the pixel. The book's description \"C(N), called connectivity number, expressing how many binary components are connected to the central pixel (x,y)\" appears to be incorrect. The conditions for removal are: R1 = \\left( B \\geq 2 \\right) \\land \\left( B \\leq 6 \\right) \\land \\left( C = 1 \\right) \\land \\left( N_{cn} \\cdot N_{nc} \\cdot N_{cp} = 0 \\right) \\land \\left( N_{pc} \\cdot N_{cn} \\cdot N_{nc} = 0 \\right) Intuitively, this means that the pixel should be removed if: It has 2-6 neighbors. (Because if B is 0 or 1, then the pixel is an end point or an isolated point.) It has exactly 1 0-1 transition in its 8-neighborhood because it is likely to be a boundary pixel. N_{cn} \\cdot N_{nc} \\cdot N_{cp} = 0 and N_{pc} \\cdot N_{cn} \\cdot N_{nc} = 0 for R1 , and N_{nc} \\cdot N_{cp} \\cdot N_{pc} = 0 and N_{cp} \\cdot N_{pc} \\cdot N_{cn} = 0 for R2 : These are specific conditions for Zhang-Suen thinning. They ensure that no spur pixels (pixels that stick out from the object) are created during the thinning process. For R1 , these conditions make sure that the pixel is not an endpoint in the south and east directions, and that removing it won't break connectivity. The conditions for R2 (see below) do a similar thing, but in the north and west directions. By separating the thinning process into two stages, the algorithm avoids the possibility of over-thinning or under-thinning. // Pass 1 int n1 = 0; cimg_for3x3(imgIn, x, y, 0, 0, N, unsigned char) { if (imgIn(x, y)) { // Compute B and C here... bool R1 = B >= 2 && B <= 6 && C == 1 && Ncn * Nnc * Ncp == 0 && Npc * Ncn * Nnc == 0; if (R1) { // Tag (x,y) D(x, y) = 1; ++n1; } } }","title":"Pass 1"},{"location":"04_morphology/#removing-tagged-pixels-from-pass-1","text":"cimg_forXY(imgIn, x, y) imgIn(x, y) -= (n1 > 0) * D(x, y);","title":"Removing Tagged Pixels from Pass 1"},{"location":"04_morphology/#pass-2","text":"The second pass is similar to the first, but with different conditions for removal: R2 = \\left( B \\geq 2 \\right) \\land \\left( B \\leq 6 \\right) \\land \\left( C = 1 \\right) \\land \\left( N_{nc} \\cdot N_{cp} \\cdot N_{pc} = 0 \\right) \\land \\left( N_{cp} \\cdot N_{pc} \\cdot N_{cn} = 0 \\right)","title":"Pass 2"},{"location":"04_morphology/#removing-tagged-pixels-from-pass-2","text":"Similar to the removal in Pass 1.","title":"Removing Tagged Pixels from Pass 2"},{"location":"04_morphology/#return-the-total-number-of-removed-pixels","text":"return n1 + n2;","title":"Return the Total Number of Removed Pixels"},{"location":"04_morphology/#iterative-skeletonization","text":"int num_removed; do { num_removed = IterSkeleton(lum); } while (num_removed > 0); Here, we keep calling IterSkeleton until no pixels are removed in a pass.","title":"Iterative Skeletonization"},{"location":"04_morphology/#results","text":"Original : Skeleton :","title":"Results"},{"location":"05_filtering/","text":"Filtering - Learning Reflection Author : Tony Fu Date : August 21, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 5 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. Convolution CImg<> sobel(3, 3, 1, 1, 0); sobel(0, 0) = -1; sobel(0, 1) = -2; sobel(0, 2) = -1; sobel(1, 0) = 0; sobel(1, 1) = 0; sobel(1, 2) = 0; sobel(2, 0) = 1; sobel(2, 1) = 2; sobel(2, 2) = 1; imgIn.convolve(sobel); The above code snippet shows how to perform a convolution on an image with a 3x3 Sobel filter. The result is shown below: Original : Convolution : Boundary Conditions Boundary conditions specify how to handle the edges and can be specified using the const usigned int boundary_conditions parameter of the convolve() method. The four boundary conditions provided by the CImg library have specific meanings: Dirichlet (0): The pixels outside the image boundaries are considered to be zero. This creates a sort of \"hard\" edge around the image and can lead to noticeable artifacts along the borders. Neumann (1) (default): The value of the border pixels is extended outside the image boundaries. Essentially, this reflects the gradient at the border, assuming that the intensity of the image doesn't change beyond the edge. This is the default boundary condition in CImg and tends to provide visually acceptable results. Periodic (2): The image is treated as if it were tiling the plane in a repeated pattern. This means that the pixels on the right edge of the image are used as the boundary condition for the left edge, and the pixels on the bottom are used for the top. This can create seamless transitions but can also lead to strange effects if the image does not naturally tile. Mirror (3): The pixels outside the image boundaries are determined by mirroring the pixels inside the boundaries. Imagine folding the image over at its edges, so the pixels just inside the border are duplicated just outside the border. This can create a more visually smooth transition at the edges but may not be appropriate for all types of images. 2. Median Fitler img.blur_median(3); The above code snippet shows how to perform a median filter on an image with a 3x3 window. The result is shown below: Origin : Median Filter : Order-Statistic (OS) Filter An OS filter (Bovik, Huang, and Munson, 1983) is a non-linear filter that computes a linear combination of these sorted values: F = w_1 \\cdot s_1 + w_2 \\cdot s_2 + \\ldots + w_n \\cdot s_n where w_i are the weights that define how much each ordered value contributes to the final result, and s_i are the sorted values of the neighborhood pixels. 3. First-Order Derivatives The get_gradient function computes the image gradient along specified axes using different numerical schemes. Parameters axes : Axes considered for the gradient computation (e.g., \"xy\"). scheme : Numerical scheme used for the gradient computation. Options are: Schemes Backward Finite Differences ( scheme = -1 ) Computes the gradient using backward finite differences: \\text{grad}[pos] = \\text{data}[pos] - \\text{data}[pos - \\text{off}] Central Finite Differences ( scheme = 0 ) Computes the gradient using central finite differences: \\text{grad}[pos] = \\frac{\\text{data}[pos + \\text{off}] - \\text{data}[pos - \\text{off}]}{2} Forward Finite Differences ( scheme = 1 ) Computes the gradient using forward finite differences: \\text{grad}[pos] = \\text{data}[pos + \\text{off}] - \\text{data}[pos] Sobel Scheme ( scheme = 2 ) Utilizes Sobel operators to compute the gradient. Rotation Invariant Scheme ( scheme = 3 ) Uses a rotation-invariant kernels: \\text{Rotation-Invariant}_{x} = \\begin{bmatrix} -a & -b & -a \\\\ 0 & 0 & 0 \\\\ a & b & a \\end{bmatrix}\\\\ \\text{Rotation-Invariant}_{y} = \\text{Rotation-Invariant}_{x}^T where a = 0.25 \\times (2 - \\sqrt{2}) and b = 0.5 \\times (\\sqrt{2} - 1) . Deriche Recursive Filter : Introduced later. Van Vliet Recursive Filter : Introduced later. Scheme Applications Pros Cons Backward finite differences General-purpose Simple, easy to implement Less accurate, sensitive to noise Centered finite differences General-purpose More accurate than forward/backward Sensitive to noise Forward finite differences General-purpose Simple, easy to implement Less accurate, sensitive to noise Using Sobel kernels Edge detection Good at capturing edges, less noisy Can miss fine details Using rotation invariant Edge detection, texture analysis Rotation invariant, captures subtle edges More computationally expensive Using Deriche recursive filter Smoothing, edge detection Smooths noise, good edge detection Computationally expensive Using Van Vliet recursive filter Smoothing, edge detection Smooths noise, efficient computation Might blur some edges Example // Gradient approximation using centered finite differences. CImgList<> grad = imageIn.get_gradient(); // Norm and phase of the gradient. CImg<> norm = (grad[0].get_sqr() + grad[1].get_sqr()).sqrt(), phi = grad[1].get_atan2(grad[0]); Original Gradient X Gradient Y Gradient Norm Gradient Phase 4. Second-Order Derivatives Second-order derivatives are useful for detecting edges (when combined with thresholding and non-maximum suppression). However, they are more commonly used for feature detection, a topic that will be covered in Chapter 6. 4.1 Laplacian The Laplacian operator calculates the divergence of the gradient of the image, effectively highlighting regions where there is a rapid change in intensity. void Laplacian(CImg<> &imageIn) { CImg<> laplacian = imageIn.get_laplacian(); laplacian.normalize(0, 255).save(\"./results/lighthouse_laplacian.png\"); } Mathematically, it is represented as: \\nabla^2 f = \\frac{{\\partial^2 f}}{{\\partial x^2}} + \\frac{{\\partial^2 f}}{{\\partial y^2}} 4.2 Hessian The Hessian matrix consists of the second-order partial derivatives of the image. void Hessian(CImg<> &imageIn) { CImg<> Ixx = imageIn.get_hessian(\"xx\")[0]; // ... rest of the code ... } It is mathematically expressed as: \\mathbf{H} = \\begin{bmatrix} \\frac{{\\partial^2 f}}{{\\partial x^2}} & \\frac{{\\partial^2 f}}{{\\partial x \\partial y}} \\\\ \\frac{{\\partial^2 f}}{{\\partial y \\partial x}} & \\frac{{\\partial^2 f}}{{\\partial y^2}} \\end{bmatrix} Hessian XX Hessian YY Hessian XY 4.3 LoG (Laplacian of Gaussian) LoG combines Gaussian smoothing with the Laplacian operator. void LoG(CImg<> &imageIn) { CImg<> log = imageIn.get_blur(2).laplacian(); // ... rest of the code ... } The expression for LoG is: \\nabla^2 (G * f) = \\nabla^2 G * f where G is the Gaussian function. 4.4 DoG (Difference of Gaussian) DoG approximates the LoG by taking the difference between two blurred images with different standard deviations. void DoG(CImg<> &imageIn) { CImg<> gauss1 = imageIn.get_blur(1); CImg<> gauss2 = imageIn.get_blur(2); CImg<> dog = gauss1 - gauss2; // ... rest of the code ... } Mathematically, DoG is represented as: \\text{DoG} = (G_{\\sigma_1} * f) - (G_{\\sigma_2} * f) where G_{\\sigma_1} and G_{\\sigma_2} are Gaussian functions with standard deviations \\sigma_1 and \\sigma_2 , respectively. Summary 2nd-Order Derivative Applications Pros Cons Laplacian Edge Detection Sensitive to edges, Simple computation Noisy, Sensitive to noise Hessian Feature Detection Captures second-order information, Rich features Computationally expensive LoG (Laplacian of Gaussian) Edge Detection, Feature Detection Reduces noise, Effective edge detection Slower than DoG DoG (Difference of Gaussian) Edge Detection, Approximation of LoG Faster approximation of LoG Less accurate than LoG 5. Adaptive Filtering (Sigma Filter) Traditional smoothing filters often blur the edges along with reducing noise, causing a loss of important details. On the other hand, adaptive or other nonlinear filters like the sigma filter promise to reduce noise in images while preserving edges and contours. The method is based on the principle of weighting the influence of neighboring pixels based on the gradient, similar to some normalization or shunting mechanisms found in neuroscience. 1. Gradient Calculation First, the code calculates the gradient of the input image using the get_gradient() method: CImgList<> g = imgIn.get_gradient(); CImg<> grad = (g[0].get_sqr() + g[1].get_sqr()).sqrt(); The gradient, \\nabla f , quantifies the rate of change in pixel values across the image and is given by the formula: \\nabla f = \\sqrt{{\\left(\\frac{{\\partial f}}{{\\partial x}}\\right)}^2 + {\\left(\\frac{{\\partial f}}{{\\partial y}}\\right)}^2} 2. Sum of Gradients Next, the sum of gradients in a 3x3 neighborhood is computed: CImg<> Sgrad = grad.get_convolve(CImg<>(3, 3, 1, 1, 1)); This step convolves the gradient with a 3x3 filter, summing the neighboring gradients, effectively measuring local variations in pixel intensities. 3. Adaptive Weighting The code then applies adaptive weighting using the following lines: float epsilon = 100; CImg<> rap = imgIn.get_div(grad + epsilon); Here, the division acts as a weighting coefficient, with the epsilon term preventing division by zero. The weight of a pixel in the sum is inversely proportional to the local gradient: \\text{{weight}} = \\frac{{f}}{{\\nabla f + \\epsilon}} In rap , the pixels with high gradients are weighted less, but in this case, less actually means darker pixels! So, this code is emphasizing the edges and contours of the image. We started with this image: And ended up with this: 4. Smoothing Operation The smoothing operation is performed in the following lines: CImg_3x3(I, float); // declare Ipp, Ipc, etc. cimg_for3x3(rap, x, y, 0, 0, I, float) imgOut(x, y) = (Ipp + Ipc + Ipn + Icp + Icc + Icn + Inp + Inc + Inn) / (Sgrad(x, y) + epsilon); Here, CImg_3x3(I, float); declares variables like Ipp , Ipc , etc., representing the neighboring pixels. The cimg_for3x3 macro iterates through the image, applying the smoothing operation. The numerator is a simple average (actually sum) filter, and the bottom term is larger for pixels with high gradients. Therefore, we emphasize the edges and contours again by making those pixels darker. The final formula for smoothing is: \\text{{imgOut}}(x, y) = \\frac{{\\sum \\text{{neighboring pixels}}}}{{\\text{{Sgrad}}(x, y) + \\epsilon}} As you can see, the noise are amplified along with the edges are emphasized. This adaptive sigma filter may not be the best choice for this image. The somewhat cartoon-like appearance of the final output image is a common effect of adaptive smoothing techniques like the sigma filter. By emphasizing edges and smoothing uniform areas, the image can take on a more stylized or abstract appearance. Connection to Neuroscience The adaptive nature of this method is akin to the way some neurons modulate their response based on local activity. It aligns with principles observed in neuroscience where the influence of neighboring neurons is normalized or shunted based on the local context, allowing for a balance between sensitivity to stimuli and adaptation to the local environment. 6. Adaptive Window Filters Adaptive window filters are smart filters that change their behavior based on the characteristics of the area they are working on. They look at each pixel and decide the best way to smooth or sharpen it based on the pixels around it. There are different ways to do this, and here are three examples using the following noisy image (Gaussian noise \\sigma = 40 ): 6.1 Nagao Filter Imagine you have a small grid (usually 5x5) around a pixel in the middle. In this grid, you create 9 different windows (smaller groups of pixels), each containing 9 pixels. Here we first intialize the windows: CImgList<unsigned char> Nagao(9, 5, 5, 1, 1, 0); Nagao(0, 0, 0) = Nagao(0, 0, 1) = Nagao(0, 0, 2) = Nagao(0, 0, 3) = Nagao(0, 0, 4) = Nagao(0, 1, 1) = Nagao(0, 1, 2) = Nagao(0, 1, 3) = Nagao(0, 2, 2) = 1; for (int i = 1; i < 4; ++i) Nagao[i] = Nagao[0].get_rotate(i * 90); Nagao(4, 1, 1) = Nagao(4, 1, 2) = Nagao(4, 1, 3) = Nagao(4, 2, 1) = Nagao(4, 2, 2) = Nagao(4, 2, 3) = Nagao(4, 3, 1) = Nagao(4, 3, 2) = Nagao(4, 3, 3) = 1; Nagao(5, 0, 0) = Nagao(5, 0, 1) = Nagao(5, 0, 2) = Nagao(5, 1, 0) = Nagao(5, 1, 1) = Nagao(5, 1, 2) = Nagao(5, 2, 0) = Nagao(5, 2, 1) = Nagao(5, 2, 2) = 1; for (int i = 1; i < 4; ++i) Nagao[5 + i] = Nagao[5].get_rotate(i * 90); For each window: You calculate the average color (mean) and how much the colors vary (variance). You pick the window where the colors vary the least (smallest variance). You replace the middle pixel with the average color from that chosen window. Here is the code: CImg<> mu(9, 1, 1, 1, 0), sigma(9, 1, 1, 1, 0), st, N(5, 5); CImg<int> permutations; cimg_for5x5(imgIn, x, y, 0, 0, N, float) { CImgList<> res(9); for (int i = 0; i < 9; ++i) { res[i] = N.get_mul(Nagao[i]); st = res[i].get_stats(); mu[i] = st[2]; sigma[i] = st[3]; } // Searching minimal variance. sigma.sort(permutations); imgOut(x, y) = mu[permutations[0]]; } This method helps to keep the details in the image while reducing noise. 6.2 Kuwahara Filter The Kuwahara filter is like the Nagao filter, but only uses windows 5 - 8 of Nagao filter. It's just a variation that might work better on certain types of images. The basic idea of finding the least varying window and using its average color remains the same. 7. Deriche Recursive Filters John Canny's work on edge detection is not only confined to the Canny edge detector but also includes mathematical foundations for defining the criteria of an effective edge detector. Canny outlined three criteria: good detection, good localization, and a single response. The first two criteria can be combined to yield a so-called Canny criterion value , adding mathematical rigor to the field of edge detection. The Deriche recursive filter is an recursive solution to Canny's criteria. However, in the book, Deriche filter is presented as a family of recursive (and potentially more efficient) alternatives to smoothing operation (0-th order), gradient computation (1st order), and Laplacian computation (2nd order). The function CImg<T>& deriche(const float sigma, const unsigned int order=0, const char axis='x') filter applies in one direction at once. sigma is the standard deviation of the filter, while order is the order of the derivative to compute. axis is the axis along which the filter is applied. 7.1 Smoothing (0-th Order) CImg<> img_deriche0 = imgIn.get_deriche(SIGMA, 0, 'x'); img_deriche0.deriche(SIGMA, 0, 'y'); 7.2 Gradient Computation (1st Order) CImg<> img_deriche1 = imgIn.get_deriche(SIGMA, 1, 'x'); img_deriche1.deriche(SIGMA, 1, 'y'); CImg<> img_deriche1_norm = (img_deriche1.get_sqr() += img_deriche1.get_sqr()).sqrt(); 7.3 Laplacian Computation (2nd Order) CImg<> img_deriche2_x = imgIn.get_deriche(SIGMA, 2, 'x'); CImg<> img_deriche2_y = imgIn.get_deriche(SIGMA, 2, 'y'); CImg<> img_deriche2_laplacian = img_deriche2_x + img_deriche2_y; 8. Frequency Domain Filtering 8.1 Using CImg<>::FFT() Load the image and convert to grayscale CImg<unsigned char> img(\"../images/lighthouse.png\"); CImg<> lum = img.get_norm().blur(0.75f); Resize the image FFT requires dimensions to be a power of 2, so resize the image to meet this requirement. int width = 1 << static_cast<int>(std::ceil(std::log2(lum.width()))); int height = 1 << static_cast<int>(std::ceil(std::log2(lum.height()))); lum.resize(width, height, -100, -100, 0); Compute the FFT CImgList<> fft = lum.get_FFT(); Process Magnitude Take the logarithm of the magnitude part to better visualize it, and then shift the zero frequency component to the center of the spectrum. CImg<> magnitude(fft[0]); magnitude += 1; // Avoid log(0) magnitude.log(); magnitude.shift(magnitude.width() / 2, magnitude.height() / 2, 0, 0, 2); (Optional) Compute Inverse FFT Perform the inverse FFT to recover the original image. CImg<> img_ifft = fft.get_FFT(true)[0]; 8.2 Butterworth Filters The idea of frequency-domain filtering involves first transforming the image into the frequency domain, then multiplying the image with a mask, and finally transforming the image back to the spatial domain. Theoretically, this is equivalent to convolving the image with the mask in the spatial domain. It might sound like a lot of extra work, but the Fast Fourier Transform (FFT) makes it much faster. However, you need to be mindful of artifacts such as the Gibbs phenomenon, which can be reduced by using appropriate windowing functions. Butterworth filters are a family of filters characterized by a maximally flat frequency response. They can be applied in both analog and digital forms, and in both time and frequency domains. In the analog frequency domain, the 2D transfer function for a Butterworth low-pass filter can be represented as: H(u,v) = \\frac{1}{1 + \\left( \\frac{D(u,v)}{D_0} \\right)^{2n}} where D(u,v) = \\sqrt{u^2 + v^2} is the distance from the origin in the frequency domain, and D_0 is the cutoff frequency. The parameter n is the order of the filter. Increasing n will result in a steeper roll-off, but at the expense of increased complexity, potential instability, and possible phase distortion. In the spatial domain, you would have to use the inverse Fourier transform to obtain a corresponding difference equation. 8.3 Gaussian Filters The Gaussian filter can be implemented in both time and frequency domains Here's how to apply a Gaussian filter in the frequency domain: Perform the Fast Fourier Transform (FFT) First, compute the FFT of the input image. CImgList<> fImg = imgIn.get_FFT(); Create the Gaussian Mask Construct the frequency response of the filter using the Gaussian function. The Gaussian mask is defined in the frequency domain, and sigma is the standard deviation controlling the spread of the Gaussian function. In the frequency domain, the filter is described by the Gaussian function: H(u, v) = 2\\pi \\sigma^2 \\exp\\left(-2\\pi^2\\sigma^2\\left(\\left(\\frac{u}{W} - 0.5\\right)^2 + \\left(\\frac{v}{H} - 0.5\\right)^2\\right)\\right) Here, W and H are the width and height of the image, \\sigma^2 is the squared standard deviation, and (u,v) are the frequency coordinates. CImg<> gaussMask(imgIn.width(), imgIn.height()); float sigma2 = cimg::sqr(sigma); cimg_forXY(gaussMask, x, y) { float fx = x / (float)imgIn.width() - 0.5f, fx2 = cimg::sqr(fx), fy = y / (float)imgIn.height() - 0.5f, fy2 = cimg::sqr(fy); gaussMask(x, y) = 2 * cimg::PI * sigma2 * std::exp(-2 * cimg::sqr(cimg::PI) * sigma2 * (fx2 + fy2)); } Zero Shift the Gaussian Mask Shift the Gaussian mask by half its width and height to center the zero frequency. // Zero shift. gaussMask.shift(-imgIn.width() / 2, -imgIn.height() / 2, 0, 0, 2); Apply the Filter Perform the element-wise multiplication of the Fourier Transformed image and the Gaussian mask for both the magnitude and phase components. cimglist_for(fImg, k) fImg[k].mul(gaussMask); Inverse FFT and Normalize Transform back to the spatial domain via inverse FFT and normalize the result. // Inverse FFT and real part. return fImg.get_FFT(true)[0].normalize(0, 255); 9. Diffusion Filtering The term \"diffusion\" in the context of diffusion filtering is analogous to the physical process of diffusion, but with pixel values rather than physical particles. The idea is that the pixel values in an image will \"diffuse\" from areas of high intensity to areas of low intensity, smoothing the image. However, unlike simple linear filtering, diffusion filtering can be controlled to be anisotropic , meaning it can prefer certain directions over others. This allows the diffusion to be guided by the content of the image, smoothing some areas while preserving edges and details. The diffusion filter introduced in the book, Perona and Malik's algorithm, is nonlinear and adaptive. ( Note: nonlinearity does not necessarily guarantee adaptivity, nor the other way around. Filters can be simultaneously linear and adaptive, like the least mean squares (LMS) filter, or nonlinear but non-adaptive, like the median filter. )","title":"5. Filtering"},{"location":"05_filtering/#filtering-learning-reflection","text":"Author : Tony Fu Date : August 21, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 5 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Filtering - Learning Reflection"},{"location":"05_filtering/#1-convolution","text":"CImg<> sobel(3, 3, 1, 1, 0); sobel(0, 0) = -1; sobel(0, 1) = -2; sobel(0, 2) = -1; sobel(1, 0) = 0; sobel(1, 1) = 0; sobel(1, 2) = 0; sobel(2, 0) = 1; sobel(2, 1) = 2; sobel(2, 2) = 1; imgIn.convolve(sobel); The above code snippet shows how to perform a convolution on an image with a 3x3 Sobel filter. The result is shown below: Original : Convolution :","title":"1. Convolution"},{"location":"05_filtering/#boundary-conditions","text":"Boundary conditions specify how to handle the edges and can be specified using the const usigned int boundary_conditions parameter of the convolve() method. The four boundary conditions provided by the CImg library have specific meanings: Dirichlet (0): The pixels outside the image boundaries are considered to be zero. This creates a sort of \"hard\" edge around the image and can lead to noticeable artifacts along the borders. Neumann (1) (default): The value of the border pixels is extended outside the image boundaries. Essentially, this reflects the gradient at the border, assuming that the intensity of the image doesn't change beyond the edge. This is the default boundary condition in CImg and tends to provide visually acceptable results. Periodic (2): The image is treated as if it were tiling the plane in a repeated pattern. This means that the pixels on the right edge of the image are used as the boundary condition for the left edge, and the pixels on the bottom are used for the top. This can create seamless transitions but can also lead to strange effects if the image does not naturally tile. Mirror (3): The pixels outside the image boundaries are determined by mirroring the pixels inside the boundaries. Imagine folding the image over at its edges, so the pixels just inside the border are duplicated just outside the border. This can create a more visually smooth transition at the edges but may not be appropriate for all types of images.","title":"Boundary Conditions"},{"location":"05_filtering/#2-median-fitler","text":"img.blur_median(3); The above code snippet shows how to perform a median filter on an image with a 3x3 window. The result is shown below: Origin : Median Filter :","title":"2. Median Fitler"},{"location":"05_filtering/#order-statistic-os-filter","text":"An OS filter (Bovik, Huang, and Munson, 1983) is a non-linear filter that computes a linear combination of these sorted values: F = w_1 \\cdot s_1 + w_2 \\cdot s_2 + \\ldots + w_n \\cdot s_n where w_i are the weights that define how much each ordered value contributes to the final result, and s_i are the sorted values of the neighborhood pixels.","title":"Order-Statistic (OS) Filter"},{"location":"05_filtering/#3-first-order-derivatives","text":"The get_gradient function computes the image gradient along specified axes using different numerical schemes.","title":"3. First-Order Derivatives"},{"location":"05_filtering/#parameters","text":"axes : Axes considered for the gradient computation (e.g., \"xy\"). scheme : Numerical scheme used for the gradient computation. Options are:","title":"Parameters"},{"location":"05_filtering/#schemes","text":"Backward Finite Differences ( scheme = -1 ) Computes the gradient using backward finite differences: \\text{grad}[pos] = \\text{data}[pos] - \\text{data}[pos - \\text{off}] Central Finite Differences ( scheme = 0 ) Computes the gradient using central finite differences: \\text{grad}[pos] = \\frac{\\text{data}[pos + \\text{off}] - \\text{data}[pos - \\text{off}]}{2} Forward Finite Differences ( scheme = 1 ) Computes the gradient using forward finite differences: \\text{grad}[pos] = \\text{data}[pos + \\text{off}] - \\text{data}[pos] Sobel Scheme ( scheme = 2 ) Utilizes Sobel operators to compute the gradient. Rotation Invariant Scheme ( scheme = 3 ) Uses a rotation-invariant kernels: \\text{Rotation-Invariant}_{x} = \\begin{bmatrix} -a & -b & -a \\\\ 0 & 0 & 0 \\\\ a & b & a \\end{bmatrix}\\\\ \\text{Rotation-Invariant}_{y} = \\text{Rotation-Invariant}_{x}^T where a = 0.25 \\times (2 - \\sqrt{2}) and b = 0.5 \\times (\\sqrt{2} - 1) . Deriche Recursive Filter : Introduced later. Van Vliet Recursive Filter : Introduced later. Scheme Applications Pros Cons Backward finite differences General-purpose Simple, easy to implement Less accurate, sensitive to noise Centered finite differences General-purpose More accurate than forward/backward Sensitive to noise Forward finite differences General-purpose Simple, easy to implement Less accurate, sensitive to noise Using Sobel kernels Edge detection Good at capturing edges, less noisy Can miss fine details Using rotation invariant Edge detection, texture analysis Rotation invariant, captures subtle edges More computationally expensive Using Deriche recursive filter Smoothing, edge detection Smooths noise, good edge detection Computationally expensive Using Van Vliet recursive filter Smoothing, edge detection Smooths noise, efficient computation Might blur some edges","title":"Schemes"},{"location":"05_filtering/#example","text":"// Gradient approximation using centered finite differences. CImgList<> grad = imageIn.get_gradient(); // Norm and phase of the gradient. CImg<> norm = (grad[0].get_sqr() + grad[1].get_sqr()).sqrt(), phi = grad[1].get_atan2(grad[0]); Original Gradient X Gradient Y Gradient Norm Gradient Phase","title":"Example"},{"location":"05_filtering/#4-second-order-derivatives","text":"Second-order derivatives are useful for detecting edges (when combined with thresholding and non-maximum suppression). However, they are more commonly used for feature detection, a topic that will be covered in Chapter 6.","title":"4. Second-Order Derivatives"},{"location":"05_filtering/#41-laplacian","text":"The Laplacian operator calculates the divergence of the gradient of the image, effectively highlighting regions where there is a rapid change in intensity. void Laplacian(CImg<> &imageIn) { CImg<> laplacian = imageIn.get_laplacian(); laplacian.normalize(0, 255).save(\"./results/lighthouse_laplacian.png\"); } Mathematically, it is represented as: \\nabla^2 f = \\frac{{\\partial^2 f}}{{\\partial x^2}} + \\frac{{\\partial^2 f}}{{\\partial y^2}}","title":"4.1 Laplacian"},{"location":"05_filtering/#42-hessian","text":"The Hessian matrix consists of the second-order partial derivatives of the image. void Hessian(CImg<> &imageIn) { CImg<> Ixx = imageIn.get_hessian(\"xx\")[0]; // ... rest of the code ... } It is mathematically expressed as: \\mathbf{H} = \\begin{bmatrix} \\frac{{\\partial^2 f}}{{\\partial x^2}} & \\frac{{\\partial^2 f}}{{\\partial x \\partial y}} \\\\ \\frac{{\\partial^2 f}}{{\\partial y \\partial x}} & \\frac{{\\partial^2 f}}{{\\partial y^2}} \\end{bmatrix} Hessian XX Hessian YY Hessian XY","title":"4.2 Hessian"},{"location":"05_filtering/#43-log-laplacian-of-gaussian","text":"LoG combines Gaussian smoothing with the Laplacian operator. void LoG(CImg<> &imageIn) { CImg<> log = imageIn.get_blur(2).laplacian(); // ... rest of the code ... } The expression for LoG is: \\nabla^2 (G * f) = \\nabla^2 G * f where G is the Gaussian function.","title":"4.3 LoG (Laplacian of Gaussian)"},{"location":"05_filtering/#44-dog-difference-of-gaussian","text":"DoG approximates the LoG by taking the difference between two blurred images with different standard deviations. void DoG(CImg<> &imageIn) { CImg<> gauss1 = imageIn.get_blur(1); CImg<> gauss2 = imageIn.get_blur(2); CImg<> dog = gauss1 - gauss2; // ... rest of the code ... } Mathematically, DoG is represented as: \\text{DoG} = (G_{\\sigma_1} * f) - (G_{\\sigma_2} * f) where G_{\\sigma_1} and G_{\\sigma_2} are Gaussian functions with standard deviations \\sigma_1 and \\sigma_2 , respectively.","title":"4.4 DoG (Difference of Gaussian)"},{"location":"05_filtering/#summary","text":"2nd-Order Derivative Applications Pros Cons Laplacian Edge Detection Sensitive to edges, Simple computation Noisy, Sensitive to noise Hessian Feature Detection Captures second-order information, Rich features Computationally expensive LoG (Laplacian of Gaussian) Edge Detection, Feature Detection Reduces noise, Effective edge detection Slower than DoG DoG (Difference of Gaussian) Edge Detection, Approximation of LoG Faster approximation of LoG Less accurate than LoG","title":"Summary"},{"location":"05_filtering/#5-adaptive-filtering-sigma-filter","text":"Traditional smoothing filters often blur the edges along with reducing noise, causing a loss of important details. On the other hand, adaptive or other nonlinear filters like the sigma filter promise to reduce noise in images while preserving edges and contours. The method is based on the principle of weighting the influence of neighboring pixels based on the gradient, similar to some normalization or shunting mechanisms found in neuroscience.","title":"5. Adaptive Filtering (Sigma Filter)"},{"location":"05_filtering/#1-gradient-calculation","text":"First, the code calculates the gradient of the input image using the get_gradient() method: CImgList<> g = imgIn.get_gradient(); CImg<> grad = (g[0].get_sqr() + g[1].get_sqr()).sqrt(); The gradient, \\nabla f , quantifies the rate of change in pixel values across the image and is given by the formula: \\nabla f = \\sqrt{{\\left(\\frac{{\\partial f}}{{\\partial x}}\\right)}^2 + {\\left(\\frac{{\\partial f}}{{\\partial y}}\\right)}^2}","title":"1. Gradient Calculation"},{"location":"05_filtering/#2-sum-of-gradients","text":"Next, the sum of gradients in a 3x3 neighborhood is computed: CImg<> Sgrad = grad.get_convolve(CImg<>(3, 3, 1, 1, 1)); This step convolves the gradient with a 3x3 filter, summing the neighboring gradients, effectively measuring local variations in pixel intensities.","title":"2. Sum of Gradients"},{"location":"05_filtering/#3-adaptive-weighting","text":"The code then applies adaptive weighting using the following lines: float epsilon = 100; CImg<> rap = imgIn.get_div(grad + epsilon); Here, the division acts as a weighting coefficient, with the epsilon term preventing division by zero. The weight of a pixel in the sum is inversely proportional to the local gradient: \\text{{weight}} = \\frac{{f}}{{\\nabla f + \\epsilon}} In rap , the pixels with high gradients are weighted less, but in this case, less actually means darker pixels! So, this code is emphasizing the edges and contours of the image. We started with this image: And ended up with this:","title":"3. Adaptive Weighting"},{"location":"05_filtering/#4-smoothing-operation","text":"The smoothing operation is performed in the following lines: CImg_3x3(I, float); // declare Ipp, Ipc, etc. cimg_for3x3(rap, x, y, 0, 0, I, float) imgOut(x, y) = (Ipp + Ipc + Ipn + Icp + Icc + Icn + Inp + Inc + Inn) / (Sgrad(x, y) + epsilon); Here, CImg_3x3(I, float); declares variables like Ipp , Ipc , etc., representing the neighboring pixels. The cimg_for3x3 macro iterates through the image, applying the smoothing operation. The numerator is a simple average (actually sum) filter, and the bottom term is larger for pixels with high gradients. Therefore, we emphasize the edges and contours again by making those pixels darker. The final formula for smoothing is: \\text{{imgOut}}(x, y) = \\frac{{\\sum \\text{{neighboring pixels}}}}{{\\text{{Sgrad}}(x, y) + \\epsilon}} As you can see, the noise are amplified along with the edges are emphasized. This adaptive sigma filter may not be the best choice for this image. The somewhat cartoon-like appearance of the final output image is a common effect of adaptive smoothing techniques like the sigma filter. By emphasizing edges and smoothing uniform areas, the image can take on a more stylized or abstract appearance.","title":"4. Smoothing Operation"},{"location":"05_filtering/#connection-to-neuroscience","text":"The adaptive nature of this method is akin to the way some neurons modulate their response based on local activity. It aligns with principles observed in neuroscience where the influence of neighboring neurons is normalized or shunted based on the local context, allowing for a balance between sensitivity to stimuli and adaptation to the local environment.","title":"Connection to Neuroscience"},{"location":"05_filtering/#6-adaptive-window-filters","text":"Adaptive window filters are smart filters that change their behavior based on the characteristics of the area they are working on. They look at each pixel and decide the best way to smooth or sharpen it based on the pixels around it. There are different ways to do this, and here are three examples using the following noisy image (Gaussian noise \\sigma = 40 ):","title":"6. Adaptive Window Filters"},{"location":"05_filtering/#61-nagao-filter","text":"Imagine you have a small grid (usually 5x5) around a pixel in the middle. In this grid, you create 9 different windows (smaller groups of pixels), each containing 9 pixels. Here we first intialize the windows: CImgList<unsigned char> Nagao(9, 5, 5, 1, 1, 0); Nagao(0, 0, 0) = Nagao(0, 0, 1) = Nagao(0, 0, 2) = Nagao(0, 0, 3) = Nagao(0, 0, 4) = Nagao(0, 1, 1) = Nagao(0, 1, 2) = Nagao(0, 1, 3) = Nagao(0, 2, 2) = 1; for (int i = 1; i < 4; ++i) Nagao[i] = Nagao[0].get_rotate(i * 90); Nagao(4, 1, 1) = Nagao(4, 1, 2) = Nagao(4, 1, 3) = Nagao(4, 2, 1) = Nagao(4, 2, 2) = Nagao(4, 2, 3) = Nagao(4, 3, 1) = Nagao(4, 3, 2) = Nagao(4, 3, 3) = 1; Nagao(5, 0, 0) = Nagao(5, 0, 1) = Nagao(5, 0, 2) = Nagao(5, 1, 0) = Nagao(5, 1, 1) = Nagao(5, 1, 2) = Nagao(5, 2, 0) = Nagao(5, 2, 1) = Nagao(5, 2, 2) = 1; for (int i = 1; i < 4; ++i) Nagao[5 + i] = Nagao[5].get_rotate(i * 90); For each window: You calculate the average color (mean) and how much the colors vary (variance). You pick the window where the colors vary the least (smallest variance). You replace the middle pixel with the average color from that chosen window. Here is the code: CImg<> mu(9, 1, 1, 1, 0), sigma(9, 1, 1, 1, 0), st, N(5, 5); CImg<int> permutations; cimg_for5x5(imgIn, x, y, 0, 0, N, float) { CImgList<> res(9); for (int i = 0; i < 9; ++i) { res[i] = N.get_mul(Nagao[i]); st = res[i].get_stats(); mu[i] = st[2]; sigma[i] = st[3]; } // Searching minimal variance. sigma.sort(permutations); imgOut(x, y) = mu[permutations[0]]; } This method helps to keep the details in the image while reducing noise.","title":"6.1 Nagao Filter"},{"location":"05_filtering/#62-kuwahara-filter","text":"The Kuwahara filter is like the Nagao filter, but only uses windows 5 - 8 of Nagao filter. It's just a variation that might work better on certain types of images. The basic idea of finding the least varying window and using its average color remains the same.","title":"6.2 Kuwahara Filter"},{"location":"05_filtering/#7-deriche-recursive-filters","text":"John Canny's work on edge detection is not only confined to the Canny edge detector but also includes mathematical foundations for defining the criteria of an effective edge detector. Canny outlined three criteria: good detection, good localization, and a single response. The first two criteria can be combined to yield a so-called Canny criterion value , adding mathematical rigor to the field of edge detection. The Deriche recursive filter is an recursive solution to Canny's criteria. However, in the book, Deriche filter is presented as a family of recursive (and potentially more efficient) alternatives to smoothing operation (0-th order), gradient computation (1st order), and Laplacian computation (2nd order). The function CImg<T>& deriche(const float sigma, const unsigned int order=0, const char axis='x') filter applies in one direction at once. sigma is the standard deviation of the filter, while order is the order of the derivative to compute. axis is the axis along which the filter is applied.","title":"7. Deriche Recursive Filters"},{"location":"05_filtering/#71-smoothing-0-th-order","text":"CImg<> img_deriche0 = imgIn.get_deriche(SIGMA, 0, 'x'); img_deriche0.deriche(SIGMA, 0, 'y');","title":"7.1 Smoothing (0-th Order)"},{"location":"05_filtering/#72-gradient-computation-1st-order","text":"CImg<> img_deriche1 = imgIn.get_deriche(SIGMA, 1, 'x'); img_deriche1.deriche(SIGMA, 1, 'y'); CImg<> img_deriche1_norm = (img_deriche1.get_sqr() += img_deriche1.get_sqr()).sqrt();","title":"7.2 Gradient Computation (1st Order)"},{"location":"05_filtering/#73-laplacian-computation-2nd-order","text":"CImg<> img_deriche2_x = imgIn.get_deriche(SIGMA, 2, 'x'); CImg<> img_deriche2_y = imgIn.get_deriche(SIGMA, 2, 'y'); CImg<> img_deriche2_laplacian = img_deriche2_x + img_deriche2_y;","title":"7.3 Laplacian Computation (2nd Order)"},{"location":"05_filtering/#8-frequency-domain-filtering","text":"","title":"8. Frequency Domain Filtering"},{"location":"05_filtering/#81-using-cimgfft","text":"","title":"8.1 Using CImg&lt;&gt;::FFT()"},{"location":"05_filtering/#load-the-image-and-convert-to-grayscale","text":"CImg<unsigned char> img(\"../images/lighthouse.png\"); CImg<> lum = img.get_norm().blur(0.75f);","title":"Load the image and convert to grayscale"},{"location":"05_filtering/#resize-the-image","text":"FFT requires dimensions to be a power of 2, so resize the image to meet this requirement. int width = 1 << static_cast<int>(std::ceil(std::log2(lum.width()))); int height = 1 << static_cast<int>(std::ceil(std::log2(lum.height()))); lum.resize(width, height, -100, -100, 0);","title":"Resize the image"},{"location":"05_filtering/#compute-the-fft","text":"CImgList<> fft = lum.get_FFT();","title":"Compute the FFT"},{"location":"05_filtering/#process-magnitude","text":"Take the logarithm of the magnitude part to better visualize it, and then shift the zero frequency component to the center of the spectrum. CImg<> magnitude(fft[0]); magnitude += 1; // Avoid log(0) magnitude.log(); magnitude.shift(magnitude.width() / 2, magnitude.height() / 2, 0, 0, 2);","title":"Process Magnitude"},{"location":"05_filtering/#optional-compute-inverse-fft","text":"Perform the inverse FFT to recover the original image. CImg<> img_ifft = fft.get_FFT(true)[0];","title":"(Optional) Compute Inverse FFT"},{"location":"05_filtering/#82-butterworth-filters","text":"The idea of frequency-domain filtering involves first transforming the image into the frequency domain, then multiplying the image with a mask, and finally transforming the image back to the spatial domain. Theoretically, this is equivalent to convolving the image with the mask in the spatial domain. It might sound like a lot of extra work, but the Fast Fourier Transform (FFT) makes it much faster. However, you need to be mindful of artifacts such as the Gibbs phenomenon, which can be reduced by using appropriate windowing functions. Butterworth filters are a family of filters characterized by a maximally flat frequency response. They can be applied in both analog and digital forms, and in both time and frequency domains. In the analog frequency domain, the 2D transfer function for a Butterworth low-pass filter can be represented as: H(u,v) = \\frac{1}{1 + \\left( \\frac{D(u,v)}{D_0} \\right)^{2n}} where D(u,v) = \\sqrt{u^2 + v^2} is the distance from the origin in the frequency domain, and D_0 is the cutoff frequency. The parameter n is the order of the filter. Increasing n will result in a steeper roll-off, but at the expense of increased complexity, potential instability, and possible phase distortion. In the spatial domain, you would have to use the inverse Fourier transform to obtain a corresponding difference equation.","title":"8.2 Butterworth Filters"},{"location":"05_filtering/#83-gaussian-filters","text":"The Gaussian filter can be implemented in both time and frequency domains Here's how to apply a Gaussian filter in the frequency domain:","title":"8.3 Gaussian Filters"},{"location":"05_filtering/#perform-the-fast-fourier-transform-fft","text":"First, compute the FFT of the input image. CImgList<> fImg = imgIn.get_FFT();","title":"Perform the Fast Fourier Transform (FFT)"},{"location":"05_filtering/#create-the-gaussian-mask","text":"Construct the frequency response of the filter using the Gaussian function. The Gaussian mask is defined in the frequency domain, and sigma is the standard deviation controlling the spread of the Gaussian function. In the frequency domain, the filter is described by the Gaussian function: H(u, v) = 2\\pi \\sigma^2 \\exp\\left(-2\\pi^2\\sigma^2\\left(\\left(\\frac{u}{W} - 0.5\\right)^2 + \\left(\\frac{v}{H} - 0.5\\right)^2\\right)\\right) Here, W and H are the width and height of the image, \\sigma^2 is the squared standard deviation, and (u,v) are the frequency coordinates. CImg<> gaussMask(imgIn.width(), imgIn.height()); float sigma2 = cimg::sqr(sigma); cimg_forXY(gaussMask, x, y) { float fx = x / (float)imgIn.width() - 0.5f, fx2 = cimg::sqr(fx), fy = y / (float)imgIn.height() - 0.5f, fy2 = cimg::sqr(fy); gaussMask(x, y) = 2 * cimg::PI * sigma2 * std::exp(-2 * cimg::sqr(cimg::PI) * sigma2 * (fx2 + fy2)); }","title":"Create the Gaussian Mask"},{"location":"05_filtering/#zero-shift-the-gaussian-mask","text":"Shift the Gaussian mask by half its width and height to center the zero frequency. // Zero shift. gaussMask.shift(-imgIn.width() / 2, -imgIn.height() / 2, 0, 0, 2);","title":"Zero Shift the Gaussian Mask"},{"location":"05_filtering/#apply-the-filter","text":"Perform the element-wise multiplication of the Fourier Transformed image and the Gaussian mask for both the magnitude and phase components. cimglist_for(fImg, k) fImg[k].mul(gaussMask);","title":"Apply the Filter"},{"location":"05_filtering/#inverse-fft-and-normalize","text":"Transform back to the spatial domain via inverse FFT and normalize the result. // Inverse FFT and real part. return fImg.get_FFT(true)[0].normalize(0, 255);","title":"Inverse FFT and Normalize"},{"location":"05_filtering/#9-diffusion-filtering","text":"The term \"diffusion\" in the context of diffusion filtering is analogous to the physical process of diffusion, but with pixel values rather than physical particles. The idea is that the pixel values in an image will \"diffuse\" from areas of high intensity to areas of low intensity, smoothing the image. However, unlike simple linear filtering, diffusion filtering can be controlled to be anisotropic , meaning it can prefer certain directions over others. This allows the diffusion to be guided by the content of the image, smoothing some areas while preserving edges and details. The diffusion filter introduced in the book, Perona and Malik's algorithm, is nonlinear and adaptive. ( Note: nonlinearity does not necessarily guarantee adaptivity, nor the other way around. Filters can be simultaneously linear and adaptive, like the least mean squares (LMS) filter, or nonlinear but non-adaptive, like the median filter. )","title":"9. Diffusion Filtering"},{"location":"06_feature_extraction/","text":"Feature Extraction - Learning Reflection Author : Tony Fu Date : August 23, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 6 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. Harris-Stephens Corner Detector I love the explanation of the Corner detection by Professor Shree K. Nayar ( Video link ). Here is the summary: The Harris-Stephens corner detection algorithm is designed to identify regions where there are substantial variations in gradient in two distinct directions. While it might be simple to detect corners aligned with the x and y axes, real-world images present corners at various angles. This challenge is addressed by using the determinant, which gives a measure of how well-separated the two gradient directions are. A larger determinant typically signifies a strong presence of two different gradient directions. However, an excessively large axis in one direction could mislead the interpretation. To counterbalance this, the algorithm subtracts a term related to the trace, squared and weighted by a parameter, from the determinant. This penalizes the response value R and aids in distinguishing between genuine corners and edges or flat regions, leading to more accurate corner detection. Here are the steps: 1.1 Gradient Calculation The gradient of the input image is calculated using: \\text{{gradXY}} = \\text{{imgIn.get_gradient()}}; This gets the gradients in the x and y directions. 1.2 Structure Tensor CImg<> Ixx = gradXY[0].get_mul(gradXY[0]).get_convolve(G), Iyy = gradXY[1].get_mul(gradXY[1]).get_convolve(G), Ixy = gradXY[0].get_mul(gradXY[1]).get_convolve(G); The structure tensor is computed as: \\begin{align*} I_{xx} &= \\text{{gradXY[0].get_mul(gradXY[0]).get_convolve(G)}}, \\\\ I_{yy} &= \\text{{gradXY[1].get_mul(gradXY[1]).get_convolve(G)}}, \\\\ I_{xy} &= \\text{{gradXY[0].get_mul(gradXY[1]).get_convolve(G)}}. \\end{align*} where the Gaussian kernel G is defined as: G(x, y) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2 + y^2}{2\\sigma^2}} Together, we can build the structure tensor M as: M = \\begin{bmatrix} I_{xx} & I_{xy} \\\\ I_{xy} & I_{yy} \\end{bmatrix} The determinant of M ( \\det(M) ) and the trace of M ( \\text{trace}(M) ) are computed as follows: \\det(M) = I_{xx} \\cdot I_{yy} - I_{xy} \\cdot I_{xy} \\text{trace}(M) = I_{xx} + I_{yy} The structure tensor M plays a key role in feature detection as it represents the distribution of gradients within a specific neighborhood around a point. Rather than directly comparing the gradient of a pixel with those of its neighbors, we use a Gaussian function to calculate an average gradient across an area. In essence, the structure tensor captures the underlying geometric structure in the vicinity of each pixel. It accomplishes this by portraying gradient orientations as an ellipse in the ( I_x, I_y ) plane within a specific window. Here, the determinant is directly proportional to the area of the ellipse, while the trace is equivalent to the sum of the lengths of the ellipse's major and minor axes. Presence of an edge : When an image contains an edge, the distribution of gradients forms a slender, elongated ellipse. This happens because the intensity changes consistently in one direction (along the edge) and shows little to no change in the direction perpendicular to it. The major axis of this ellipse aligns with the direction of the edge. Presence of a corner : If a corner is present, the gradients are distributed more evenly, resulting in an elliptical shape that resembles a circle. This is because a corner features significant intensity changes in multiple directions. Flat region : In a flat region of the image, where there is minimal change in intensity in any direction, the ellipse is small, signaling the absence of distinctive features. 1.3 R Function Calculation CImg<> det = Ixx.get_mul(Iyy) - Ixy.get_sqr(), trace = Ixx + Iyy, R = det - k * trace.get_sqr(); Often, in the theoretical explanation of the Harris-Stephens corner detection algorithm, we will see the eigenvalues \\lambda_1 and \\lambda_2 are often introduced to provide an intuitive understanding of the underlying geometric properties of the image. However, in the actual implementation, you can compute the response function R directly from the components of the second-moment matrix I_{xx} , I_{yy} , and I_{xy} , without having to explicitly calculate the eigenvalues. It is given as: R = \\det - \\, k \\cdot \\text{{trace}}^2 = (I_{xx} \\cdot I_{yy} - I_{xy}^2) - k \\cdot (I_{xx} + I_{yy})^2 Condition Region Type Explanation R is close to 0 Flat Region No significant change in intensity in any direction, both eigenvalues of the structure tensor are small. R is small Edge Significant change in intensity in one direction but not the other, one large and one small eigenvalue of the structure tensor. R is positive Corner Significant changes in intensity in both directions, both eigenvalues of the structure tensor are large, indicating two dominant and different edge directions. 1.4 Local Maxima Detection CImgList<> imgGradR = R.get_gradient(); CImg_3x3(I, float); CImg<> harrisValues(imgIn.width() * imgIn.height(), 1, 1, 1, 0); CImg<int> harrisXY(imgIn.width() * imgIn.height(), 2, 1, 1, 0), perm(imgIn.width() * imgIn.height(), 1, 1, 1, 0); int nbHarris = 0; cimg_for3x3(R, x, y, 0, 0, I, float) { if (imgGradR[0](x, y) < eps && imgGradR[1](x, y) < eps) { float befx = Ipc - Icc, befy = Icp - Icc, afty = Icn - Icc, aftx = Inc - Icc; if (befx < 0 && befy < 0 && aftx < 0 && afty < 0) { harrisValues(nbHarris) = R(x, y); harrisXY(nbHarris, 0) = x; harrisXY(nbHarris++, 1) = y; } } } Local maxima of the R function are detected. This part of the code finds points that are potential corners. 1.5 Sorting the Corners harrisValues.sort(perm, false); The values are sorted, and the top n corners are drawn on the image. In other implementations, this step is usually replaced by non-maximum suppression. 2. Shi-Tomasi Algorithm Shi-Tomasi algorithm uses similar techniques to compute eigenvalues that represent the local structure of the image, but it applies a different criteria to determine if a region is a corner: R = \\min(\\lambda_1, \\lambda_2) The algorithm is implemented as follows: CImg<> det = Ixx.get_mul(Iyy) - Ixy.get_sqr(), trace = Ixx + Iyy, diff = (trace.get_sqr() - 4 * det).sqrt(), lambda1 = (trace + diff) / 2, lambda2 = (trace - diff) / 2, R = lambda1.min(lambda2); Shi-Tomasi's reliance on the minimum eigenvalue often leads to better detection of true corners. Not sure about this one. 3. Hough Transform Again, I recommend watching Professor Shree K. Nayar's video on the Hough Transform. Here's a summary: The Hough Transform is a technique used to detect shapes that can be represented by a mathematical equation. It's particularly useful for finding lines and circles. Essentially, it involves a \"transformation\" from the image space to the parameter space. For detecting lines, you might represent them with the equation y = mx + b , where the parameter space consists of the slope m and the intercept b . However, this representation can be problematic since the slope m can become infinite. A better approach uses the polar form r = x \\cos \\theta + y \\sin \\theta , where r is the distance from the origin to the line, and \\theta is the angle between the line and the x-axis. In this case, the parameter space is defined by r and \\theta . This parameter space is divided into a grid, where each cell represents a potential line in the image space. The algorithm then iterates through each pixel in the image space, incrementing the corresponding cell in the parameter space. The cell with the highest count (or \"votes,\" as Professor Nayar puts it) represents the detected line. Shape in Image Space Representation in Parameter Space Parameters Equation (if applicable) Line Point Slope (m), Intercept (b) y = mx + b Line (Polar Form) Sinusoidal Curve Distance (r), Angle (\u03b8) r = x \\cos \\theta + y \\sin \\theta Circle 3D Surface Center (a, b), Radius (r) (x - a)^2 + (y - b)^2 = r^2 Ellipse 4D Surface Center (a, b), Major/Minor Axes (r1, r2) \\frac{{(x - a)^2}}{{r1^2}} + \\frac{{(y - b)^2}}{{r2^2}} = 1 3.1 Initializing Variables The code starts by defining variables for the accumulator, image dimensions, and bounds of the parameters. CImg<> acc(500, 400, 1, 1, 0), imgOut(imgIn); int wx = imgIn.width(), wy = imgIn.height(); float rhomax = std::sqrt((float)(wx * wx + wy * wy)) / 2, thetamax = 2 * cimg::PI; 3.2 Gradient Calculation and Smoothing The code calculates the gradient of the input image and applies a blur to smooth it. CImgList<> grad = imgIn.get_gradient(); cimglist_for(grad, l) grad[l].blur(1.5f); 3.3 Hough Space Calculation The Hough space is a mathematical representation that helps in identifying lines in an image. In the Hough space, a line can be represented by two parameters: \\rho and \\theta , where \\rho (same as r mentioned above) is the distance from the origin to the closest point on the straight line, and \\theta is the angle formed by this perpendicular line and the horizontal axis. 3.3.1 Calculating the Gradient and the Angles The code snippet begins by iterating over all the pixels in the input image to calculate the gradient at each pixel: float X = (float)x - wx / 2, Y = (float)y - wy / 2, gx = grad(0, x, y), gy = grad(1, x, y), theta = std::atan2(gy, gx); Here, X and Y represent the coordinates if the origin is at the center of the image. The gradient at each pixel is given by (gx, gy) , and \\theta is calculated using the arctangent function, which gives the angle of the gradient vector. 3.3.2 Calculating \\rho Next, the code calculates \\rho as follows: rho = std::sqrt(X * X + Y * Y) * std::cos(std::atan2(Y, X) - theta); The value of \\rho is computed using the distance formula and the cosine of the difference between the angle of the vector to the origin (X, Y) and \\theta . 3.3.3 Adjusting \\rho and \\theta If \\rho is negative, it's multiplied by -1, and \\theta is adjusted by adding \\pi : if (rho < 0) { rho *= -1; theta += cimg::PI; } theta = cimg::mod(theta, thetamax); This ensures that \\rho is positive, and \\theta is within the valid range. 3.3.4 Populating the Accumulator Finally, the accumulator is updated based on the calculated \\rho and \\theta : acc((int)(theta * acc.width() / thetamax), (int)(rho * acc.height() / rhomax)) += (float)std::sqrt(gx * gx + gy * gy); The accumulator's cell corresponding to \\rho and \\theta is incremented by the magnitude of the gradient. This process effectively votes for the parameters of the line that the current pixel might be part of. By the end of this process, the accumulator will contain information about the lines present in the image, represented in the Hough space. 3.4 Accumulator Smoothing and Thresholding The accumulator is smoothed and thresholded to identify significant lines. // Smoothing the accumulators. acc.blur(0.5f); CImg<> acc2(acc); // Log transform to enhance the contrast of small values. cimg_forXY(acc2, x, y) acc2(x, y) = (float)std::log(1 + acc(x, y)); // Thresholding and filtering the accumulators. int size_max = acc2.get_threshold(thr * acc2.max()).get_label().max(); CImg<int> coordinates(size_max, 2, 1, 1, 0); int accNumber = 0; AccThreshold(acc2, thr * acc2.max(), 4, coordinates, accNumber); The AccThreshold() function is also defined in hough.cpp . It modifies coordinates and accNumber in place. coordinates contains the coordinates of the local maxima in the accumulator grid that are above the given threshold, and accNumber contains the count of such maxima. The image below shows the accumulator after smoothing and thresholding with a threshold value of 0.9. 3.5 Line Display Finally, the detected lines are drawn on the output image using the calculated rho and theta values. unsigned char col1[3] = {255, 255, 0}; for (unsigned i = 0; i < accNumber; ++i) { // Drawing lines // ... imgOut.draw_line(x0, y0, x1, y1, col1, 1.0f).draw_line(x0 + 1, y0, x1 + 1, y1, col1, 1.0f).draw_line(x0, y0 + 1, x1, y1 + 1, col1, 1.0f); } return imgOut; Threshold = 0.6 Threshold = 0.7 Threshold = 0.8 Threshold = 0.9 Circle Detection The Hough Transform can also be used to detect circles. In this case, the parameter space is 3D, with the parameters being the center of the circle (xc, yc) and the radius r . The equation of a circle is given by: (x - xc)^2 + (y - yc)^2 = r^2 See hough_circle.cpp for the implementation. Starting with a binarized image of coins: The Hough Transform is applied to detect the circles: The middle coin at the bottom was not detected perhaps because the binarization process caused it to be broken. 4. Texture Spectrum He and Wang (1990) proposed a method to characterize textures in an image. As the first step of most algorithms, we break down the problem into smaller pieces. Instead of characterizing the whole image at once, we analyze each pixel individually. For each pixel, we define a so-called texture unit , a vector of size 8 \\{E_1, E_2, E_3, \\ldots\\} . The formula for E_i is given as: \\begin{equation} \\text{E_i}(v_1, v_2, \\tau) = \\begin{cases} 0 & \\text{if } v_1 < v_2 - \\tau \\\\ 1 & \\text{if } \\left| v_1 - v_2 \\right| \\leq \\tau \\\\ 2 & \\text{otherwise} \\end{cases} \\end{equation} where v_1 is the pixel value of the i-th neighboring pixel, and v_2 is the pixel value of the current pixel. \\tau is a threshold that determines sensitivity. Here is an diagram that illustrates the calculation of E_i : Next, we summarize this texture unit, which is a vector of size 8, into a single value: N = \\sum_{i=1}^{8} 3^{i-1} E_i To visualize this texture encoding, let's consider the original image: And here is the texture encoding (using \\tau = 5 ): To obtain the texture spectrum, we go through each pixel and calculate the texture encoding. Then, we plot the histogram of the texture encodings. This histogram represents the texture spectrum of the image: The sharp peak in the middle means that most texture units are encoded as a vector of ones. This is because the image contains a lot of flat regions. 5. Tamura Coefficients Tamura et al. (1978) introduce six texture features that are considered to correspond well to human visual perception. These features were proposed to capture essential characteristics of visual textures that humans typically recognize. The six texture features are: Coarseness , Contrast , Directionality , Line-Likeness , Regularity , and Roughness . The book only covers the first three, so I will only discuss those. 5.1 Contrast Tamura's contrast is defined as the ratio of the standard deviation to the kurtosis of the image's pixel values. Contrast = \\frac{\\sigma}{\\kappa^n} Here, \\sigma^2 is the variance, \\kappa is the kurtosis, and n is a given exponent (I use 0.5). The book has made a mistake to calculate this value based on the histogram counts, which is not the usual way to calculate these statistics for Tamura's contrast. The reason kurtosis is included in the formula is that standard deviation alone may not provide a complete picture of how most pixels are distributed within the image. This is because standard deviation is highly sensitive to outliers. While outliers might not significantly alter our perception of contrast in an image, they can dramatically increase the value of the standard deviation. By dividing the standard deviation by the kurtosis, the formula incorporates a term that mitigates the impact of outliers, providing a more accurate measure of contrast that aligns with human perception. For this following image, the contrast is calculated to be 1,158,660: After normalizing the image to be between (50, 200), the contrast decreases to 235,809: 5.2 Coarseness Four functions work together to compute Tamura's coarseness in an image. The functions provide a series of steps, calculating integral means, local means, differences, and coarseness. The functions are as follows: IntegralMean : This function calculates the local mean within a window of size k around a given pixel (x,y) using the integral image. The integral image helps to compute sum queries over image subregions efficiently. ComputeAk : Computes the local means at different scales using the IntegralMean function. This will provide an image Ak where each pixel holds the average intensity of its surrounding pixels for different window sizes. ComputeE : Calculates the absolute differences between local means at different scales, creating two images Ekh and Ekv , representing horizontal and vertical differences. These capture texture changes in different directions. ComputeS : Uses Ekh and Ekv to compute Tamura's coarseness measure. It calculates the scale at which the largest difference between neighboring local means is observed. The formulas represented by the functions are as follows: Horizontal and Vertical Differences ( Ekh , Ekv ) : E_k^h(x, y) = |A_k(x + 2^{k-1}, y) - A_k(x - 2^{k-1}, y)| E_k^v(x, y) = |A_k(x, y + 2^{k-1}) - A_k(x, y - 2^{k-1})| where A_k is the local mean at scale k . Coarseness ( ComputeS ) : Coarseness = \\frac{{\\sum_{x,y} 2^{k_{\\text{max}}(x,y)}}}{{\\text{width} \\times \\text{height}}} where k_{\\text{max}}(x,y) is the scale at which the maximum difference is found for pixel (x,y) . For the following image, the coarseness is calculated to be 25.244: After Gaussian blur with a sigma of 5, the coarseness increases to 28.919: The unexpected behavior may be due to the interpretation of what coarseness means in this context. In Tamura's coarseness, it's not necessarily related to roughness but more about the granularity or scale of the texture. A smoother image may have larger, more uniform regions, which would be captured by this measure as being \"coarser.\" 5.3 Directionality Tamura's directionality coefficient aims to quantify the extent and directionality of edge-like features in an image. Higher values often indicate more dominant directions in the textures or features of the image. The formula for directionality look like: D = 1 + r \\times \\text{nb_pics} \\times \\sum_{p=0}^{\\text{nb_pics}} \\sum_{x} [ -h(x) \\times (x - \\text{perm}(p))^2 ] Here, \\text{nb_pics} is the number of maxima in the histogram, and r is a constant set to 1 in your code. The following are a few texture images with their directionality coefficients: 6. Local Binary Pattern (LBP) I appreciate the explanation of LBP by Moacir Antonelli Ponti . It's worth noting that the implementation from the book, which is also used here, is simplified. Specifically, it lacks translation invariance, and sequences of U with an identical number of '1's are treated as equivalent (e.g., {0101} and {0011}). 6.1 Sampling p Points on a Circle of Radius R The first step involves sampling p points on a circle with radius R . In the example code from the book, p=20 and R=2 . These sampled points likely won't align with the pixel grid, so interpolation using CImg<>linear_atXY(x, y) is necessary. 6.2 Computing Uniformity ( U ) This is the step where LBP earns its \"binary\" moniker. For each point, we compare its value V(n) to the value V_c of the central pixel as well as the value of the preceding point V(n-1) : U = \\sum_{n=0}^{p-1} \\left[ \\left( V(n) - V_c > 0 \\right) - \\left( V(n-1) - V_c > 0 \\right) \\right] Here, V(n) is the value of the sampled point, and V_c is the value of the center pixel. If n=0 , V(n-1) is out of range, so we use V(p-1) . 6.3 Computing LBP We categorize the LBP value by comparing U with 2. If U > 2 , indicating a non-uniform pattern (i.e., more than two transitions between 0 and 1), we label it with a special value p+1 . Otherwise, the following applies: \\begin{cases} lbp(x,y) = p+1 & \\text{if } U > 2 \\\\ lbp(x,y) = \\sum_{n=0}^{p-1} \\left[ V(n) - V_c > 0 \\right] & \\text{otherwise} \\end{cases} Using a small subset of textures from the Describable Textures Dataset (DTD) (found in the \"textures\" folder), I got some interesting results: The example above demonstrates that LBP can capture the \"fibrous\" texture in the original image. Similarly, LBP worked well for a \"grid\" texture, as can be seen from the top-1 result. However, LBP struggled to capture the \"banded\" texture effectively, as the top results don't look like the original image.","title":"6. Feature Extraction"},{"location":"06_feature_extraction/#feature-extraction-learning-reflection","text":"Author : Tony Fu Date : August 23, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 6 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Feature Extraction - Learning Reflection"},{"location":"06_feature_extraction/#1-harris-stephens-corner-detector","text":"I love the explanation of the Corner detection by Professor Shree K. Nayar ( Video link ). Here is the summary: The Harris-Stephens corner detection algorithm is designed to identify regions where there are substantial variations in gradient in two distinct directions. While it might be simple to detect corners aligned with the x and y axes, real-world images present corners at various angles. This challenge is addressed by using the determinant, which gives a measure of how well-separated the two gradient directions are. A larger determinant typically signifies a strong presence of two different gradient directions. However, an excessively large axis in one direction could mislead the interpretation. To counterbalance this, the algorithm subtracts a term related to the trace, squared and weighted by a parameter, from the determinant. This penalizes the response value R and aids in distinguishing between genuine corners and edges or flat regions, leading to more accurate corner detection. Here are the steps:","title":"1. Harris-Stephens Corner Detector"},{"location":"06_feature_extraction/#11-gradient-calculation","text":"The gradient of the input image is calculated using: \\text{{gradXY}} = \\text{{imgIn.get_gradient()}}; This gets the gradients in the x and y directions.","title":"1.1 Gradient Calculation"},{"location":"06_feature_extraction/#12-structure-tensor","text":"CImg<> Ixx = gradXY[0].get_mul(gradXY[0]).get_convolve(G), Iyy = gradXY[1].get_mul(gradXY[1]).get_convolve(G), Ixy = gradXY[0].get_mul(gradXY[1]).get_convolve(G); The structure tensor is computed as: \\begin{align*} I_{xx} &= \\text{{gradXY[0].get_mul(gradXY[0]).get_convolve(G)}}, \\\\ I_{yy} &= \\text{{gradXY[1].get_mul(gradXY[1]).get_convolve(G)}}, \\\\ I_{xy} &= \\text{{gradXY[0].get_mul(gradXY[1]).get_convolve(G)}}. \\end{align*} where the Gaussian kernel G is defined as: G(x, y) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2 + y^2}{2\\sigma^2}} Together, we can build the structure tensor M as: M = \\begin{bmatrix} I_{xx} & I_{xy} \\\\ I_{xy} & I_{yy} \\end{bmatrix} The determinant of M ( \\det(M) ) and the trace of M ( \\text{trace}(M) ) are computed as follows: \\det(M) = I_{xx} \\cdot I_{yy} - I_{xy} \\cdot I_{xy} \\text{trace}(M) = I_{xx} + I_{yy} The structure tensor M plays a key role in feature detection as it represents the distribution of gradients within a specific neighborhood around a point. Rather than directly comparing the gradient of a pixel with those of its neighbors, we use a Gaussian function to calculate an average gradient across an area. In essence, the structure tensor captures the underlying geometric structure in the vicinity of each pixel. It accomplishes this by portraying gradient orientations as an ellipse in the ( I_x, I_y ) plane within a specific window. Here, the determinant is directly proportional to the area of the ellipse, while the trace is equivalent to the sum of the lengths of the ellipse's major and minor axes. Presence of an edge : When an image contains an edge, the distribution of gradients forms a slender, elongated ellipse. This happens because the intensity changes consistently in one direction (along the edge) and shows little to no change in the direction perpendicular to it. The major axis of this ellipse aligns with the direction of the edge. Presence of a corner : If a corner is present, the gradients are distributed more evenly, resulting in an elliptical shape that resembles a circle. This is because a corner features significant intensity changes in multiple directions. Flat region : In a flat region of the image, where there is minimal change in intensity in any direction, the ellipse is small, signaling the absence of distinctive features.","title":"1.2 Structure Tensor"},{"location":"06_feature_extraction/#13-r-function-calculation","text":"CImg<> det = Ixx.get_mul(Iyy) - Ixy.get_sqr(), trace = Ixx + Iyy, R = det - k * trace.get_sqr(); Often, in the theoretical explanation of the Harris-Stephens corner detection algorithm, we will see the eigenvalues \\lambda_1 and \\lambda_2 are often introduced to provide an intuitive understanding of the underlying geometric properties of the image. However, in the actual implementation, you can compute the response function R directly from the components of the second-moment matrix I_{xx} , I_{yy} , and I_{xy} , without having to explicitly calculate the eigenvalues. It is given as: R = \\det - \\, k \\cdot \\text{{trace}}^2 = (I_{xx} \\cdot I_{yy} - I_{xy}^2) - k \\cdot (I_{xx} + I_{yy})^2 Condition Region Type Explanation R is close to 0 Flat Region No significant change in intensity in any direction, both eigenvalues of the structure tensor are small. R is small Edge Significant change in intensity in one direction but not the other, one large and one small eigenvalue of the structure tensor. R is positive Corner Significant changes in intensity in both directions, both eigenvalues of the structure tensor are large, indicating two dominant and different edge directions.","title":"1.3 R Function Calculation"},{"location":"06_feature_extraction/#14-local-maxima-detection","text":"CImgList<> imgGradR = R.get_gradient(); CImg_3x3(I, float); CImg<> harrisValues(imgIn.width() * imgIn.height(), 1, 1, 1, 0); CImg<int> harrisXY(imgIn.width() * imgIn.height(), 2, 1, 1, 0), perm(imgIn.width() * imgIn.height(), 1, 1, 1, 0); int nbHarris = 0; cimg_for3x3(R, x, y, 0, 0, I, float) { if (imgGradR[0](x, y) < eps && imgGradR[1](x, y) < eps) { float befx = Ipc - Icc, befy = Icp - Icc, afty = Icn - Icc, aftx = Inc - Icc; if (befx < 0 && befy < 0 && aftx < 0 && afty < 0) { harrisValues(nbHarris) = R(x, y); harrisXY(nbHarris, 0) = x; harrisXY(nbHarris++, 1) = y; } } } Local maxima of the R function are detected. This part of the code finds points that are potential corners.","title":"1.4 Local Maxima Detection"},{"location":"06_feature_extraction/#15-sorting-the-corners","text":"harrisValues.sort(perm, false); The values are sorted, and the top n corners are drawn on the image. In other implementations, this step is usually replaced by non-maximum suppression.","title":"1.5 Sorting the Corners"},{"location":"06_feature_extraction/#2-shi-tomasi-algorithm","text":"Shi-Tomasi algorithm uses similar techniques to compute eigenvalues that represent the local structure of the image, but it applies a different criteria to determine if a region is a corner: R = \\min(\\lambda_1, \\lambda_2) The algorithm is implemented as follows: CImg<> det = Ixx.get_mul(Iyy) - Ixy.get_sqr(), trace = Ixx + Iyy, diff = (trace.get_sqr() - 4 * det).sqrt(), lambda1 = (trace + diff) / 2, lambda2 = (trace - diff) / 2, R = lambda1.min(lambda2); Shi-Tomasi's reliance on the minimum eigenvalue often leads to better detection of true corners. Not sure about this one.","title":"2. Shi-Tomasi Algorithm"},{"location":"06_feature_extraction/#3-hough-transform","text":"Again, I recommend watching Professor Shree K. Nayar's video on the Hough Transform. Here's a summary: The Hough Transform is a technique used to detect shapes that can be represented by a mathematical equation. It's particularly useful for finding lines and circles. Essentially, it involves a \"transformation\" from the image space to the parameter space. For detecting lines, you might represent them with the equation y = mx + b , where the parameter space consists of the slope m and the intercept b . However, this representation can be problematic since the slope m can become infinite. A better approach uses the polar form r = x \\cos \\theta + y \\sin \\theta , where r is the distance from the origin to the line, and \\theta is the angle between the line and the x-axis. In this case, the parameter space is defined by r and \\theta . This parameter space is divided into a grid, where each cell represents a potential line in the image space. The algorithm then iterates through each pixel in the image space, incrementing the corresponding cell in the parameter space. The cell with the highest count (or \"votes,\" as Professor Nayar puts it) represents the detected line. Shape in Image Space Representation in Parameter Space Parameters Equation (if applicable) Line Point Slope (m), Intercept (b) y = mx + b Line (Polar Form) Sinusoidal Curve Distance (r), Angle (\u03b8) r = x \\cos \\theta + y \\sin \\theta Circle 3D Surface Center (a, b), Radius (r) (x - a)^2 + (y - b)^2 = r^2 Ellipse 4D Surface Center (a, b), Major/Minor Axes (r1, r2) \\frac{{(x - a)^2}}{{r1^2}} + \\frac{{(y - b)^2}}{{r2^2}} = 1","title":"3. Hough Transform"},{"location":"06_feature_extraction/#31-initializing-variables","text":"The code starts by defining variables for the accumulator, image dimensions, and bounds of the parameters. CImg<> acc(500, 400, 1, 1, 0), imgOut(imgIn); int wx = imgIn.width(), wy = imgIn.height(); float rhomax = std::sqrt((float)(wx * wx + wy * wy)) / 2, thetamax = 2 * cimg::PI;","title":"3.1 Initializing Variables"},{"location":"06_feature_extraction/#32-gradient-calculation-and-smoothing","text":"The code calculates the gradient of the input image and applies a blur to smooth it. CImgList<> grad = imgIn.get_gradient(); cimglist_for(grad, l) grad[l].blur(1.5f);","title":"3.2 Gradient Calculation and Smoothing"},{"location":"06_feature_extraction/#33-hough-space-calculation","text":"The Hough space is a mathematical representation that helps in identifying lines in an image. In the Hough space, a line can be represented by two parameters: \\rho and \\theta , where \\rho (same as r mentioned above) is the distance from the origin to the closest point on the straight line, and \\theta is the angle formed by this perpendicular line and the horizontal axis.","title":"3.3 Hough Space Calculation"},{"location":"06_feature_extraction/#331-calculating-the-gradient-and-the-angles","text":"The code snippet begins by iterating over all the pixels in the input image to calculate the gradient at each pixel: float X = (float)x - wx / 2, Y = (float)y - wy / 2, gx = grad(0, x, y), gy = grad(1, x, y), theta = std::atan2(gy, gx); Here, X and Y represent the coordinates if the origin is at the center of the image. The gradient at each pixel is given by (gx, gy) , and \\theta is calculated using the arctangent function, which gives the angle of the gradient vector.","title":"3.3.1 Calculating the Gradient and the Angles"},{"location":"06_feature_extraction/#332-calculating-rho","text":"Next, the code calculates \\rho as follows: rho = std::sqrt(X * X + Y * Y) * std::cos(std::atan2(Y, X) - theta); The value of \\rho is computed using the distance formula and the cosine of the difference between the angle of the vector to the origin (X, Y) and \\theta .","title":"3.3.2 Calculating \\rho"},{"location":"06_feature_extraction/#333-adjusting-rho-and-theta","text":"If \\rho is negative, it's multiplied by -1, and \\theta is adjusted by adding \\pi : if (rho < 0) { rho *= -1; theta += cimg::PI; } theta = cimg::mod(theta, thetamax); This ensures that \\rho is positive, and \\theta is within the valid range.","title":"3.3.3 Adjusting \\rho and \\theta"},{"location":"06_feature_extraction/#334-populating-the-accumulator","text":"Finally, the accumulator is updated based on the calculated \\rho and \\theta : acc((int)(theta * acc.width() / thetamax), (int)(rho * acc.height() / rhomax)) += (float)std::sqrt(gx * gx + gy * gy); The accumulator's cell corresponding to \\rho and \\theta is incremented by the magnitude of the gradient. This process effectively votes for the parameters of the line that the current pixel might be part of. By the end of this process, the accumulator will contain information about the lines present in the image, represented in the Hough space.","title":"3.3.4 Populating the Accumulator"},{"location":"06_feature_extraction/#34-accumulator-smoothing-and-thresholding","text":"The accumulator is smoothed and thresholded to identify significant lines. // Smoothing the accumulators. acc.blur(0.5f); CImg<> acc2(acc); // Log transform to enhance the contrast of small values. cimg_forXY(acc2, x, y) acc2(x, y) = (float)std::log(1 + acc(x, y)); // Thresholding and filtering the accumulators. int size_max = acc2.get_threshold(thr * acc2.max()).get_label().max(); CImg<int> coordinates(size_max, 2, 1, 1, 0); int accNumber = 0; AccThreshold(acc2, thr * acc2.max(), 4, coordinates, accNumber); The AccThreshold() function is also defined in hough.cpp . It modifies coordinates and accNumber in place. coordinates contains the coordinates of the local maxima in the accumulator grid that are above the given threshold, and accNumber contains the count of such maxima. The image below shows the accumulator after smoothing and thresholding with a threshold value of 0.9.","title":"3.4 Accumulator Smoothing and Thresholding"},{"location":"06_feature_extraction/#35-line-display","text":"Finally, the detected lines are drawn on the output image using the calculated rho and theta values. unsigned char col1[3] = {255, 255, 0}; for (unsigned i = 0; i < accNumber; ++i) { // Drawing lines // ... imgOut.draw_line(x0, y0, x1, y1, col1, 1.0f).draw_line(x0 + 1, y0, x1 + 1, y1, col1, 1.0f).draw_line(x0, y0 + 1, x1, y1 + 1, col1, 1.0f); } return imgOut; Threshold = 0.6 Threshold = 0.7 Threshold = 0.8 Threshold = 0.9","title":"3.5 Line Display"},{"location":"06_feature_extraction/#circle-detection","text":"The Hough Transform can also be used to detect circles. In this case, the parameter space is 3D, with the parameters being the center of the circle (xc, yc) and the radius r . The equation of a circle is given by: (x - xc)^2 + (y - yc)^2 = r^2 See hough_circle.cpp for the implementation. Starting with a binarized image of coins: The Hough Transform is applied to detect the circles: The middle coin at the bottom was not detected perhaps because the binarization process caused it to be broken.","title":"Circle Detection"},{"location":"06_feature_extraction/#4-texture-spectrum","text":"He and Wang (1990) proposed a method to characterize textures in an image. As the first step of most algorithms, we break down the problem into smaller pieces. Instead of characterizing the whole image at once, we analyze each pixel individually. For each pixel, we define a so-called texture unit , a vector of size 8 \\{E_1, E_2, E_3, \\ldots\\} . The formula for E_i is given as: \\begin{equation} \\text{E_i}(v_1, v_2, \\tau) = \\begin{cases} 0 & \\text{if } v_1 < v_2 - \\tau \\\\ 1 & \\text{if } \\left| v_1 - v_2 \\right| \\leq \\tau \\\\ 2 & \\text{otherwise} \\end{cases} \\end{equation} where v_1 is the pixel value of the i-th neighboring pixel, and v_2 is the pixel value of the current pixel. \\tau is a threshold that determines sensitivity. Here is an diagram that illustrates the calculation of E_i : Next, we summarize this texture unit, which is a vector of size 8, into a single value: N = \\sum_{i=1}^{8} 3^{i-1} E_i To visualize this texture encoding, let's consider the original image: And here is the texture encoding (using \\tau = 5 ): To obtain the texture spectrum, we go through each pixel and calculate the texture encoding. Then, we plot the histogram of the texture encodings. This histogram represents the texture spectrum of the image: The sharp peak in the middle means that most texture units are encoded as a vector of ones. This is because the image contains a lot of flat regions.","title":"4. Texture Spectrum"},{"location":"06_feature_extraction/#5-tamura-coefficients","text":"Tamura et al. (1978) introduce six texture features that are considered to correspond well to human visual perception. These features were proposed to capture essential characteristics of visual textures that humans typically recognize. The six texture features are: Coarseness , Contrast , Directionality , Line-Likeness , Regularity , and Roughness . The book only covers the first three, so I will only discuss those.","title":"5. Tamura Coefficients"},{"location":"06_feature_extraction/#51-contrast","text":"Tamura's contrast is defined as the ratio of the standard deviation to the kurtosis of the image's pixel values. Contrast = \\frac{\\sigma}{\\kappa^n} Here, \\sigma^2 is the variance, \\kappa is the kurtosis, and n is a given exponent (I use 0.5). The book has made a mistake to calculate this value based on the histogram counts, which is not the usual way to calculate these statistics for Tamura's contrast. The reason kurtosis is included in the formula is that standard deviation alone may not provide a complete picture of how most pixels are distributed within the image. This is because standard deviation is highly sensitive to outliers. While outliers might not significantly alter our perception of contrast in an image, they can dramatically increase the value of the standard deviation. By dividing the standard deviation by the kurtosis, the formula incorporates a term that mitigates the impact of outliers, providing a more accurate measure of contrast that aligns with human perception. For this following image, the contrast is calculated to be 1,158,660: After normalizing the image to be between (50, 200), the contrast decreases to 235,809:","title":"5.1 Contrast"},{"location":"06_feature_extraction/#52-coarseness","text":"Four functions work together to compute Tamura's coarseness in an image. The functions provide a series of steps, calculating integral means, local means, differences, and coarseness. The functions are as follows: IntegralMean : This function calculates the local mean within a window of size k around a given pixel (x,y) using the integral image. The integral image helps to compute sum queries over image subregions efficiently. ComputeAk : Computes the local means at different scales using the IntegralMean function. This will provide an image Ak where each pixel holds the average intensity of its surrounding pixels for different window sizes. ComputeE : Calculates the absolute differences between local means at different scales, creating two images Ekh and Ekv , representing horizontal and vertical differences. These capture texture changes in different directions. ComputeS : Uses Ekh and Ekv to compute Tamura's coarseness measure. It calculates the scale at which the largest difference between neighboring local means is observed. The formulas represented by the functions are as follows: Horizontal and Vertical Differences ( Ekh , Ekv ) : E_k^h(x, y) = |A_k(x + 2^{k-1}, y) - A_k(x - 2^{k-1}, y)| E_k^v(x, y) = |A_k(x, y + 2^{k-1}) - A_k(x, y - 2^{k-1})| where A_k is the local mean at scale k . Coarseness ( ComputeS ) : Coarseness = \\frac{{\\sum_{x,y} 2^{k_{\\text{max}}(x,y)}}}{{\\text{width} \\times \\text{height}}} where k_{\\text{max}}(x,y) is the scale at which the maximum difference is found for pixel (x,y) . For the following image, the coarseness is calculated to be 25.244: After Gaussian blur with a sigma of 5, the coarseness increases to 28.919: The unexpected behavior may be due to the interpretation of what coarseness means in this context. In Tamura's coarseness, it's not necessarily related to roughness but more about the granularity or scale of the texture. A smoother image may have larger, more uniform regions, which would be captured by this measure as being \"coarser.\"","title":"5.2 Coarseness"},{"location":"06_feature_extraction/#53-directionality","text":"Tamura's directionality coefficient aims to quantify the extent and directionality of edge-like features in an image. Higher values often indicate more dominant directions in the textures or features of the image. The formula for directionality look like: D = 1 + r \\times \\text{nb_pics} \\times \\sum_{p=0}^{\\text{nb_pics}} \\sum_{x} [ -h(x) \\times (x - \\text{perm}(p))^2 ] Here, \\text{nb_pics} is the number of maxima in the histogram, and r is a constant set to 1 in your code. The following are a few texture images with their directionality coefficients:","title":"5.3 Directionality"},{"location":"06_feature_extraction/#6-local-binary-pattern-lbp","text":"I appreciate the explanation of LBP by Moacir Antonelli Ponti . It's worth noting that the implementation from the book, which is also used here, is simplified. Specifically, it lacks translation invariance, and sequences of U with an identical number of '1's are treated as equivalent (e.g., {0101} and {0011}).","title":"6. Local Binary Pattern (LBP)"},{"location":"06_feature_extraction/#61-sampling-p-points-on-a-circle-of-radius-r","text":"The first step involves sampling p points on a circle with radius R . In the example code from the book, p=20 and R=2 . These sampled points likely won't align with the pixel grid, so interpolation using CImg<>linear_atXY(x, y) is necessary.","title":"6.1 Sampling p Points on a Circle of Radius R"},{"location":"06_feature_extraction/#62-computing-uniformity-u","text":"This is the step where LBP earns its \"binary\" moniker. For each point, we compare its value V(n) to the value V_c of the central pixel as well as the value of the preceding point V(n-1) : U = \\sum_{n=0}^{p-1} \\left[ \\left( V(n) - V_c > 0 \\right) - \\left( V(n-1) - V_c > 0 \\right) \\right] Here, V(n) is the value of the sampled point, and V_c is the value of the center pixel. If n=0 , V(n-1) is out of range, so we use V(p-1) .","title":"6.2 Computing Uniformity (U)"},{"location":"06_feature_extraction/#63-computing-lbp","text":"We categorize the LBP value by comparing U with 2. If U > 2 , indicating a non-uniform pattern (i.e., more than two transitions between 0 and 1), we label it with a special value p+1 . Otherwise, the following applies: \\begin{cases} lbp(x,y) = p+1 & \\text{if } U > 2 \\\\ lbp(x,y) = \\sum_{n=0}^{p-1} \\left[ V(n) - V_c > 0 \\right] & \\text{otherwise} \\end{cases} Using a small subset of textures from the Describable Textures Dataset (DTD) (found in the \"textures\" folder), I got some interesting results: The example above demonstrates that LBP can capture the \"fibrous\" texture in the original image. Similarly, LBP worked well for a \"grid\" texture, as can be seen from the top-1 result. However, LBP struggled to capture the \"banded\" texture effectively, as the top results don't look like the original image.","title":"6.3 Computing LBP"},{"location":"07_segmentation/","text":"Segmentation - Learning Reflection Author : Tony Fu Date : August 25, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 7 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. Active Contours This chapter is quite math-heavy, so for a more intuitive understanding of the concepts, I recommend checking out Professor Shree Nayar's lecture on active contours. Active contours provide a method for analyzing images and delineating shapes within them. Think of it as placing an elastic band around an object and then letting the band adjust itself to fit the object. In more technical terms, these contours work by minimizing a total energy (i.e., potential energy derived from the image itself and kinetic energy that allows the contour to move and adjust). Initially, you place a starting \"loop\" or initial contour around the area of interest. The system then works to minimize this total energy. 1.1 Initialize Level Set \\psi Unlike explicit representations like polygonal approximations or parametric curves, the implicit representation\u2014referred to as level set ( \\psi ) representation by the book\u2014offers more flexibility. In this context, a level set is a collection of pixels that are all at the same signed distance from the contour. In particular, the contour is the zero level set, and the pixels inside the contour have negative values, while those outside have positive values. The contour is then the zero crossing of the level set. At the start of the algorithm, we need to initialize the level set. In this case, we can draw a circle centered at x_0, y_0 with radius r . This can be bigger than the object if we are contracting the contour or smaller if we are expanding it. The level set is then initialized as: \\psi_0(x, y) = \\sqrt{(x - x_0)^2 + (y - y_0)^2} - r 1.2 Define Forces The level set \\psi is then evolved under the influence of two forces: the propagation force F_{prop} and the advective force F_{adv} . Stopping Function (Geodesic Model) A stopping function f(x,y) is defined to control how fast the contour expands or contracts based on the image gradient. f(x, y) = exp\\_cont \\cdot \\left( \\frac{1}{1 + \\| \\nabla I(x, y) \\|} + balloon \\right) where exp\\_cont = \\begin{cases} 1 & \\text{if expanding contour} \\\\ -1 & \\text{if contracting contour} \\end{cases} and balloon is a parameter that controls the amount of expansion or contraction. Gradients of Level Set Here we define two functions \\nabla ^+ and \\nabla ^- that takes the level set \\psi as input and returns the gradient of \\psi in the positive and negative directions, respectively. They are defined as: \\nabla ^+ (\\psi) = \\sqrt{\\max(D^{-x}(\\psi), 0)^2 + \\min(D^{+x}(\\psi), 0)^2 + \\max(D^{-y}(\\psi), 0)^2 + \\min(D^{+y}(\\psi), 0)^2}\\\\ \\nabla ^- (\\psi) = \\sqrt{\\max(D^{+x}(\\psi), 0)^2 + \\min(D^{-x}(\\psi), 0)^2 + \\max(D^{+y}(\\psi), 0)^2 + \\min(D^{-y}(\\psi), 0)^2} where D^{-x}(\\psi) is the backward difference of \\psi in the x direction, and D^{+x}(\\psi) is the forward difference of \\psi in the x direction. The same applies to the y direction. Propagation Force The propagation force is then defined as: F_{prop}(x, y) = - \\nabla ^+ (\\psi (x, y)) \\cdot \\max(f(x, y), 0) - \\nabla ^- (\\psi (x, y)) \\cdot \\min(f(x, y), 0) Advective Force The advective force is defined as: F_{adv}(x, y) = \\\\ - \\max(\\nabla_x (f(x, y)), 0) \\cdot \\nabla^{-x} (\\psi (x, y))\\\\ - \\min(\\nabla_x (f(x, y)), 0) \\cdot \\nabla ^{+x} (\\psi (x, y)) \\\\ - \\max(\\nabla_y (f(x, y)), 0) \\cdot \\nabla^{-y} (\\psi (x, y)) \\\\ - \\min(\\nabla_y (f(x, y)), 0) \\cdot \\nabla ^{+y} (\\psi (x, y)) 1.3 Evolve Level Set The level set is then evolved by the following equation: \\psi_{t+1}(x, y) = \\psi_t(x, y) + \\Delta t \\cdot \\left( \\alpha F_{prop}(x, y) + \\beta F_{adv}(x, y) \\right) where \\alpha and \\beta are parameters that control the relative influence of the two forces. 1.4 Normalize Level Set After evolving the level set, we need to normalize it so that the zero level set remains the contour. This is done using the Eikonal equation: if (!(iter % 20)) LevelSet.distance_eikonal(10, 3); As the contour evolves, numerical irregularities may cause the function to deviate from being a proper signed distance function. Solving the Eikonal equation periodically helps to re-initialize or \"normalize\" the level set function. Example Here I start with the binarized image of coins: And iteratively apply active contours. I am using expansion here, so the contour is initialized as a smaller circle inside the coin and expands to fit the coin. This is the contour after 40 iterations: After 200 iterations: After 400 iterations: 2. Otsu's Algorithm I recommended this video by Jian Wei Tay for a more intuitive understanding of Otsu's algorithm. Otsu's algorithm is used for finding a threshold value for binarization, separating the image into foreground and background. This threshold should maxmize the between-class variance, which is defined as: \\sigma_B^2(t) = w_0(t) \\cdot w_1(t) \\cdot \\left( \\mu_0(t) - \\mu_1(t) \\right)^2 where w_0(t) and w_1(t) are the probabilities of the two classes separated by the threshold t , and \\mu_0(t) and \\mu_1(t) are the means of the two classes. The threshold t that maximizes \\sigma_B^2(t) is the optimal threshold. Intuitively, this threshold should be the one that maximizes the difference between the two classes. However, note that they are weighted by the product w_0(t) \\cdot w_1(t) , which biases the algorithm towards thresholds that result in balanced classes. Starting with a grayscale image: We first compute the histogram of the image: The optimal threshold was found to be 81. We then binarize the image using this threshold: As you can see, Otsu's algorithm works well when the pixel values have a bimodal distribution. However, it does not work well when the distribution is not bimodal. For example, here is the result when applied to the following image: The histogram is tri-modal: and the algorithm fails to find a good threshold (it found 103). The result is a binarized image that is not very useful: 3. Bernsen's Algorithm Bersen's algorithm is a local thresholding algorithm that is more robust to uneven illumination. It works by finding the minimum and maximum pixel values in a local neighborhood and then using the average of these two values as the threshold. The size of the neighborhood is a parameter that can be tuned. Mathematically, the threshold is defined as: T(x, y) = \\begin{cases} \\frac{N_{max}(x, y) + N_{min}(x, y)}{2} \\text{ if } N_{max}(x, y) - N_{min}(x, y) < contrast \\\\ 0 \\text{ otherwise} \\end{cases} where N_{max}(x, y) = \\max_{(x', y') \\in N(x, y)} and N_{min}(x, y) = \\min_{(x', y') \\in N(x, y)} , i.e., the maximum and minimum pixel values in the neighborhood N(x, y) . The contrast parameter is a threshold that controls the contrast of the image. If the contrast is below this threshold, then the pixel is set to the average of the maximum and minimum pixel values in the neighborhood. Otherwise, the pixel is set to 0. Personally, I think Bernsen's algorithm improve the segmentation of the previous image, but notice that there are many holes in the result. We can further threshold the image: 4. K-means Clustering For a deeper understanding of K-means clustering, I highly recommend Professor Shree Nayar's lecture . K-means clustering is an algorithm used for partitioning a dataset into k distinct clusters. The algorithm iteratively assigns each data point to the nearest cluster center and recalculates the centers until the assignments stabilize. Although convergence is guaranteed, the algorithm may converge to a local minimum. Initial cluster centers are commonly chosen randomly, though various methods exist for this step. K-means Pseudocode Initialize data points and number of clusters (k) The ComputeFeatures() function converts the original image of dimensions (x, y) into feature vectors with dimensions (x, y, 2). For each pixel at (x, y), it computes: (x, y, 0): The mean of a 5x5 neighborhood around the pixel. (x, y, 1): The variance of the same 5x5 neighborhood. Randomly initialize k cluster centers Choose k random data points as initial centers. The distance between a data point and a cluster center is calculated using the squared Euclidean distance as follows: d^2 = \\sum_{dim=0}^{1} (data(x, y, dim) - g_i(dim))^2 where dim = 0 is the mean and dim = 1 is the variance of the 5x5 neighborhood around the pixel. Function: PerformKMeans() initializes this inside the loop with cimg_forX , and d2() computes this squared Euclidean distance. Loop until convergence (or max iterations): Function: PerformKMeans() handles the loop, and the convergence criterion is checked at the end of the loop. Assign each data point to the nearest cluster center Calculate the distance between each data point and all k cluster centers. Assign the data point to the cluster center with the smallest distance. Function: AssignToNearestClass() Recompute cluster centers based on the points in each cluster Calculate the mean feature vector for each cluster based on its current members. Function: RecomputeClassCenters() Check for convergence Convergence is checked by calculating the total within-cluster variance before and after reassignment. This total within-cluster variance is the objective function and is defined as: \\sum_{i=0}^{k-1} \\sum_{(x, y) \\in C_i} d^2 If the variance changes insignificantly (the book uses 1e-3 as the threshold), the algorithm has converged. Function: TotalWithinClusterVariance() provides the measure used for checking convergence. Return the final cluster centers and assignments Function: PerformKMeans() returns outputImage , which contains the final cluster assignments. Example 2 clusters: 3 clusters: 4 clusters: 5 clusters: 6 clusters: 7 clusters: 8 clusters: 5. Simple Linear Iterative Clustering (SLIC) While k-means is effective for clustering based on pixel feature similarity, it doesn't consider spatial proximity. As demonstrated above, k-means often results in clusters that are scattered across the image. Simple Linear Iterative Clustering (SLIC) solves this by creating more coherent and compact clusters, known as \"super-pixels.\" These super-pixels adapt to the image's local geometry, offering a more natural segmentation. This dual focus on feature similarity and spatial proximity gives SLIC an advantage over traditional k-means in image segmentation tasks. SLIC Pseudocode Despite its name, the algorithm is quite complex. For a detailed example, check out this video by Thales Sehn K\u00f6rting. Here's a summary: Convert image to CIELab color space CIELab is a perceptually uniform color space, which helps in clustering pixels by perceived color similarity. Function: CImg<T>::get_RGBtoLab() . Remove the alpha channel if it exists. Initialize Centroids This occurs within initialize_centroids() . The centroids are evenly spaced on a grid, each separated by S pixels (I use S=40 ). Centroids are initialized at the pixel with the lowest gradient within a S \\times S neighborhood. The output is a centroids variable with dimensions ((img.width() / S) * (img.height() / S), 5) . The last dimension contains: (0): x-coordinate in the original image (1): y-coordinate in the original image (2): L value (3): a value (4): b value Iterate Until Convergence (or Max Iterations) Assign pixels to nearest centroids Handled in get_labels() . Here, each centroid (indexed by k ) is assessed. For each centroid, we iterate through pixels within a 2S \\times 2S neighborhood and calculate the distance D in CIELab space. D = (L_i - L_k)^2 + (a_i - a_k)^2 + (b_i - b_k)^2 + \\left( \\frac{x_i - x_k}{S} \\right)^2 m^2 + \\left( \\frac{y_i - y_k}{S} \\right)^2 m^2 If D is smaller than the existing distance, the pixel's label is updated to the centroid's index. Label Remaining Pixels Unlabeled pixels are identified and assigned to the nearest centroid. Recompute centroids In recompute_centroid() , centroids are recalculated based on the mean of the pixels assigned to them. Check for Convergence Convergence is declared if centroids don't change significantly, using a threshold of 0.25. The change is quantified as: \\text{residualError} = \\sum_{k=0}^{K-1} | \\text{centroids}_k - \\text{centroids}_{k}^{old} | Example We can apply SLIC to the following image (after Gaussian smoothing): And here is the result (with S = 40 and m = 10 ):","title":"7. Segmentation"},{"location":"07_segmentation/#segmentation-learning-reflection","text":"Author : Tony Fu Date : August 25, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 7 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Segmentation - Learning Reflection"},{"location":"07_segmentation/#1-active-contours","text":"This chapter is quite math-heavy, so for a more intuitive understanding of the concepts, I recommend checking out Professor Shree Nayar's lecture on active contours. Active contours provide a method for analyzing images and delineating shapes within them. Think of it as placing an elastic band around an object and then letting the band adjust itself to fit the object. In more technical terms, these contours work by minimizing a total energy (i.e., potential energy derived from the image itself and kinetic energy that allows the contour to move and adjust). Initially, you place a starting \"loop\" or initial contour around the area of interest. The system then works to minimize this total energy.","title":"1. Active Contours"},{"location":"07_segmentation/#11-initialize-level-set-psi","text":"Unlike explicit representations like polygonal approximations or parametric curves, the implicit representation\u2014referred to as level set ( \\psi ) representation by the book\u2014offers more flexibility. In this context, a level set is a collection of pixels that are all at the same signed distance from the contour. In particular, the contour is the zero level set, and the pixels inside the contour have negative values, while those outside have positive values. The contour is then the zero crossing of the level set. At the start of the algorithm, we need to initialize the level set. In this case, we can draw a circle centered at x_0, y_0 with radius r . This can be bigger than the object if we are contracting the contour or smaller if we are expanding it. The level set is then initialized as: \\psi_0(x, y) = \\sqrt{(x - x_0)^2 + (y - y_0)^2} - r","title":"1.1 Initialize Level Set  \\psi"},{"location":"07_segmentation/#12-define-forces","text":"The level set \\psi is then evolved under the influence of two forces: the propagation force F_{prop} and the advective force F_{adv} .","title":"1.2 Define Forces"},{"location":"07_segmentation/#stopping-function-geodesic-model","text":"A stopping function f(x,y) is defined to control how fast the contour expands or contracts based on the image gradient. f(x, y) = exp\\_cont \\cdot \\left( \\frac{1}{1 + \\| \\nabla I(x, y) \\|} + balloon \\right) where exp\\_cont = \\begin{cases} 1 & \\text{if expanding contour} \\\\ -1 & \\text{if contracting contour} \\end{cases} and balloon is a parameter that controls the amount of expansion or contraction.","title":"Stopping Function (Geodesic Model)"},{"location":"07_segmentation/#gradients-of-level-set","text":"Here we define two functions \\nabla ^+ and \\nabla ^- that takes the level set \\psi as input and returns the gradient of \\psi in the positive and negative directions, respectively. They are defined as: \\nabla ^+ (\\psi) = \\sqrt{\\max(D^{-x}(\\psi), 0)^2 + \\min(D^{+x}(\\psi), 0)^2 + \\max(D^{-y}(\\psi), 0)^2 + \\min(D^{+y}(\\psi), 0)^2}\\\\ \\nabla ^- (\\psi) = \\sqrt{\\max(D^{+x}(\\psi), 0)^2 + \\min(D^{-x}(\\psi), 0)^2 + \\max(D^{+y}(\\psi), 0)^2 + \\min(D^{-y}(\\psi), 0)^2} where D^{-x}(\\psi) is the backward difference of \\psi in the x direction, and D^{+x}(\\psi) is the forward difference of \\psi in the x direction. The same applies to the y direction.","title":"Gradients of Level Set"},{"location":"07_segmentation/#propagation-force","text":"The propagation force is then defined as: F_{prop}(x, y) = - \\nabla ^+ (\\psi (x, y)) \\cdot \\max(f(x, y), 0) - \\nabla ^- (\\psi (x, y)) \\cdot \\min(f(x, y), 0)","title":"Propagation Force"},{"location":"07_segmentation/#advective-force","text":"The advective force is defined as: F_{adv}(x, y) = \\\\ - \\max(\\nabla_x (f(x, y)), 0) \\cdot \\nabla^{-x} (\\psi (x, y))\\\\ - \\min(\\nabla_x (f(x, y)), 0) \\cdot \\nabla ^{+x} (\\psi (x, y)) \\\\ - \\max(\\nabla_y (f(x, y)), 0) \\cdot \\nabla^{-y} (\\psi (x, y)) \\\\ - \\min(\\nabla_y (f(x, y)), 0) \\cdot \\nabla ^{+y} (\\psi (x, y))","title":"Advective Force"},{"location":"07_segmentation/#13-evolve-level-set","text":"The level set is then evolved by the following equation: \\psi_{t+1}(x, y) = \\psi_t(x, y) + \\Delta t \\cdot \\left( \\alpha F_{prop}(x, y) + \\beta F_{adv}(x, y) \\right) where \\alpha and \\beta are parameters that control the relative influence of the two forces.","title":"1.3 Evolve Level Set"},{"location":"07_segmentation/#14-normalize-level-set","text":"After evolving the level set, we need to normalize it so that the zero level set remains the contour. This is done using the Eikonal equation: if (!(iter % 20)) LevelSet.distance_eikonal(10, 3); As the contour evolves, numerical irregularities may cause the function to deviate from being a proper signed distance function. Solving the Eikonal equation periodically helps to re-initialize or \"normalize\" the level set function.","title":"1.4 Normalize Level Set"},{"location":"07_segmentation/#example","text":"Here I start with the binarized image of coins: And iteratively apply active contours. I am using expansion here, so the contour is initialized as a smaller circle inside the coin and expands to fit the coin. This is the contour after 40 iterations: After 200 iterations: After 400 iterations:","title":"Example"},{"location":"07_segmentation/#2-otsus-algorithm","text":"I recommended this video by Jian Wei Tay for a more intuitive understanding of Otsu's algorithm. Otsu's algorithm is used for finding a threshold value for binarization, separating the image into foreground and background. This threshold should maxmize the between-class variance, which is defined as: \\sigma_B^2(t) = w_0(t) \\cdot w_1(t) \\cdot \\left( \\mu_0(t) - \\mu_1(t) \\right)^2 where w_0(t) and w_1(t) are the probabilities of the two classes separated by the threshold t , and \\mu_0(t) and \\mu_1(t) are the means of the two classes. The threshold t that maximizes \\sigma_B^2(t) is the optimal threshold. Intuitively, this threshold should be the one that maximizes the difference between the two classes. However, note that they are weighted by the product w_0(t) \\cdot w_1(t) , which biases the algorithm towards thresholds that result in balanced classes. Starting with a grayscale image: We first compute the histogram of the image: The optimal threshold was found to be 81. We then binarize the image using this threshold: As you can see, Otsu's algorithm works well when the pixel values have a bimodal distribution. However, it does not work well when the distribution is not bimodal. For example, here is the result when applied to the following image: The histogram is tri-modal: and the algorithm fails to find a good threshold (it found 103). The result is a binarized image that is not very useful:","title":"2. Otsu's Algorithm"},{"location":"07_segmentation/#3-bernsens-algorithm","text":"Bersen's algorithm is a local thresholding algorithm that is more robust to uneven illumination. It works by finding the minimum and maximum pixel values in a local neighborhood and then using the average of these two values as the threshold. The size of the neighborhood is a parameter that can be tuned. Mathematically, the threshold is defined as: T(x, y) = \\begin{cases} \\frac{N_{max}(x, y) + N_{min}(x, y)}{2} \\text{ if } N_{max}(x, y) - N_{min}(x, y) < contrast \\\\ 0 \\text{ otherwise} \\end{cases} where N_{max}(x, y) = \\max_{(x', y') \\in N(x, y)} and N_{min}(x, y) = \\min_{(x', y') \\in N(x, y)} , i.e., the maximum and minimum pixel values in the neighborhood N(x, y) . The contrast parameter is a threshold that controls the contrast of the image. If the contrast is below this threshold, then the pixel is set to the average of the maximum and minimum pixel values in the neighborhood. Otherwise, the pixel is set to 0. Personally, I think Bernsen's algorithm improve the segmentation of the previous image, but notice that there are many holes in the result. We can further threshold the image:","title":"3. Bernsen's Algorithm"},{"location":"07_segmentation/#4-k-means-clustering","text":"For a deeper understanding of K-means clustering, I highly recommend Professor Shree Nayar's lecture . K-means clustering is an algorithm used for partitioning a dataset into k distinct clusters. The algorithm iteratively assigns each data point to the nearest cluster center and recalculates the centers until the assignments stabilize. Although convergence is guaranteed, the algorithm may converge to a local minimum. Initial cluster centers are commonly chosen randomly, though various methods exist for this step.","title":"4. K-means Clustering"},{"location":"07_segmentation/#k-means-pseudocode","text":"Initialize data points and number of clusters (k) The ComputeFeatures() function converts the original image of dimensions (x, y) into feature vectors with dimensions (x, y, 2). For each pixel at (x, y), it computes: (x, y, 0): The mean of a 5x5 neighborhood around the pixel. (x, y, 1): The variance of the same 5x5 neighborhood. Randomly initialize k cluster centers Choose k random data points as initial centers. The distance between a data point and a cluster center is calculated using the squared Euclidean distance as follows: d^2 = \\sum_{dim=0}^{1} (data(x, y, dim) - g_i(dim))^2 where dim = 0 is the mean and dim = 1 is the variance of the 5x5 neighborhood around the pixel. Function: PerformKMeans() initializes this inside the loop with cimg_forX , and d2() computes this squared Euclidean distance. Loop until convergence (or max iterations): Function: PerformKMeans() handles the loop, and the convergence criterion is checked at the end of the loop. Assign each data point to the nearest cluster center Calculate the distance between each data point and all k cluster centers. Assign the data point to the cluster center with the smallest distance. Function: AssignToNearestClass() Recompute cluster centers based on the points in each cluster Calculate the mean feature vector for each cluster based on its current members. Function: RecomputeClassCenters() Check for convergence Convergence is checked by calculating the total within-cluster variance before and after reassignment. This total within-cluster variance is the objective function and is defined as: \\sum_{i=0}^{k-1} \\sum_{(x, y) \\in C_i} d^2 If the variance changes insignificantly (the book uses 1e-3 as the threshold), the algorithm has converged. Function: TotalWithinClusterVariance() provides the measure used for checking convergence. Return the final cluster centers and assignments Function: PerformKMeans() returns outputImage , which contains the final cluster assignments.","title":"K-means Pseudocode"},{"location":"07_segmentation/#example_1","text":"2 clusters: 3 clusters: 4 clusters: 5 clusters: 6 clusters: 7 clusters: 8 clusters:","title":"Example"},{"location":"07_segmentation/#5-simple-linear-iterative-clustering-slic","text":"While k-means is effective for clustering based on pixel feature similarity, it doesn't consider spatial proximity. As demonstrated above, k-means often results in clusters that are scattered across the image. Simple Linear Iterative Clustering (SLIC) solves this by creating more coherent and compact clusters, known as \"super-pixels.\" These super-pixels adapt to the image's local geometry, offering a more natural segmentation. This dual focus on feature similarity and spatial proximity gives SLIC an advantage over traditional k-means in image segmentation tasks.","title":"5. Simple Linear Iterative Clustering (SLIC)"},{"location":"07_segmentation/#slic-pseudocode","text":"Despite its name, the algorithm is quite complex. For a detailed example, check out this video by Thales Sehn K\u00f6rting. Here's a summary: Convert image to CIELab color space CIELab is a perceptually uniform color space, which helps in clustering pixels by perceived color similarity. Function: CImg<T>::get_RGBtoLab() . Remove the alpha channel if it exists. Initialize Centroids This occurs within initialize_centroids() . The centroids are evenly spaced on a grid, each separated by S pixels (I use S=40 ). Centroids are initialized at the pixel with the lowest gradient within a S \\times S neighborhood. The output is a centroids variable with dimensions ((img.width() / S) * (img.height() / S), 5) . The last dimension contains: (0): x-coordinate in the original image (1): y-coordinate in the original image (2): L value (3): a value (4): b value Iterate Until Convergence (or Max Iterations) Assign pixels to nearest centroids Handled in get_labels() . Here, each centroid (indexed by k ) is assessed. For each centroid, we iterate through pixels within a 2S \\times 2S neighborhood and calculate the distance D in CIELab space. D = (L_i - L_k)^2 + (a_i - a_k)^2 + (b_i - b_k)^2 + \\left( \\frac{x_i - x_k}{S} \\right)^2 m^2 + \\left( \\frac{y_i - y_k}{S} \\right)^2 m^2 If D is smaller than the existing distance, the pixel's label is updated to the centroid's index. Label Remaining Pixels Unlabeled pixels are identified and assigned to the nearest centroid. Recompute centroids In recompute_centroid() , centroids are recalculated based on the mean of the pixels assigned to them. Check for Convergence Convergence is declared if centroids don't change significantly, using a threshold of 0.25. The change is quantified as: \\text{residualError} = \\sum_{k=0}^{K-1} | \\text{centroids}_k - \\text{centroids}_{k}^{old} |","title":"SLIC Pseudocode"},{"location":"07_segmentation/#example_2","text":"We can apply SLIC to the following image (after Gaussian smoothing): And here is the result (with S = 40 and m = 10 ):","title":"Example"},{"location":"08_motion/","text":"Motion Estimation Author : Tony Fu Date : August 27, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 8 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. Horn-Schunck Optical Flow Problem Formulation The Horn-Schunck method frames optical flow as an energy minimization problem by defining an energy function E that encapsulates two main terms: Data Term : This measures how well the flow (u,v) is consistent with the pixel intensities in the given images. It's based on the brightness constancy constraint: I(x,y,t) = I(x+u, y+v, t+1) , which states that the intensity of a point in an image should remain constant over time. E_{\\text{data}} = \\int \\int (I_x u + I_y v + I_t)^2 \\, dx \\, dy Smoothness Term : To encourage smoothness in the flow field, Horn and Schunck include a regularization term. This term imposes a penalty on abrupt changes in u and v . E_{\\text{smooth}} = \\int \\int (\\nabla u)^2 + (\\nabla v)^2 \\, dx \\, dy The combined energy function E to be minimized is: E = E_{\\text{data}} + \\alpha E_{\\text{smooth}} = \\int \\int \\left[ (I_x u + I_y v + I_t)^2 + \\alpha ((\\nabla u)^2 + (\\nabla v)^2) \\right] \\, dx \\, dy Minimization Process To find the flow fields u and v that minimize this energy function, Horn and Schunck uses the Euler-Lagrange equations derived from E . Take the first variation of E with respect to u and v and set them to zero. \\frac{\\partial E}{\\partial u} = 0 \\quad \\text{and} \\quad \\frac{\\partial E}{\\partial v} = 0 This results in a set of PDEs that are solved iteratively: I_x(I_x u + I_y v + I_t) + \\alpha \\Delta u = 0 I_y(I_x u + I_y v + I_t) + \\alpha \\Delta v = 0 In the original paper, Horn and Schunck approximated the Laplacians with: \\nabla u = 4 (\\bar{u} - u) \\quad \\text{and} \\quad \\nabla v = 4 (\\bar{v} - v) where \\bar{u} and \\bar{v} are the averages of u and v in the 3-neighborhood of the current pixel. The PDEs are solved iteratively until convergence (in the code, I set the maximum number of iterations to 100). The iterative update equations are: u = \\bar{u} - \\frac{I_x(I_x \\bar{u} + I_y \\bar{v} + I_t)}{4\\alpha + I_x^2 + I_y^2} \\quad \\text{and} \\quad v = \\bar{v} - \\frac{I_y(I_x \\bar{u} + I_y \\bar{v} + I_t)}{4\\alpha + I_x^2 + I_y^2} Example I use two frames from the following GIF as input to the Horn-Schunck optical flow algorithm: The result optical flow is shown below:","title":"8. Motion Estimation"},{"location":"08_motion/#motion-estimation","text":"Author : Tony Fu Date : August 27, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 8 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Motion Estimation"},{"location":"08_motion/#1-horn-schunck-optical-flow","text":"","title":"1. Horn-Schunck Optical Flow"},{"location":"08_motion/#problem-formulation","text":"The Horn-Schunck method frames optical flow as an energy minimization problem by defining an energy function E that encapsulates two main terms: Data Term : This measures how well the flow (u,v) is consistent with the pixel intensities in the given images. It's based on the brightness constancy constraint: I(x,y,t) = I(x+u, y+v, t+1) , which states that the intensity of a point in an image should remain constant over time. E_{\\text{data}} = \\int \\int (I_x u + I_y v + I_t)^2 \\, dx \\, dy Smoothness Term : To encourage smoothness in the flow field, Horn and Schunck include a regularization term. This term imposes a penalty on abrupt changes in u and v . E_{\\text{smooth}} = \\int \\int (\\nabla u)^2 + (\\nabla v)^2 \\, dx \\, dy The combined energy function E to be minimized is: E = E_{\\text{data}} + \\alpha E_{\\text{smooth}} = \\int \\int \\left[ (I_x u + I_y v + I_t)^2 + \\alpha ((\\nabla u)^2 + (\\nabla v)^2) \\right] \\, dx \\, dy","title":"Problem Formulation"},{"location":"08_motion/#minimization-process","text":"To find the flow fields u and v that minimize this energy function, Horn and Schunck uses the Euler-Lagrange equations derived from E . Take the first variation of E with respect to u and v and set them to zero. \\frac{\\partial E}{\\partial u} = 0 \\quad \\text{and} \\quad \\frac{\\partial E}{\\partial v} = 0 This results in a set of PDEs that are solved iteratively: I_x(I_x u + I_y v + I_t) + \\alpha \\Delta u = 0 I_y(I_x u + I_y v + I_t) + \\alpha \\Delta v = 0 In the original paper, Horn and Schunck approximated the Laplacians with: \\nabla u = 4 (\\bar{u} - u) \\quad \\text{and} \\quad \\nabla v = 4 (\\bar{v} - v) where \\bar{u} and \\bar{v} are the averages of u and v in the 3-neighborhood of the current pixel. The PDEs are solved iteratively until convergence (in the code, I set the maximum number of iterations to 100). The iterative update equations are: u = \\bar{u} - \\frac{I_x(I_x \\bar{u} + I_y \\bar{v} + I_t)}{4\\alpha + I_x^2 + I_y^2} \\quad \\text{and} \\quad v = \\bar{v} - \\frac{I_y(I_x \\bar{u} + I_y \\bar{v} + I_t)}{4\\alpha + I_x^2 + I_y^2}","title":"Minimization Process"},{"location":"08_motion/#example","text":"I use two frames from the following GIF as input to the Horn-Schunck optical flow algorithm: The result optical flow is shown below:","title":"Example"},{"location":"appendix_1/","text":"Math Expressions in CImg's Fill Method Author : Tony Fu Date : August 19, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub This page provides a guide to using mathematical expressions within the CImg<unsigned char>::fill() method of the CImg library. Syntax and Operators Mathematical expressions can include the following: Basic arithmetic: + , - , * , / , % Trigonometric functions: sin , cos , tan , etc. Logarithmic functions: log , exp , etc. Variables I : Represents the current pixel value. For example, I*2 would double the intensity of every pixel. J(x, y) : Refers to the neighboring pixel values at relative coordinates (x, y) x , y , z , c : Represents the coordinates of the current pixel. Conditional Expressions You can use conditional expressions like condition ? value_if_true : value_if_false . Boolean Logic Use logical operators like && (AND), || (OR), and ! (NOT) for conditional logic, and comparison operators ( == , != , < , <= , > , >= ) are also available. Examples 0. Original Image 1. Doubling the Intensity of an Image CImg<unsigned char> img(\"image.jpg\"); img.fill(\"I*2\", true); 2. Inverting an Image CImg<unsigned char> img(\"image.jpg\"); img.fill(\"255-I\", true); 3. Spiral Effect CImg<unsigned char> img(\"image.jpg\"); img_spiral.fill(\"(x*y)%500\",true); 4. Conditional Operations CImg<unsigned char> img(\"image.jpg\"); img.fill(\"I*(I!=J(1,0) || I!=J(0,1)?0:1)\", true);","title":"Appendix 1 - Math Expressions in CImg's Fill Method"},{"location":"appendix_1/#math-expressions-in-cimgs-fill-method","text":"Author : Tony Fu Date : August 19, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub This page provides a guide to using mathematical expressions within the CImg<unsigned char>::fill() method of the CImg library.","title":"Math Expressions in CImg's Fill Method"},{"location":"appendix_1/#syntax-and-operators","text":"Mathematical expressions can include the following: Basic arithmetic: + , - , * , / , % Trigonometric functions: sin , cos , tan , etc. Logarithmic functions: log , exp , etc.","title":"Syntax and Operators"},{"location":"appendix_1/#variables","text":"I : Represents the current pixel value. For example, I*2 would double the intensity of every pixel. J(x, y) : Refers to the neighboring pixel values at relative coordinates (x, y) x , y , z , c : Represents the coordinates of the current pixel.","title":"Variables"},{"location":"appendix_1/#conditional-expressions","text":"You can use conditional expressions like condition ? value_if_true : value_if_false .","title":"Conditional Expressions"},{"location":"appendix_1/#boolean-logic","text":"Use logical operators like && (AND), || (OR), and ! (NOT) for conditional logic, and comparison operators ( == , != , < , <= , > , >= ) are also available.","title":"Boolean Logic"},{"location":"appendix_1/#examples","text":"","title":"Examples"},{"location":"appendix_1/#0-original-image","text":"","title":"0. Original Image"},{"location":"appendix_1/#1-doubling-the-intensity-of-an-image","text":"CImg<unsigned char> img(\"image.jpg\"); img.fill(\"I*2\", true);","title":"1. Doubling the Intensity of an Image"},{"location":"appendix_1/#2-inverting-an-image","text":"CImg<unsigned char> img(\"image.jpg\"); img.fill(\"255-I\", true);","title":"2. Inverting an Image"},{"location":"appendix_1/#3-spiral-effect","text":"CImg<unsigned char> img(\"image.jpg\"); img_spiral.fill(\"(x*y)%500\",true);","title":"3. Spiral Effect"},{"location":"appendix_1/#4-conditional-operations","text":"CImg<unsigned char> img(\"image.jpg\"); img.fill(\"I*(I!=J(1,0) || I!=J(0,1)?0:1)\", true);","title":"4. Conditional Operations"},{"location":"where_did_i_get_my_images/","text":"Where Did I Get My Images? The stunning visuals that bring our image processing examples to life aren't just random pictures; they're a tribute to the breathtaking beauty of Washington State. These images capture the essence of a region known for its diverse geography and inspiring vistas. Browns Point Lighthouse Standing guard over Puget Sound, Browns Point Lighthouse is a beacon of maritime history and a symbol of the region's nautical heritage. Marble Mountain With its sweeping vistas and challenging terrain, Marble Mountain offers an exciting exploration ground for outdoor enthusiasts. Its peaks and trails are a tribute to Washington's rugged beauty. Port Angeles A popular stop for ice cream enthusiasts, the windy Port Angeles Wharf serves as a gateway for travelers heading to Canada or Olympic National Park. Lake Crescent Nestled near an elusive restaurant and a charming lodge, Lake Crescent offers breathtaking views and an escape into Washington's tranquil beauty. Forks Home to the filming of Twilight, Forks presents a nostalgic scene with Bella's truck, capturing the spirit of the famous series. Mount Rainier A majestic sight from the hiking trails, Mount Rainier's peak remains a distant wonder, its beauty inspiring awe without the need to conquer its summit. San Juan Islands An early visit to San Juan Islands reveals a peaceful landscape, waiting for the lavender to bloom in the months to come. Leavenworth A festive winter wonderland, Leavenworth recalls the times before the pandemic, filled with lively celebrations, beer, and sausage. Sequim Bay With boats bathed in the soft glow of sunset, Sequim Bay offers a tranquil scene of coastal serenity. Broadway Captured on a tranquil winter's day, this view of Broadway showcases a rare moment of calm on one of Seattle's busiest streets. Stretching all the way to Yesler Terrace, the image paints a peaceful contrast to the normally bustling avenue, offering a contemplative glimpse into the heart of the city. Little Gray (My Cat) Meet Little Gray, my delightful cat. While not a Washington landmark, he's a cherished part of my world and adds a touch of whimsy to this collection of images.","title":"Where Did I Get My Images?"},{"location":"where_did_i_get_my_images/#where-did-i-get-my-images","text":"The stunning visuals that bring our image processing examples to life aren't just random pictures; they're a tribute to the breathtaking beauty of Washington State. These images capture the essence of a region known for its diverse geography and inspiring vistas.","title":"Where Did I Get My Images?"},{"location":"where_did_i_get_my_images/#browns-point-lighthouse","text":"Standing guard over Puget Sound, Browns Point Lighthouse is a beacon of maritime history and a symbol of the region's nautical heritage.","title":"Browns Point Lighthouse"},{"location":"where_did_i_get_my_images/#marble-mountain","text":"With its sweeping vistas and challenging terrain, Marble Mountain offers an exciting exploration ground for outdoor enthusiasts. Its peaks and trails are a tribute to Washington's rugged beauty.","title":"Marble Mountain"},{"location":"where_did_i_get_my_images/#port-angeles","text":"A popular stop for ice cream enthusiasts, the windy Port Angeles Wharf serves as a gateway for travelers heading to Canada or Olympic National Park.","title":"Port Angeles"},{"location":"where_did_i_get_my_images/#lake-crescent","text":"Nestled near an elusive restaurant and a charming lodge, Lake Crescent offers breathtaking views and an escape into Washington's tranquil beauty.","title":"Lake Crescent"},{"location":"where_did_i_get_my_images/#forks","text":"Home to the filming of Twilight, Forks presents a nostalgic scene with Bella's truck, capturing the spirit of the famous series.","title":"Forks"},{"location":"where_did_i_get_my_images/#mount-rainier","text":"A majestic sight from the hiking trails, Mount Rainier's peak remains a distant wonder, its beauty inspiring awe without the need to conquer its summit.","title":"Mount Rainier"},{"location":"where_did_i_get_my_images/#san-juan-islands","text":"An early visit to San Juan Islands reveals a peaceful landscape, waiting for the lavender to bloom in the months to come.","title":"San Juan Islands"},{"location":"where_did_i_get_my_images/#leavenworth","text":"A festive winter wonderland, Leavenworth recalls the times before the pandemic, filled with lively celebrations, beer, and sausage.","title":"Leavenworth"},{"location":"where_did_i_get_my_images/#sequim-bay","text":"With boats bathed in the soft glow of sunset, Sequim Bay offers a tranquil scene of coastal serenity.","title":"Sequim Bay"},{"location":"where_did_i_get_my_images/#broadway","text":"Captured on a tranquil winter's day, this view of Broadway showcases a rare moment of calm on one of Seattle's busiest streets. Stretching all the way to Yesler Terrace, the image paints a peaceful contrast to the normally bustling avenue, offering a contemplative glimpse into the heart of the city.","title":"Broadway"},{"location":"where_did_i_get_my_images/#little-gray-my-cat","text":"Meet Little Gray, my delightful cat. While not a Washington landmark, he's a cherished part of my world and adds a touch of whimsy to this collection of images.","title":"Little Gray (My Cat)"},{"location":"textures/","text":"Texture Images Credit: Describable Textures Dataset (DTD)","title":"Texture Images"},{"location":"textures/#texture-images","text":"","title":"Texture Images"},{"location":"textures/#credit-describable-textures-dataset-dtd","text":"","title":"Credit: Describable Textures Dataset (DTD)"}]}
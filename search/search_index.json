{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Digital Image Processing with C++ : Study Notes This website includes my study notes for the book Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, and Barra. Please note that this website does not include the book's content, except for some of the code snippets used. If you're interested in more details, you can purchase the book here . I am not affiliated with the authors or the publisher in any way. I simply enjoy the book and want to share my notes with others. While I have some background in image processing, I have attempted to make my notes beginner-friendly. I have included things that I personally found to need more explanation or clarification. I hope you find them useful. Disclaimer All intellectual property rights related to the book, including content, images, and related materials, are owned by the authors and publisher. The information provided on this website is intended for educational purposes and personal use only, and should not be considered a substitute for purchasing the book. Introduction I relied on OpenCV for my image processing needs. Though powerful, setting it up on a new machine was always cumbersome, requiring complex installation and linking processes. Plus, OpenCV's extensive set of features often felt overwhelming, making it difficult to manage and understand specific algorithm implementations. That's when I discovered CImg\u2014a refreshingly simple library that encapsulates everything into a single header file . This approach means I can copy the header into my project and start using it immediately, without the hassle. I have also always wanted to learn more about the inner workings and design patterns of image processing libraries. CImg's simplicity makes it a great candidate for this purpose. This repository will document my journey and the insights I gain along the way. References Primary reference : Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra CImg Library Principles of Digital Image Processing series by Burger & Burge (2009, 2013)","title":"Home"},{"location":"#digital-image-processing-with-c-study-notes","text":"This website includes my study notes for the book Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, and Barra. Please note that this website does not include the book's content, except for some of the code snippets used. If you're interested in more details, you can purchase the book here . I am not affiliated with the authors or the publisher in any way. I simply enjoy the book and want to share my notes with others. While I have some background in image processing, I have attempted to make my notes beginner-friendly. I have included things that I personally found to need more explanation or clarification. I hope you find them useful.","title":"Digital Image Processing with C++ : Study Notes"},{"location":"#disclaimer","text":"All intellectual property rights related to the book, including content, images, and related materials, are owned by the authors and publisher. The information provided on this website is intended for educational purposes and personal use only, and should not be considered a substitute for purchasing the book.","title":"Disclaimer"},{"location":"#introduction","text":"I relied on OpenCV for my image processing needs. Though powerful, setting it up on a new machine was always cumbersome, requiring complex installation and linking processes. Plus, OpenCV's extensive set of features often felt overwhelming, making it difficult to manage and understand specific algorithm implementations. That's when I discovered CImg\u2014a refreshingly simple library that encapsulates everything into a single header file . This approach means I can copy the header into my project and start using it immediately, without the hassle. I have also always wanted to learn more about the inner workings and design patterns of image processing libraries. CImg's simplicity makes it a great candidate for this purpose. This repository will document my journey and the insights I gain along the way.","title":"Introduction"},{"location":"#references","text":"Primary reference : Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra CImg Library Principles of Digital Image Processing series by Burger & Burge (2009, 2013)","title":"References"},{"location":"01_getting_started/","text":"Getting Started with the CImg Library - Learning Reflection Author : Tony Fu Date : August 18, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 2.1 - 2.2 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. Installing XQuartz (X11 on macOS) Yes, I lied. I said you only need to include the CImg.h header file to get started. But that's not entirely true. You also need to install the X11 library, which is used to display images. The X11 library is used to display images. It is an open-source effort to develop a version of the X.Org X Window System that runs on macOS. You can download the latest version of XQuartz here . Follow the instructions to install it on your machine. Because X11 is often installs in a non-standard location (/opt/X11). This means that the compiler and linker may not automatically look in this location when trying to find the X11 libraries and include files. So, we need to modified the ~/.zshrc file (or whatever shell you're using) to add the following lines: # XQuartz (for CImg) export LIBRARY_PATH=/opt/X11/lib:$LIBRARY_PATH export CPATH=/opt/X11/include:$CPATH This effectively includes the X11 library in the compiler's search path. Don't forget to restart the shell or execute source ~/.zshrc to apply the changes to the current shell. 2. Installing the PNG Library If you want to read and save images in .png format, as demonstrated in the upcoming example, you'll need to install the libpng library. You can install it using Homebrew with the following command: brew install libpng You won't need to modify any environment variables for libpng . When you install libpng using Homebrew (or another standard package manager), it places the library and include files in standard locations that the compiler and linker already recognize. 3. First Program To display an image using the CImg library, include the CImg.h header file and use the CImgDisplay class. Here's a simple example first_code.cpp : #define cimg_use_png #include \"CImg.h\" using namespace cimg_library; int main() { CImg<unsigned char> img(\"../images/lighthouse.png\"); img.display(\"Lighthouse\"); return 0; } The CImg class is a template class that represents an image. The template parameter specifies the type of the image pixels. In this case, the image is a color image with unsigned 8-bit integer pixels. The CImg class has a constructor that takes a filename as input and loads the image from the file. The display() method of the CImg class displays the image in a window. The first argument is the title of the window. Here the macro cimg_use_png is used to specify that the libpng library should be used to read and write images in .png format. This gives us a peak into how we can customize the CImg library to suit our needs. To compile the program, you need to link the X11 and png library. Here is the command I used: g++ -o first_code first_code.cpp -lX11 -lpthread -lpng The -lpthread option links the pthread library. The pthread library is used to create threads, which is needed by the CImg library. To run the program, do the following: ./first_code 4. Visual Studio Code Configuration If you're coding inside Visual Studio Code like I am, you might see a red squiggly line under #include \"CImg.h\" , accompanied by the complaint Cannot open source file \"X11/Xlib.h\" (dependency of \"CImg.h\") . Here's how to fix this issue: Open the Command Palette by pressing Cmd+Shift+P on a Mac or Ctrl+Shift+P on Windows/Linux. Type \"C/C++\" in the Command Palette, and then select \"Edit Configurations (JSON)\" from the dropdown list. This action will open the c_cpp_properties.json file directly. Add the following line to the includePath array: \"includePath\": [ \"${workspaceFolder}/**\", \"/opt/X11/include/**\" // Add this line ], Save the file. The red squiggly line should now disappear.","title":"1. Getting Started"},{"location":"01_getting_started/#getting-started-with-the-cimg-library-learning-reflection","text":"Author : Tony Fu Date : August 18, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 2.1 - 2.2 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Getting Started with the CImg Library - Learning Reflection"},{"location":"01_getting_started/#1-installing-xquartz-x11-on-macos","text":"Yes, I lied. I said you only need to include the CImg.h header file to get started. But that's not entirely true. You also need to install the X11 library, which is used to display images. The X11 library is used to display images. It is an open-source effort to develop a version of the X.Org X Window System that runs on macOS. You can download the latest version of XQuartz here . Follow the instructions to install it on your machine. Because X11 is often installs in a non-standard location (/opt/X11). This means that the compiler and linker may not automatically look in this location when trying to find the X11 libraries and include files. So, we need to modified the ~/.zshrc file (or whatever shell you're using) to add the following lines: # XQuartz (for CImg) export LIBRARY_PATH=/opt/X11/lib:$LIBRARY_PATH export CPATH=/opt/X11/include:$CPATH This effectively includes the X11 library in the compiler's search path. Don't forget to restart the shell or execute source ~/.zshrc to apply the changes to the current shell.","title":"1. Installing XQuartz (X11 on macOS)"},{"location":"01_getting_started/#2-installing-the-png-library","text":"If you want to read and save images in .png format, as demonstrated in the upcoming example, you'll need to install the libpng library. You can install it using Homebrew with the following command: brew install libpng You won't need to modify any environment variables for libpng . When you install libpng using Homebrew (or another standard package manager), it places the library and include files in standard locations that the compiler and linker already recognize.","title":"2. Installing the PNG Library"},{"location":"01_getting_started/#3-first-program","text":"To display an image using the CImg library, include the CImg.h header file and use the CImgDisplay class. Here's a simple example first_code.cpp : #define cimg_use_png #include \"CImg.h\" using namespace cimg_library; int main() { CImg<unsigned char> img(\"../images/lighthouse.png\"); img.display(\"Lighthouse\"); return 0; } The CImg class is a template class that represents an image. The template parameter specifies the type of the image pixels. In this case, the image is a color image with unsigned 8-bit integer pixels. The CImg class has a constructor that takes a filename as input and loads the image from the file. The display() method of the CImg class displays the image in a window. The first argument is the title of the window. Here the macro cimg_use_png is used to specify that the libpng library should be used to read and write images in .png format. This gives us a peak into how we can customize the CImg library to suit our needs. To compile the program, you need to link the X11 and png library. Here is the command I used: g++ -o first_code first_code.cpp -lX11 -lpthread -lpng The -lpthread option links the pthread library. The pthread library is used to create threads, which is needed by the CImg library. To run the program, do the following: ./first_code","title":"3. First Program"},{"location":"01_getting_started/#4-visual-studio-code-configuration","text":"If you're coding inside Visual Studio Code like I am, you might see a red squiggly line under #include \"CImg.h\" , accompanied by the complaint Cannot open source file \"X11/Xlib.h\" (dependency of \"CImg.h\") . Here's how to fix this issue: Open the Command Palette by pressing Cmd+Shift+P on a Mac or Ctrl+Shift+P on Windows/Linux. Type \"C/C++\" in the Command Palette, and then select \"Edit Configurations (JSON)\" from the dropdown list. This action will open the c_cpp_properties.json file directly. Add the following line to the includePath array: \"includePath\": [ \"${workspaceFolder}/**\", \"/opt/X11/include/**\" // Add this line ], Save the file. The red squiggly line should now disappear.","title":"4. Visual Studio Code Configuration"},{"location":"02_block_decomposition/","text":"Block Decomposition - Learning Reflection Author : Tony Fu Date : August 18, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : Github Reference : Chapter 2.3 - 2.7 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. CImg Template Class The CImg library is a template-based image manipulation library, and its template argument specifies the pixel type. By default, if you don't specify the template argument, it's instantiated with float . So when you declare an image like this: CImg<> img(\"image.png\"); It is equivalent to: CImg<float> img(\"image.png\"); You can specify a different type if you want, such as unsigned char , int , etc. But if you simply use CImg<> , then it defaults to using float . Below are some commonly used constructors for CImg<T> : Default Constructor : CImg<T>(); This constructs an empty image. Constructor with Dimensions : CImg<T>(const unsigned int width, const unsigned int height, const unsigned int depth = 1, const unsigned int spectrum = 1, const T& value = 0); * `width`: Width of the image. * `height`: Height of the image. * `depth`: Depth of the image (default is 1 for 2D images). * `spectrum`: Number of channels (e.g., 3 for RGB image). * `value`: Initial value for all pixels. Copy Constructor : CImg<T>(const CImg<T>& img); Constructs a copy of the given image img . Constructor from File : CImg<T>(const char* filename); Constructs an image by reading from a file specified by filename . Constructor from Data : CImg<T>(const T* data, const unsigned int width, const unsigned int height, const unsigned int depth = 1, const unsigned int spectrum = 1, const bool shared = false); * `data`: Pointer to pixel data. * `width`, `height`, `depth`, `spectrum`: Same as above. * `shared`: If `true`, the data is shared with the original pointer without making a separate copy. Constructor from Expression : CImg<T>(const char* expression, const char* variable_name = 0, const T& variable_value = 0, const char* variable_name1 = 0, const T& variable_value1 = 0); This constructor creates an image from a mathematical expression, allowing for variable substitutions. 2. Reading Command-Line Parameters The cimg_usage() and cimg_option() functions are used to handle command-line arguments. Here's a brief description of each function: cimg_usage(const char *const format, ...) : This function is typically used to print a description of your program when it's invoked from the command line. cimg_option(const char *const opt, type_def variable, const char *const format, ...) : This function is a command-line option parser. It's used to handle options passed to your program when it's invoked from the command line. Here's a breakdown of the parameters: opt : the name of the command-line option. variable : the default value that will be assigned to the variable if the corresponding command-line option is not provided. format : a string that may contain a description of what the option does (this will be printed if a specific help option is invoked, like --help ). Here's a simple example showing how you might use these functions: #include \"CImg.h\" int main(int argc, char **argv) { cimg_usage(\"My simple program that does XYZ.\"); int my_option1 = cimg_option(\"-o1\", 0, \"An optional parameter that affects behavior.\"); int my_option2 = cimg_option(\"-o2\", 99, \"Another optional parameter that affects behavior.\"); // Rest of the program } If the user runs the program with the options, like ./my_program -o1 5 , the my_option1 variable will be set to 5 , and the my_option2 variable will be set to the default value of 99 . If they run the program with the --help option, they will see the usage string followed by the options descriptions. 3. Get vs. Non-Get Methods in Image Processing with CImg In image processing using CImg, it's really helpful to know whether a method is going to give you a new object (a get method) or change the one you already have (a non-get method). CImg has both types for most of its methods: Get Methods : These create a new object with the changes you want, leaving the one you started with the same. Like CImg<float>::get_blur() , which makes a new blurred image but doesn't touch the original. Non-get Methods : These change the object you call them on and usually give you back a reference to that changed object. For example, CImg<float>::blur() changes the image and gives you back a reference to it. Here's an example to show how this works: CImg<> lum = img.get_norm().blur(sigma).normalize(0, 255); In this code, get_norm makes a new image (make it gray-scale by taking the L2-norm of the RGB channel), and blur and normalize change it and give you back references. This way of doing things makes it easy to chain operations together and save memory. Resutls: Original Image Luminance Image 4. Gradient Magnitude Computation Gradient is computed using the get_gradient() method. The gradient is computed using the centered finite differences by default. We will discuss the spatial filtering in Chapter 5. The method returns a CImgList object, which is a list of images in the order you specify. In this case, we want the gradient in the x and y directions, so we specify \"xy\" as the argument. CImgList<> grad = lum.get_gradient(\"xy\"); Then we compute the gradient magnitude using the following formula: \\|\\nabla I\\| = \\sqrt{G_x^2 + G_y^2} CImg<> normGrad = (grad[0].get_sqr() += grad[1].get_sqr()).sqrt(); Notice the use of the \"non-get\" += operator, which prevents the creation of an unnecessary temporary image. Results: Gradient Magnitude Image 5. Block Decomposition See Algorithm 1 in the book for the pseudocode. Here's my breakdown: Accessing the Current Block : The loop iterates through a list of blocks ( blocks ), where each block is represented by a CImg<int> object containing four integers representing the coordinates of the top-left and bottom-right corners of the block (x0, y0) and (x1, y1). (Yes, CImg can be used as 1D vectors using CImg<int>::vector() . Checking Conditions : For each block, the code checks two conditions: (a) If the maximum value of the normGrad image, when cropped to the current block, is greater than a given threshold. (b) If both the width and height of the block are greater than 8. Splitting the Block : If both conditions are met, the block is divided into four equal parts. The new blocks are created by calculating the midpoint of the original block (xc, yc) and using these coordinates to define the four new blocks. Updating the List of Blocks : The four new blocks are added to the blocks list using the move_to() method to avoid creating unnecessary copies. The original block is then removed from the list using the remove() method. Continuing Iteration : If the conditions are not met, the loop simply moves on to the next block by incrementing the index l . 6. Loop Iteration in CImg The following macros greatly simplify writing loops that iterate over various parts of an image: cimg_for(img,ptr,T) : Iterates over all pixels of an image. cimg_for(img, ptr, T) { // Do something with ptr, a pointer to the pixel value. } cimg_forX(img,x) : Iterates over the width of an image. cimg_forX(img, x) { // x is the x-coordinate, ranging from 0 to img.width() - 1. } For macro cimg_forX(img,x) , you do not need to declare x as an integer before using it in the loop. The macro itself takes care of that. If there is already a variable named x in the same scope where you're using this macro, you can place the code inside a different scope. cimg_forY(img,y) : Iterates over the height of an image. cimg_forY(img, y) { // y is the y-coordinate, ranging from 0 to img.height() - 1. } cimg_forZ(img,z) : Iterates over the depth of an image (for 3D images). cimg_forZ(img, z) { // z is the z-coordinate, ranging from 0 to img.depth() - 1. } cimg_forC(img,c) : Iterates over the channels (spectrum) of an image. cimg_forC(img, c) { // c is the channel index, ranging from 0 to img.spectrum() - 1. } cimg_forXY(img,x,y) : Iterates over both the width and height of an image. cimg_forXY(img, x, y) { // Do something with x and y coordinates. } cimg_forXYZ(img,x,y,z) : Iterates over width, height, and depth of a 3D image. cimg_forXYZ(img, x, y, z) { // Do something with x, y, and z coordinates. } cimg_forXYZC(img,x,y,z,c) : Iterates over all dimensions, including channels. cimg_forXYZC(img, x, y, z, c) { // Do something with x, y, z coordinates and channel c. } 7. Drawing Blocks blocks is a vector of (x0, y0, x1, y1) coordinates. In the following code, we iterate through each block. We use get_crop() and resize() the cropped image to 1x1 pixels, which is the average color of the block. Then we draw a rectangle using the draw_rectangle() method. // Rendering of the decomposition. CImg<unsigned char> res(img.width(), img.height(), 1, 3, 0); CImg<int> coords(img.width(), img.height(), 1, 4, 0); cimglist_for(blocks, l) { CImg<int> &block = blocks[l]; int x0 = block[0], y0 = block[1], x1 = block[2], y1 = block[3]; CImg<unsigned char> color = img.get_crop(x0, y0, x1, y1).resize(1, 1, 1, 3, 2); res.draw_rectangle(x0, y0, x1, y1, color.data(), 1); coords.draw_rectangle(x0, y0, x1, y1, block.data()); } There are two CImg objects here: res : This is the final image that will be displayed. It is initialized to all black pixels. coords : This maps each pixel to the block it belongs to. It will be used later for user interaction. // Adding black borders. res.mul(1 - (res.get_shift(1, 1, 0, 0, 0) - res).norm().cut(0, 1)); This above is a clever way to add black borders: Part 1: Edge Detection The first part of the expression is a kind of high-pass edge detection filter that calculates the difference between adjacent pixels in the image, thereby emphasizing sharp changes or edges. The expression for this part is: \\text{mask}(x, y) = \\left(1 - \\min\\left(1, \\max\\left(0, \\|res(x, y) - res(x - 1, y - 1)\\|\\right)\\right)\\right) Here, \\|res(x, y) - res(x - 1, y - 1)\\| calculates the difference between adjacent pixels. By using the min and max functions, this difference is clipped to the range [0, 1] , with 0 representing no change (no edge) and 1 representing a large change (an edge). res mask Part 2: Multiplication The second part of the expression involves multiplying the original pixel value with the edge value obtained in Part 1. This has the effect of enhancing the detected edges in the image. The expression for this part is: \\text{result}(x, y) = res(x, y) \\cdot \\text{mask}(x, y) Block Decomposition The book also propose two other ways to render the borders: CImg<unsigned char>::fill() and cimg_for3x3 . 8. GUI The CImgDisplay class is used to create a window and display an image. It has the following constructor: CImgDisplay disp(const CImg<T>& img, const char* title = 0, const int normalization_type = 3); The normalization type is used to specify how the image is normalized when displayed. The following table shows the different options: Normalization Value Description 0 No normalization applied. 1 Automatic linear normalization to the [0, 255] range. 2 A one-time linear normalization with parameters calculated at the first display. These are then reused for subsequent images in the same window. Ideal for preserving consistent gray levels. 3 Default automatic mode, with behavior depending on the type. The following code creates a window and displays the image res : // Start the interactive viewer. CImgDisplay disp(res, \"CImg Tutorial: Block Decomposition\", 0); unsigned char white[] = {255, 255, 255}, black[] = {0, 0, 0}; while (!disp.is_closed() && !disp.is_keyESC()) { int x = disp.mouse_x(), y = disp.mouse_y(); if (x >= 0 && y >= 0) { // Get the coordinates of the block under the mouse position. int x0 = coords(x, y, 0), y0 = coords(x, y, 1), x1 = coords(x, y, 2), y1 = coords(x, y, 3), xc = (x0 + x1) / 2, yc = (y0 + y1) / 2; // Get the block and its gradient. CImg<unsigned char> pImg = img.get_crop(x0, y0, x1, y1).resize(128, 128, 1, 3, 1), pGrad = normGrad.get_crop(x0, y0, x1, y1).resize(128, 128, 1, 3, 1).normalize(0, 255).map(CImg<unsigned char>::hot_LUT256()); // Display the block and its gradient. (+res). draw_text(10, 3, \"X, Y = %d, %d\", white, 0, 1, 24, x, y). draw_rectangle(x0, y0, x1, y1, black, 0.25f). draw_line(74, 109, xc, yc, white, 0.75, 0xCCCCCCCC). draw_line(74, 264, xc, yc, white, 0.75, 0xCCCCCCCC). draw_rectangle(7, 32, 140, 165, white). draw_rectangle(7, 197, 140, 330, white). draw_image(10, 35, pImg). draw_image(10, 200, pGrad). display(disp); } disp.wait(); // Wait for an user event if (disp.is_resized()) disp.resize(disp); } Read a book for more details. Note that (+res) is a trick used to make a copy of the image res without creating a new object.","title":"2. Block Decomposition"},{"location":"02_block_decomposition/#block-decomposition-learning-reflection","text":"Author : Tony Fu Date : August 18, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : Github Reference : Chapter 2.3 - 2.7 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Block Decomposition - Learning Reflection"},{"location":"02_block_decomposition/#1-cimg-template-class","text":"The CImg library is a template-based image manipulation library, and its template argument specifies the pixel type. By default, if you don't specify the template argument, it's instantiated with float . So when you declare an image like this: CImg<> img(\"image.png\"); It is equivalent to: CImg<float> img(\"image.png\"); You can specify a different type if you want, such as unsigned char , int , etc. But if you simply use CImg<> , then it defaults to using float . Below are some commonly used constructors for CImg<T> : Default Constructor : CImg<T>(); This constructs an empty image. Constructor with Dimensions : CImg<T>(const unsigned int width, const unsigned int height, const unsigned int depth = 1, const unsigned int spectrum = 1, const T& value = 0); * `width`: Width of the image. * `height`: Height of the image. * `depth`: Depth of the image (default is 1 for 2D images). * `spectrum`: Number of channels (e.g., 3 for RGB image). * `value`: Initial value for all pixels. Copy Constructor : CImg<T>(const CImg<T>& img); Constructs a copy of the given image img . Constructor from File : CImg<T>(const char* filename); Constructs an image by reading from a file specified by filename . Constructor from Data : CImg<T>(const T* data, const unsigned int width, const unsigned int height, const unsigned int depth = 1, const unsigned int spectrum = 1, const bool shared = false); * `data`: Pointer to pixel data. * `width`, `height`, `depth`, `spectrum`: Same as above. * `shared`: If `true`, the data is shared with the original pointer without making a separate copy. Constructor from Expression : CImg<T>(const char* expression, const char* variable_name = 0, const T& variable_value = 0, const char* variable_name1 = 0, const T& variable_value1 = 0); This constructor creates an image from a mathematical expression, allowing for variable substitutions.","title":"1. CImg Template Class"},{"location":"02_block_decomposition/#2-reading-command-line-parameters","text":"The cimg_usage() and cimg_option() functions are used to handle command-line arguments. Here's a brief description of each function: cimg_usage(const char *const format, ...) : This function is typically used to print a description of your program when it's invoked from the command line. cimg_option(const char *const opt, type_def variable, const char *const format, ...) : This function is a command-line option parser. It's used to handle options passed to your program when it's invoked from the command line. Here's a breakdown of the parameters: opt : the name of the command-line option. variable : the default value that will be assigned to the variable if the corresponding command-line option is not provided. format : a string that may contain a description of what the option does (this will be printed if a specific help option is invoked, like --help ). Here's a simple example showing how you might use these functions: #include \"CImg.h\" int main(int argc, char **argv) { cimg_usage(\"My simple program that does XYZ.\"); int my_option1 = cimg_option(\"-o1\", 0, \"An optional parameter that affects behavior.\"); int my_option2 = cimg_option(\"-o2\", 99, \"Another optional parameter that affects behavior.\"); // Rest of the program } If the user runs the program with the options, like ./my_program -o1 5 , the my_option1 variable will be set to 5 , and the my_option2 variable will be set to the default value of 99 . If they run the program with the --help option, they will see the usage string followed by the options descriptions.","title":"2. Reading Command-Line Parameters"},{"location":"02_block_decomposition/#3-get-vs-non-get-methods-in-image-processing-with-cimg","text":"In image processing using CImg, it's really helpful to know whether a method is going to give you a new object (a get method) or change the one you already have (a non-get method). CImg has both types for most of its methods: Get Methods : These create a new object with the changes you want, leaving the one you started with the same. Like CImg<float>::get_blur() , which makes a new blurred image but doesn't touch the original. Non-get Methods : These change the object you call them on and usually give you back a reference to that changed object. For example, CImg<float>::blur() changes the image and gives you back a reference to it. Here's an example to show how this works: CImg<> lum = img.get_norm().blur(sigma).normalize(0, 255); In this code, get_norm makes a new image (make it gray-scale by taking the L2-norm of the RGB channel), and blur and normalize change it and give you back references. This way of doing things makes it easy to chain operations together and save memory. Resutls: Original Image Luminance Image","title":"3. Get vs. Non-Get Methods in Image Processing with CImg"},{"location":"02_block_decomposition/#4-gradient-magnitude-computation","text":"Gradient is computed using the get_gradient() method. The gradient is computed using the centered finite differences by default. We will discuss the spatial filtering in Chapter 5. The method returns a CImgList object, which is a list of images in the order you specify. In this case, we want the gradient in the x and y directions, so we specify \"xy\" as the argument. CImgList<> grad = lum.get_gradient(\"xy\"); Then we compute the gradient magnitude using the following formula: \\|\\nabla I\\| = \\sqrt{G_x^2 + G_y^2} CImg<> normGrad = (grad[0].get_sqr() += grad[1].get_sqr()).sqrt(); Notice the use of the \"non-get\" += operator, which prevents the creation of an unnecessary temporary image. Results: Gradient Magnitude Image","title":"4. Gradient Magnitude Computation"},{"location":"02_block_decomposition/#5-block-decomposition","text":"See Algorithm 1 in the book for the pseudocode. Here's my breakdown: Accessing the Current Block : The loop iterates through a list of blocks ( blocks ), where each block is represented by a CImg<int> object containing four integers representing the coordinates of the top-left and bottom-right corners of the block (x0, y0) and (x1, y1). (Yes, CImg can be used as 1D vectors using CImg<int>::vector() . Checking Conditions : For each block, the code checks two conditions: (a) If the maximum value of the normGrad image, when cropped to the current block, is greater than a given threshold. (b) If both the width and height of the block are greater than 8. Splitting the Block : If both conditions are met, the block is divided into four equal parts. The new blocks are created by calculating the midpoint of the original block (xc, yc) and using these coordinates to define the four new blocks. Updating the List of Blocks : The four new blocks are added to the blocks list using the move_to() method to avoid creating unnecessary copies. The original block is then removed from the list using the remove() method. Continuing Iteration : If the conditions are not met, the loop simply moves on to the next block by incrementing the index l .","title":"5. Block Decomposition"},{"location":"02_block_decomposition/#6-loop-iteration-in-cimg","text":"The following macros greatly simplify writing loops that iterate over various parts of an image: cimg_for(img,ptr,T) : Iterates over all pixels of an image. cimg_for(img, ptr, T) { // Do something with ptr, a pointer to the pixel value. } cimg_forX(img,x) : Iterates over the width of an image. cimg_forX(img, x) { // x is the x-coordinate, ranging from 0 to img.width() - 1. } For macro cimg_forX(img,x) , you do not need to declare x as an integer before using it in the loop. The macro itself takes care of that. If there is already a variable named x in the same scope where you're using this macro, you can place the code inside a different scope. cimg_forY(img,y) : Iterates over the height of an image. cimg_forY(img, y) { // y is the y-coordinate, ranging from 0 to img.height() - 1. } cimg_forZ(img,z) : Iterates over the depth of an image (for 3D images). cimg_forZ(img, z) { // z is the z-coordinate, ranging from 0 to img.depth() - 1. } cimg_forC(img,c) : Iterates over the channels (spectrum) of an image. cimg_forC(img, c) { // c is the channel index, ranging from 0 to img.spectrum() - 1. } cimg_forXY(img,x,y) : Iterates over both the width and height of an image. cimg_forXY(img, x, y) { // Do something with x and y coordinates. } cimg_forXYZ(img,x,y,z) : Iterates over width, height, and depth of a 3D image. cimg_forXYZ(img, x, y, z) { // Do something with x, y, and z coordinates. } cimg_forXYZC(img,x,y,z,c) : Iterates over all dimensions, including channels. cimg_forXYZC(img, x, y, z, c) { // Do something with x, y, z coordinates and channel c. }","title":"6. Loop Iteration in CImg"},{"location":"02_block_decomposition/#7-drawing-blocks","text":"blocks is a vector of (x0, y0, x1, y1) coordinates. In the following code, we iterate through each block. We use get_crop() and resize() the cropped image to 1x1 pixels, which is the average color of the block. Then we draw a rectangle using the draw_rectangle() method. // Rendering of the decomposition. CImg<unsigned char> res(img.width(), img.height(), 1, 3, 0); CImg<int> coords(img.width(), img.height(), 1, 4, 0); cimglist_for(blocks, l) { CImg<int> &block = blocks[l]; int x0 = block[0], y0 = block[1], x1 = block[2], y1 = block[3]; CImg<unsigned char> color = img.get_crop(x0, y0, x1, y1).resize(1, 1, 1, 3, 2); res.draw_rectangle(x0, y0, x1, y1, color.data(), 1); coords.draw_rectangle(x0, y0, x1, y1, block.data()); } There are two CImg objects here: res : This is the final image that will be displayed. It is initialized to all black pixels. coords : This maps each pixel to the block it belongs to. It will be used later for user interaction. // Adding black borders. res.mul(1 - (res.get_shift(1, 1, 0, 0, 0) - res).norm().cut(0, 1)); This above is a clever way to add black borders:","title":"7. Drawing Blocks"},{"location":"02_block_decomposition/#part-1-edge-detection","text":"The first part of the expression is a kind of high-pass edge detection filter that calculates the difference between adjacent pixels in the image, thereby emphasizing sharp changes or edges. The expression for this part is: \\text{mask}(x, y) = \\left(1 - \\min\\left(1, \\max\\left(0, \\|res(x, y) - res(x - 1, y - 1)\\|\\right)\\right)\\right) Here, \\|res(x, y) - res(x - 1, y - 1)\\| calculates the difference between adjacent pixels. By using the min and max functions, this difference is clipped to the range [0, 1] , with 0 representing no change (no edge) and 1 representing a large change (an edge). res mask","title":"Part 1: Edge Detection"},{"location":"02_block_decomposition/#part-2-multiplication","text":"The second part of the expression involves multiplying the original pixel value with the edge value obtained in Part 1. This has the effect of enhancing the detected edges in the image. The expression for this part is: \\text{result}(x, y) = res(x, y) \\cdot \\text{mask}(x, y) Block Decomposition The book also propose two other ways to render the borders: CImg<unsigned char>::fill() and cimg_for3x3 .","title":"Part 2: Multiplication"},{"location":"02_block_decomposition/#8-gui","text":"The CImgDisplay class is used to create a window and display an image. It has the following constructor: CImgDisplay disp(const CImg<T>& img, const char* title = 0, const int normalization_type = 3); The normalization type is used to specify how the image is normalized when displayed. The following table shows the different options: Normalization Value Description 0 No normalization applied. 1 Automatic linear normalization to the [0, 255] range. 2 A one-time linear normalization with parameters calculated at the first display. These are then reused for subsequent images in the same window. Ideal for preserving consistent gray levels. 3 Default automatic mode, with behavior depending on the type. The following code creates a window and displays the image res : // Start the interactive viewer. CImgDisplay disp(res, \"CImg Tutorial: Block Decomposition\", 0); unsigned char white[] = {255, 255, 255}, black[] = {0, 0, 0}; while (!disp.is_closed() && !disp.is_keyESC()) { int x = disp.mouse_x(), y = disp.mouse_y(); if (x >= 0 && y >= 0) { // Get the coordinates of the block under the mouse position. int x0 = coords(x, y, 0), y0 = coords(x, y, 1), x1 = coords(x, y, 2), y1 = coords(x, y, 3), xc = (x0 + x1) / 2, yc = (y0 + y1) / 2; // Get the block and its gradient. CImg<unsigned char> pImg = img.get_crop(x0, y0, x1, y1).resize(128, 128, 1, 3, 1), pGrad = normGrad.get_crop(x0, y0, x1, y1).resize(128, 128, 1, 3, 1).normalize(0, 255).map(CImg<unsigned char>::hot_LUT256()); // Display the block and its gradient. (+res). draw_text(10, 3, \"X, Y = %d, %d\", white, 0, 1, 24, x, y). draw_rectangle(x0, y0, x1, y1, black, 0.25f). draw_line(74, 109, xc, yc, white, 0.75, 0xCCCCCCCC). draw_line(74, 264, xc, yc, white, 0.75, 0xCCCCCCCC). draw_rectangle(7, 32, 140, 165, white). draw_rectangle(7, 197, 140, 330, white). draw_image(10, 35, pImg). draw_image(10, 200, pGrad). display(disp); } disp.wait(); // Wait for an user event if (disp.is_resized()) disp.resize(disp); } Read a book for more details. Note that (+res) is a trick used to make a copy of the image res without creating a new object.","title":"8. GUI"},{"location":"03_point_processing/","text":"Point Processing Transformations - Learning Reflection Author : Tony Fu Date : August 19, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 3 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. Mathematical Transformations 1.0. Orignal Image 1.1. Exponential Transformations CImg<> expImg = lum.get_exp()/50; 1.2. Square Root Transformations CImg<> sqrtImg = lum.get_sqrt()*10; 1.3. Logarithmic Transformations CImg<> logImg = (2 + lum.get_abs()).log(); 1.4 Cube Transformations CImg<> cubeImg = lum.get_pow(3); 2. Bitwise Transformations 2.0. Orignal Images 2.1. Bitwise AND CImg<unsigned char> img_and = img1 & img2; If you have two 8-bit pixels, one with the value 0101 (5 in decimal) and the other with 1010 (10 in decimal), then the bitwise AND operation between these two pixels would give you 0000 (0 in decimal). 2.2. Bitwise OR CImg<unsigned char> img_or = img1 | img2; 2.3. Bitwise XOR CImg<unsigned char> img_xor = img1 ^ img2; As we seen in the above example, bitwise operations might seem somewhat abstract when applied to images (5 & 10 = 0. What?), but they are actually quite useful in in specific contexts: Masking : If you have a binary mask where certain pixels are set to 1 and others are set to 0, you can use the bitwise AND operation with an image to \"mask\" or isolate those areas. Steganography or Watermarking : Bitwise operations might be used to embed or extract information within an image. By operating at the bit level, you can hide data within the least significant bits of an image in a way that's almost visually imperceptible. Refer to Code 3.3 in the book for an example. Thresholding : If you have two binary images representing different thresholded features, the bitwise AND can be used to find the overlapping areas between those features. 3. Histogram Equalization 3.0. Orignal Image And the corresponding histogram: 3.1. Histogram Equalization CImg<> equalizeHisto(CImg<> &imgIn, unsigned int nb) { CImg<> imgOut(imgIn); // Create a copy of the input image for the output float vmin, vmax = imgIn.max_min(vmin); // Find the minimum and maximum pixel values int size = imgIn.size(); // Get the total number of pixels in the image int vdiff = vmax - vmin; // Compute the difference between max and min values CImg<> hist = imgIn.get_histogram(nb, vmin, vmax); // Calculate the histogram with nb bins long int cumul = 0; // Compute the cumulative histogram cimg_forX(hist, pos) { cumul += hist[pos]; hist[pos] = cumul; } if (cumul == 0) // Check for a special case where the image has no non-zero pixels cumul = 1; // Avoid division by zero later in the code // Equalize the image by adjusting pixel values according to the cumulative histogram cimg_foroff(imgIn, off) // Iterate through all offsets (positions) in the input image { int pos = (int)((imgIn[off] - vmin) * (nb - 1) / vdiff); if (pos >= 0 && pos < (int)nb) imgOut[off] = vmin + vdiff * hist[pos] / size; } return imgOut; } In the example code above, cimg_foroff is a macro provided by the CImg library that creates a loop to iterate over all the pixels in the image. Inside this loop, we first calculate the position of the pixel in the histogram ( pos ). \\text{{pos}} = \\left\\lfloor \\frac{{(I_{\\text{{off}}} - \\text{{vmin}}) \\cdot (\\text{{nb}} - 1)}}{{\\text{{vdiff}}}} \\right\\rfloor where I_{\\text{{off}}} is the intensity of the pixel at offset off , \\text{{vmin}} and \\text{{vmax}} are the minimum and maximum intensity values in the image, \\text{{vdiff}} = \\text{{vmax}} - \\text{{vmin}} , and \\text{{nb}} is the number of bins in the histogram. Then, we check if the position is within the valid range of the histogram: \\text{if } \\text{{pos}} \\geq 0 \\text{ and } \\text{{pos}} < \\text{{nb}} If so, we calculate the new pixel value using the following expression: I_{\\text{{out, off}}} = \\text{{vmin}} + \\frac{{\\text{{vdiff}} \\cdot H_{\\text{{pos}}}}}{{\\text{{size}}}} where I_{\\text{{out, off}}} is the new intensity value at offset off , H_{\\text{{pos}}} is the value at the pos position in the cumulative histogram, and \\text{{size}} is the total number of pixels in the image. And the corresponding histogram:","title":"3. Point Processing Transformations"},{"location":"03_point_processing/#point-processing-transformations-learning-reflection","text":"Author : Tony Fu Date : August 19, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 3 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Point Processing Transformations - Learning Reflection"},{"location":"03_point_processing/#1-mathematical-transformations","text":"","title":"1. Mathematical Transformations"},{"location":"03_point_processing/#10-orignal-image","text":"","title":"1.0. Orignal Image"},{"location":"03_point_processing/#11-exponential-transformations","text":"CImg<> expImg = lum.get_exp()/50;","title":"1.1. Exponential Transformations"},{"location":"03_point_processing/#12-square-root-transformations","text":"CImg<> sqrtImg = lum.get_sqrt()*10;","title":"1.2. Square Root Transformations"},{"location":"03_point_processing/#13-logarithmic-transformations","text":"CImg<> logImg = (2 + lum.get_abs()).log();","title":"1.3. Logarithmic Transformations"},{"location":"03_point_processing/#14-cube-transformations","text":"CImg<> cubeImg = lum.get_pow(3);","title":"1.4 Cube Transformations"},{"location":"03_point_processing/#2-bitwise-transformations","text":"","title":"2. Bitwise Transformations"},{"location":"03_point_processing/#20-orignal-images","text":"","title":"2.0. Orignal Images"},{"location":"03_point_processing/#21-bitwise-and","text":"CImg<unsigned char> img_and = img1 & img2; If you have two 8-bit pixels, one with the value 0101 (5 in decimal) and the other with 1010 (10 in decimal), then the bitwise AND operation between these two pixels would give you 0000 (0 in decimal).","title":"2.1. Bitwise AND"},{"location":"03_point_processing/#22-bitwise-or","text":"CImg<unsigned char> img_or = img1 | img2;","title":"2.2. Bitwise OR"},{"location":"03_point_processing/#23-bitwise-xor","text":"CImg<unsigned char> img_xor = img1 ^ img2; As we seen in the above example, bitwise operations might seem somewhat abstract when applied to images (5 & 10 = 0. What?), but they are actually quite useful in in specific contexts: Masking : If you have a binary mask where certain pixels are set to 1 and others are set to 0, you can use the bitwise AND operation with an image to \"mask\" or isolate those areas. Steganography or Watermarking : Bitwise operations might be used to embed or extract information within an image. By operating at the bit level, you can hide data within the least significant bits of an image in a way that's almost visually imperceptible. Refer to Code 3.3 in the book for an example. Thresholding : If you have two binary images representing different thresholded features, the bitwise AND can be used to find the overlapping areas between those features.","title":"2.3. Bitwise XOR"},{"location":"03_point_processing/#3-histogram-equalization","text":"","title":"3. Histogram Equalization"},{"location":"03_point_processing/#30-orignal-image","text":"And the corresponding histogram:","title":"3.0. Orignal Image"},{"location":"03_point_processing/#31-histogram-equalization","text":"CImg<> equalizeHisto(CImg<> &imgIn, unsigned int nb) { CImg<> imgOut(imgIn); // Create a copy of the input image for the output float vmin, vmax = imgIn.max_min(vmin); // Find the minimum and maximum pixel values int size = imgIn.size(); // Get the total number of pixels in the image int vdiff = vmax - vmin; // Compute the difference between max and min values CImg<> hist = imgIn.get_histogram(nb, vmin, vmax); // Calculate the histogram with nb bins long int cumul = 0; // Compute the cumulative histogram cimg_forX(hist, pos) { cumul += hist[pos]; hist[pos] = cumul; } if (cumul == 0) // Check for a special case where the image has no non-zero pixels cumul = 1; // Avoid division by zero later in the code // Equalize the image by adjusting pixel values according to the cumulative histogram cimg_foroff(imgIn, off) // Iterate through all offsets (positions) in the input image { int pos = (int)((imgIn[off] - vmin) * (nb - 1) / vdiff); if (pos >= 0 && pos < (int)nb) imgOut[off] = vmin + vdiff * hist[pos] / size; } return imgOut; } In the example code above, cimg_foroff is a macro provided by the CImg library that creates a loop to iterate over all the pixels in the image. Inside this loop, we first calculate the position of the pixel in the histogram ( pos ). \\text{{pos}} = \\left\\lfloor \\frac{{(I_{\\text{{off}}} - \\text{{vmin}}) \\cdot (\\text{{nb}} - 1)}}{{\\text{{vdiff}}}} \\right\\rfloor where I_{\\text{{off}}} is the intensity of the pixel at offset off , \\text{{vmin}} and \\text{{vmax}} are the minimum and maximum intensity values in the image, \\text{{vdiff}} = \\text{{vmax}} - \\text{{vmin}} , and \\text{{nb}} is the number of bins in the histogram. Then, we check if the position is within the valid range of the histogram: \\text{if } \\text{{pos}} \\geq 0 \\text{ and } \\text{{pos}} < \\text{{nb}} If so, we calculate the new pixel value using the following expression: I_{\\text{{out, off}}} = \\text{{vmin}} + \\frac{{\\text{{vdiff}} \\cdot H_{\\text{{pos}}}}}{{\\text{{size}}}} where I_{\\text{{out, off}}} is the new intensity value at offset off , H_{\\text{{pos}}} is the value at the pos position in the cumulative histogram, and \\text{{size}} is the total number of pixels in the image. And the corresponding histogram:","title":"3.1. Histogram Equalization"},{"location":"04_morphology/","text":"Mathematical Morphology - Learning Reflection Author : Tony Fu Date : August 19, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 4 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. Erosion and Dilation I found the book's explanation of erosion and dilation to be a bit confusing. I would recommend watching this video for a more intuitive explanation of the concepts. Mathematical morphology operations have a straightforward definition when dealing with binary images. For grayscale images, the definitions become more complex: Binary Images For binary images A and a structuring element B , the operations can be defined as follows: Dilation : (A \\oplus B)(x,y) = \\max \\{ A(x-i, y-j) \\cdot B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Erosion : (A \\ominus B)(x,y) = \\min \\{ A(x+i, y+j) \\cdot B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Gray-Level Images For grayscale images A and a structuring element B , the definitions change slightly: Dilation : (A \\oplus B)(x,y) = \\max \\{ A(x-i, y-j) + B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Dilation causes bright regions to expand and dark regions to contract. Erosion : (A \\ominus B)(x,y) = \\min \\{ A(x+i, y+j) - B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Erosion causes bright regions to contract and dark regions to expand. In CImg, we can perform erosion and dilation using the erode() and dilate() functions. Both functions take a structuring element B as an argument. CImg<unsigned char> img(\"coins.png\"); CImg<> lum = img.get_norm().blur(0.75f); lum.threshold(lum.median()).normalize(0, 255); CImg<unsigned char> B = CImg<unsigned char>(3, 3).fill(1); CImg<unsigned char> imgErode = lum.get_erode(B), // Erosion imgDilate = lum.get_dilate(B); // Dilation\" Original : Binarized Luminance : Then Apply Erosion : ... or Dilation : 2. Opening and Closing The opening and closing operations are defined as follows (works for both binary and grayscale images): Opening : A \\circ B = (A \\ominus B) \\oplus B Closing : A \\bullet B = (A \\oplus B) \\ominus B CImg<unsigned char> B = CImg<unsigned char>(3, 3).fill(1); CImg<unsigned char> imgErode = lum.get_erode(B), // Erosion imgDilate = lum.get_dilate(B), // Dilation\" imgOpen = imgErode.get_dilate(B), // Opening imgClose = imgDilate.get_erode(B); // Closing Opening : Erode, then dilate (removes small objects and smooths larger ones) Closing : Dilate, then erode (closes small holes and joins nearby objects) 3. Kramer-Bruckner Filter The Kramer-Bruckner filter is a specific morphological filter used to enhance contrast and reduce noise in an image. It performs a combination of dilation and erosion, typically using a circular structuring element. In math, it can be expressed as the following two formulas: Compute the mid-value M for each pixel: M(x,y) = 0.5 \\times \\left( (A \\ominus B)(x,y) + (A \\oplus B)(x,y) \\right) Assign the output image values based on the input image and mid-value: A_{\\text{out}}(x,y) = \\begin{cases} (A \\ominus B)(x,y) & \\text{if } A(x,y) \\leq M(x,y) \\\\ (A \\oplus B)(x,y) & \\text{if } A(x,y) > M(x,y) \\end{cases} Here, A is the input image, B is the structuring element, \\ominus denotes erosion, and \\oplus denotes dilation. CImg<> KramerBruckner(CImg<> &imgIn, int n) { CImg<> imgOut(imgIn), mask = CImg<>(n, n).fill(1), imgErode = imgIn.get_erode(mask), imgDilate = imgIn.get_dilate(mask); // Dilation cimg_forXY(imgOut, x, y) { float M = 0.5f * (imgErode(x, y) + imgDilate(x, y)); imgOut(x, y) = (imgIn(x, y) <= M ? imgErode(x, y) : imgDilate(x, y)); } return imgOut; } 4. Alternating Sequential Filters (ASF): Alternating Sequential Filters are a series of morphological operations that are applied sequentially, often involving both erosions and dilations with increasing sizes of structuring elements. ASF (n = 1) : Erosion, then dilation ASF (n = 3) : (Erosion, then dilation) * 3 ASF (n = 11) : 5. Other Morphological Operations 1. Morphological Gradients Morphological gradients measure the difference between dilation and erosion of an image. In this code, two gradients are computed: the erosion gradient and the dilation gradient. Erosion Gradient ( \\text{gradE} ): \\text{gradE} = A - (A \\ominus B) Dilation Gradient ( \\text{gradD} ): \\text{gradD} = (A \\oplus B) - A Gradient E: Gradient D: 2. Beucher Gradient The Beucher gradient is another way of expressing the difference between dilation and erosion. It's calculated as the difference between the dilated and eroded images. Beucher Gradient: \\text{imgBeucher} = (A \\oplus B) - (A \\ominus B) 3. Top Hat Transformations Top hat transformations emphasize differences between the original image and its morphological opening or closing. White Top Hat: \\text{whiteTopHat} = A - (A \\ominus B) \\oplus B = A - A \\circ B Black Top Hat: \\text{blackTopHat} = (A \\oplus B) \\ominus B - A = A \\bullet B - A White Top Hat: Black Top Hat: 4. Edge Detector (contourMin and contourMax) These operations compute the minimum and maximum between the erosion and dilation gradients, respectively. Contour Min: \\text{contourMin} = \\min(\\text{gradE}, \\text{gradD}) Contour Max: \\text{contourMax} = \\max(\\text{gradE}, \\text{gradD}) Contour Min (this is blank because there is no overlap between gradE and gradD ) Contour Max: 5. Nonlinear Laplacian The nonlinear Laplacian is the difference between the dilation and erosion gradients. Nonlinear Laplacian: \\text{Laplician} = \\text{gradD} - \\text{gradE} 6. Skeletonization Here we implement the Zhang-Suen skeletonization algorithm using the CImg library in C++. Very Important Note : this method requires that the input image is binary (i.e., pixels are either 0 or 1). Iterative Skeletonization Function The main function for skeletonization is IterSkeleton . This function takes a binary image as input and performs two passes, removing certain edge pixels in each pass. Initializing Variables CImg<unsigned char> D(imgIn.width(), imgIn.height(), 1, 1, 0); CImg_3x3(N, unsigned char); Here, we use D to tag pixels for removal. Whenever a pixel is tagged, we set its value to 1. Note that CImg_3x3 has the macro format CImg_3x3(I,T) : #define CImg_3x3(I,T) T I[9]; \\ T& I##pp = I[0]; T& I##cp = I[1]; T& I##np = I[2]; \\ T& I##pc = I[3]; T& I##cc = I[4]; T& I##nc = I[5]; \\ T& I##pn = I[6]; T& I##cn = I[7]; T& I##nn = I[8]; \\ I##pp = I##cp = I##np = \\ I##pc = I##cc = I##nc = \\ I##pn = I##cn = I##nn = 0 So that in the subsequent code, we can use N to access the 8 neighbors of a pixel. The variables Npp, Ncp, Nnp, Npc, Nnc, Npn, Ncn, Nnn represent the 8 neighboring pixels around the central pixel (x, y) . The variable names correspond to their relative positions: Npp : Previous row, previous column. Ncp : Current row, previous column. Nnp : Next row, previous column. Npc : Previous row, current column. Nnc : Next row, current column. Npn : Previous row, next column. Ncn : Current row, next column. Nnn : Next row, next column. The central pixel itself is referred to as Ncc . Pass 1 The first pass checks each pixel and its 8 neighbors to determine if it should be removed: B = N_{pp} + N_{cp} + N_{np} + N_{pc} + N_{nc} + N_{pn} + N_{cn} + N_{nn} C = N_{nc} \\cdot (N_{nc} - N_{np}) + N_{np} \\cdot (N_{np} - N_{cp}) + \\cdots + N_{nn} \\cdot (N_{nn} - N_{nc}) Here, C is the the number of 0-1 transitions in the 8-neighborhood of the pixel. The book's description \"C(N), called connectivity number, expressing how many binary components are connected to the central pixel (x,y)\" appears to be incorrect. The conditions for removal are: R1 = \\left( B \\geq 2 \\right) \\land \\left( B \\leq 6 \\right) \\land \\left( C = 1 \\right) \\land \\left( N_{cn} \\cdot N_{nc} \\cdot N_{cp} = 0 \\right) \\land \\left( N_{pc} \\cdot N_{cn} \\cdot N_{nc} = 0 \\right) Intuitively, this means that the pixel should be removed if: It has 2-6 neighbors. (Because if B is 0 or 1, then the pixel is an end point or an isolated point.) It has exactly 1 0-1 transition in its 8-neighborhood because it is likely to be a boundary pixel. N_{cn} \\cdot N_{nc} \\cdot N_{cp} = 0 and N_{pc} \\cdot N_{cn} \\cdot N_{nc} = 0 for R1 , and N_{nc} \\cdot N_{cp} \\cdot N_{pc} = 0 and N_{cp} \\cdot N_{pc} \\cdot N_{cn} = 0 for R2 : These are specific conditions for Zhang-Suen thinning. They ensure that no spur pixels (pixels that stick out from the object) are created during the thinning process. For R1 , these conditions make sure that the pixel is not an endpoint in the south and east directions, and that removing it won't break connectivity. The conditions for R2 (see below) do a similar thing, but in the north and west directions. By separating the thinning process into two stages, the algorithm avoids the possibility of over-thinning or under-thinning. // Pass 1 int n1 = 0; cimg_for3x3(imgIn, x, y, 0, 0, N, unsigned char) { if (imgIn(x, y)) { // Compute B and C here... bool R1 = B >= 2 && B <= 6 && C == 1 && Ncn * Nnc * Ncp == 0 && Npc * Ncn * Nnc == 0; if (R1) { // Tag (x,y) D(x, y) = 1; ++n1; } } } Removing Tagged Pixels from Pass 1 cimg_forXY(imgIn, x, y) imgIn(x, y) -= (n1 > 0) * D(x, y); Pass 2 The second pass is similar to the first, but with different conditions for removal: R2 = \\left( B \\geq 2 \\right) \\land \\left( B \\leq 6 \\right) \\land \\left( C = 1 \\right) \\land \\left( N_{nc} \\cdot N_{cp} \\cdot N_{pc} = 0 \\right) \\land \\left( N_{cp} \\cdot N_{pc} \\cdot N_{cn} = 0 \\right) Removing Tagged Pixels from Pass 2 Similar to the removal in Pass 1. Return the Total Number of Removed Pixels return n1 + n2; Iterative Skeletonization int num_removed; do { num_removed = IterSkeleton(lum); } while (num_removed > 0); Here, we keep calling IterSkeleton until no pixels are removed in a pass. Results Original : Skeleton :","title":"4. Morphology"},{"location":"04_morphology/#mathematical-morphology-learning-reflection","text":"Author : Tony Fu Date : August 19, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 4 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Mathematical Morphology - Learning Reflection"},{"location":"04_morphology/#1-erosion-and-dilation","text":"I found the book's explanation of erosion and dilation to be a bit confusing. I would recommend watching this video for a more intuitive explanation of the concepts. Mathematical morphology operations have a straightforward definition when dealing with binary images. For grayscale images, the definitions become more complex:","title":"1. Erosion and Dilation"},{"location":"04_morphology/#binary-images","text":"For binary images A and a structuring element B , the operations can be defined as follows: Dilation : (A \\oplus B)(x,y) = \\max \\{ A(x-i, y-j) \\cdot B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Erosion : (A \\ominus B)(x,y) = \\min \\{ A(x+i, y+j) \\cdot B(i,j) : (i,j) \\in \\text{{domain of }} B \\}","title":"Binary Images"},{"location":"04_morphology/#gray-level-images","text":"For grayscale images A and a structuring element B , the definitions change slightly: Dilation : (A \\oplus B)(x,y) = \\max \\{ A(x-i, y-j) + B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Dilation causes bright regions to expand and dark regions to contract. Erosion : (A \\ominus B)(x,y) = \\min \\{ A(x+i, y+j) - B(i,j) : (i,j) \\in \\text{{domain of }} B \\} Erosion causes bright regions to contract and dark regions to expand. In CImg, we can perform erosion and dilation using the erode() and dilate() functions. Both functions take a structuring element B as an argument. CImg<unsigned char> img(\"coins.png\"); CImg<> lum = img.get_norm().blur(0.75f); lum.threshold(lum.median()).normalize(0, 255); CImg<unsigned char> B = CImg<unsigned char>(3, 3).fill(1); CImg<unsigned char> imgErode = lum.get_erode(B), // Erosion imgDilate = lum.get_dilate(B); // Dilation\" Original : Binarized Luminance : Then Apply Erosion : ... or Dilation :","title":"Gray-Level Images"},{"location":"04_morphology/#2-opening-and-closing","text":"The opening and closing operations are defined as follows (works for both binary and grayscale images): Opening : A \\circ B = (A \\ominus B) \\oplus B Closing : A \\bullet B = (A \\oplus B) \\ominus B CImg<unsigned char> B = CImg<unsigned char>(3, 3).fill(1); CImg<unsigned char> imgErode = lum.get_erode(B), // Erosion imgDilate = lum.get_dilate(B), // Dilation\" imgOpen = imgErode.get_dilate(B), // Opening imgClose = imgDilate.get_erode(B); // Closing Opening : Erode, then dilate (removes small objects and smooths larger ones) Closing : Dilate, then erode (closes small holes and joins nearby objects)","title":"2. Opening and Closing"},{"location":"04_morphology/#3-kramer-bruckner-filter","text":"The Kramer-Bruckner filter is a specific morphological filter used to enhance contrast and reduce noise in an image. It performs a combination of dilation and erosion, typically using a circular structuring element. In math, it can be expressed as the following two formulas: Compute the mid-value M for each pixel: M(x,y) = 0.5 \\times \\left( (A \\ominus B)(x,y) + (A \\oplus B)(x,y) \\right) Assign the output image values based on the input image and mid-value: A_{\\text{out}}(x,y) = \\begin{cases} (A \\ominus B)(x,y) & \\text{if } A(x,y) \\leq M(x,y) \\\\ (A \\oplus B)(x,y) & \\text{if } A(x,y) > M(x,y) \\end{cases} Here, A is the input image, B is the structuring element, \\ominus denotes erosion, and \\oplus denotes dilation. CImg<> KramerBruckner(CImg<> &imgIn, int n) { CImg<> imgOut(imgIn), mask = CImg<>(n, n).fill(1), imgErode = imgIn.get_erode(mask), imgDilate = imgIn.get_dilate(mask); // Dilation cimg_forXY(imgOut, x, y) { float M = 0.5f * (imgErode(x, y) + imgDilate(x, y)); imgOut(x, y) = (imgIn(x, y) <= M ? imgErode(x, y) : imgDilate(x, y)); } return imgOut; }","title":"3. Kramer-Bruckner Filter"},{"location":"04_morphology/#4-alternating-sequential-filters-asf","text":"Alternating Sequential Filters are a series of morphological operations that are applied sequentially, often involving both erosions and dilations with increasing sizes of structuring elements. ASF (n = 1) : Erosion, then dilation ASF (n = 3) : (Erosion, then dilation) * 3 ASF (n = 11) :","title":"4. Alternating Sequential Filters (ASF):"},{"location":"04_morphology/#5-other-morphological-operations","text":"","title":"5. Other Morphological Operations"},{"location":"04_morphology/#1-morphological-gradients","text":"Morphological gradients measure the difference between dilation and erosion of an image. In this code, two gradients are computed: the erosion gradient and the dilation gradient. Erosion Gradient ( \\text{gradE} ): \\text{gradE} = A - (A \\ominus B) Dilation Gradient ( \\text{gradD} ): \\text{gradD} = (A \\oplus B) - A Gradient E: Gradient D:","title":"1. Morphological Gradients"},{"location":"04_morphology/#2-beucher-gradient","text":"The Beucher gradient is another way of expressing the difference between dilation and erosion. It's calculated as the difference between the dilated and eroded images. Beucher Gradient: \\text{imgBeucher} = (A \\oplus B) - (A \\ominus B)","title":"2. Beucher Gradient"},{"location":"04_morphology/#3-top-hat-transformations","text":"Top hat transformations emphasize differences between the original image and its morphological opening or closing. White Top Hat: \\text{whiteTopHat} = A - (A \\ominus B) \\oplus B = A - A \\circ B Black Top Hat: \\text{blackTopHat} = (A \\oplus B) \\ominus B - A = A \\bullet B - A White Top Hat: Black Top Hat:","title":"3. Top Hat Transformations"},{"location":"04_morphology/#4-edge-detector-contourmin-and-contourmax","text":"These operations compute the minimum and maximum between the erosion and dilation gradients, respectively. Contour Min: \\text{contourMin} = \\min(\\text{gradE}, \\text{gradD}) Contour Max: \\text{contourMax} = \\max(\\text{gradE}, \\text{gradD}) Contour Min (this is blank because there is no overlap between gradE and gradD ) Contour Max:","title":"4. Edge Detector (contourMin and contourMax)"},{"location":"04_morphology/#5-nonlinear-laplacian","text":"The nonlinear Laplacian is the difference between the dilation and erosion gradients. Nonlinear Laplacian: \\text{Laplician} = \\text{gradD} - \\text{gradE}","title":"5. Nonlinear Laplacian"},{"location":"04_morphology/#6-skeletonization","text":"Here we implement the Zhang-Suen skeletonization algorithm using the CImg library in C++. Very Important Note : this method requires that the input image is binary (i.e., pixels are either 0 or 1).","title":"6. Skeletonization"},{"location":"04_morphology/#iterative-skeletonization-function","text":"The main function for skeletonization is IterSkeleton . This function takes a binary image as input and performs two passes, removing certain edge pixels in each pass.","title":"Iterative Skeletonization Function"},{"location":"04_morphology/#initializing-variables","text":"CImg<unsigned char> D(imgIn.width(), imgIn.height(), 1, 1, 0); CImg_3x3(N, unsigned char); Here, we use D to tag pixels for removal. Whenever a pixel is tagged, we set its value to 1. Note that CImg_3x3 has the macro format CImg_3x3(I,T) : #define CImg_3x3(I,T) T I[9]; \\ T& I##pp = I[0]; T& I##cp = I[1]; T& I##np = I[2]; \\ T& I##pc = I[3]; T& I##cc = I[4]; T& I##nc = I[5]; \\ T& I##pn = I[6]; T& I##cn = I[7]; T& I##nn = I[8]; \\ I##pp = I##cp = I##np = \\ I##pc = I##cc = I##nc = \\ I##pn = I##cn = I##nn = 0 So that in the subsequent code, we can use N to access the 8 neighbors of a pixel. The variables Npp, Ncp, Nnp, Npc, Nnc, Npn, Ncn, Nnn represent the 8 neighboring pixels around the central pixel (x, y) . The variable names correspond to their relative positions: Npp : Previous row, previous column. Ncp : Current row, previous column. Nnp : Next row, previous column. Npc : Previous row, current column. Nnc : Next row, current column. Npn : Previous row, next column. Ncn : Current row, next column. Nnn : Next row, next column. The central pixel itself is referred to as Ncc .","title":"Initializing Variables"},{"location":"04_morphology/#pass-1","text":"The first pass checks each pixel and its 8 neighbors to determine if it should be removed: B = N_{pp} + N_{cp} + N_{np} + N_{pc} + N_{nc} + N_{pn} + N_{cn} + N_{nn} C = N_{nc} \\cdot (N_{nc} - N_{np}) + N_{np} \\cdot (N_{np} - N_{cp}) + \\cdots + N_{nn} \\cdot (N_{nn} - N_{nc}) Here, C is the the number of 0-1 transitions in the 8-neighborhood of the pixel. The book's description \"C(N), called connectivity number, expressing how many binary components are connected to the central pixel (x,y)\" appears to be incorrect. The conditions for removal are: R1 = \\left( B \\geq 2 \\right) \\land \\left( B \\leq 6 \\right) \\land \\left( C = 1 \\right) \\land \\left( N_{cn} \\cdot N_{nc} \\cdot N_{cp} = 0 \\right) \\land \\left( N_{pc} \\cdot N_{cn} \\cdot N_{nc} = 0 \\right) Intuitively, this means that the pixel should be removed if: It has 2-6 neighbors. (Because if B is 0 or 1, then the pixel is an end point or an isolated point.) It has exactly 1 0-1 transition in its 8-neighborhood because it is likely to be a boundary pixel. N_{cn} \\cdot N_{nc} \\cdot N_{cp} = 0 and N_{pc} \\cdot N_{cn} \\cdot N_{nc} = 0 for R1 , and N_{nc} \\cdot N_{cp} \\cdot N_{pc} = 0 and N_{cp} \\cdot N_{pc} \\cdot N_{cn} = 0 for R2 : These are specific conditions for Zhang-Suen thinning. They ensure that no spur pixels (pixels that stick out from the object) are created during the thinning process. For R1 , these conditions make sure that the pixel is not an endpoint in the south and east directions, and that removing it won't break connectivity. The conditions for R2 (see below) do a similar thing, but in the north and west directions. By separating the thinning process into two stages, the algorithm avoids the possibility of over-thinning or under-thinning. // Pass 1 int n1 = 0; cimg_for3x3(imgIn, x, y, 0, 0, N, unsigned char) { if (imgIn(x, y)) { // Compute B and C here... bool R1 = B >= 2 && B <= 6 && C == 1 && Ncn * Nnc * Ncp == 0 && Npc * Ncn * Nnc == 0; if (R1) { // Tag (x,y) D(x, y) = 1; ++n1; } } }","title":"Pass 1"},{"location":"04_morphology/#removing-tagged-pixels-from-pass-1","text":"cimg_forXY(imgIn, x, y) imgIn(x, y) -= (n1 > 0) * D(x, y);","title":"Removing Tagged Pixels from Pass 1"},{"location":"04_morphology/#pass-2","text":"The second pass is similar to the first, but with different conditions for removal: R2 = \\left( B \\geq 2 \\right) \\land \\left( B \\leq 6 \\right) \\land \\left( C = 1 \\right) \\land \\left( N_{nc} \\cdot N_{cp} \\cdot N_{pc} = 0 \\right) \\land \\left( N_{cp} \\cdot N_{pc} \\cdot N_{cn} = 0 \\right)","title":"Pass 2"},{"location":"04_morphology/#removing-tagged-pixels-from-pass-2","text":"Similar to the removal in Pass 1.","title":"Removing Tagged Pixels from Pass 2"},{"location":"04_morphology/#return-the-total-number-of-removed-pixels","text":"return n1 + n2;","title":"Return the Total Number of Removed Pixels"},{"location":"04_morphology/#iterative-skeletonization","text":"int num_removed; do { num_removed = IterSkeleton(lum); } while (num_removed > 0); Here, we keep calling IterSkeleton until no pixels are removed in a pass.","title":"Iterative Skeletonization"},{"location":"04_morphology/#results","text":"Original : Skeleton :","title":"Results"},{"location":"05_filtering/","text":"Filtering - Learning Reflection Author : Tony Fu Date : August 21, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 5 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra 1. Convolution CImg<> sobel(3, 3, 1, 1, 0); sobel(0, 0) = -1; sobel(0, 1) = -2; sobel(0, 2) = -1; sobel(1, 0) = 0; sobel(1, 1) = 0; sobel(1, 2) = 0; sobel(2, 0) = 1; sobel(2, 1) = 2; sobel(2, 2) = 1; imgIn.convolve(sobel); The above code snippet shows how to perform a convolution on an image with a 3x3 Sobel filter. The result is shown below: Original : Convolution : Boundary Conditions Boundary conditions specify how to handle the edges and can be specified using the const usigned int boundary_conditions parameter of the convolve() method. The four boundary conditions provided by the CImg library have specific meanings: Dirichlet (0): The pixels outside the image boundaries are considered to be zero. This creates a sort of \"hard\" edge around the image and can lead to noticeable artifacts along the borders. Neumann (1) (default): The value of the border pixels is extended outside the image boundaries. Essentially, this reflects the gradient at the border, assuming that the intensity of the image doesn't change beyond the edge. This is the default boundary condition in CImg and tends to provide visually acceptable results. Periodic (2): The image is treated as if it were tiling the plane in a repeated pattern. This means that the pixels on the right edge of the image are used as the boundary condition for the left edge, and the pixels on the bottom are used for the top. This can create seamless transitions but can also lead to strange effects if the image does not naturally tile. Mirror (3): The pixels outside the image boundaries are determined by mirroring the pixels inside the boundaries. Imagine folding the image over at its edges, so the pixels just inside the border are duplicated just outside the border. This can create a more visually smooth transition at the edges but may not be appropriate for all types of images. 2. Median Fitler img.blur_median(3); The above code snippet shows how to perform a median filter on an image with a 3x3 window. The result is shown below: Origin : Median Filter : Order-Statistic (OS) Filter An OS filter (Bovik, Huang, and Munson, 1983) is a non-linear filter that computes a linear combination of these sorted values: F = w_1 \\cdot s_1 + w_2 \\cdot s_2 + \\ldots + w_n \\cdot s_n where w_i are the weights that define how much each ordered value contributes to the final result, and s_i are the sorted values of the neighborhood pixels. 3. First-Order Derivatives The get_gradient function computes the image gradient along specified axes using different numerical schemes. Parameters axes : Axes considered for the gradient computation (e.g., \"xy\"). scheme : Numerical scheme used for the gradient computation. Options are: Schemes Backward Finite Differences ( scheme = -1 ) Computes the gradient using backward finite differences: \\text{grad}[pos] = \\text{data}[pos] - \\text{data}[pos - \\text{off}] Central Finite Differences ( scheme = 0 ) Computes the gradient using central finite differences: \\text{grad}[pos] = \\frac{\\text{data}[pos + \\text{off}] - \\text{data}[pos - \\text{off}]}{2} Forward Finite Differences ( scheme = 1 ) Computes the gradient using forward finite differences: \\text{grad}[pos] = \\text{data}[pos + \\text{off}] - \\text{data}[pos] Sobel Scheme ( scheme = 2 ) Utilizes Sobel operators to compute the gradient. Rotation Invariant Scheme ( scheme = 3 ) Uses a rotation-invariant kernels: \\text{Rotation-Invariant}_{x} = \\begin{bmatrix} -a & -b & -a \\\\ 0 & 0 & 0 \\\\ a & b & a \\end{bmatrix}\\\\ \\text{Rotation-Invariant}_{y} = \\text{Rotation-Invariant}_{x}^T where a = 0.25 \\times (2 - \\sqrt{2}) and b = 0.5 \\times (\\sqrt{2} - 1) . Deriche Recursive Filter : Introduced later. Van Vliet Recursive Filter : Introduced later. Scheme Applications Pros Cons Backward finite differences General-purpose Simple, easy to implement Less accurate, sensitive to noise Centered finite differences General-purpose More accurate than forward/backward Sensitive to noise Forward finite differences General-purpose Simple, easy to implement Less accurate, sensitive to noise Using Sobel kernels Edge detection Good at capturing edges, less noisy Can miss fine details Using rotation invariant Edge detection, texture analysis Rotation invariant, captures subtle edges More computationally expensive Using Deriche recursive filter Smoothing, edge detection Smooths noise, good edge detection Computationally expensive Using Van Vliet recursive filter Smoothing, edge detection Smooths noise, efficient computation Might blur some edges Example // Gradient approximation using centered finite differences. CImgList<> grad = imageIn.get_gradient(); // Norm and phase of the gradient. CImg<> norm = (grad[0].get_sqr() + grad[1].get_sqr()).sqrt(), phi = grad[1].get_atan2(grad[0]); Original Gradient X Gradient Y Gradient Norm Gradient Phase 4. Second-Order Derivatives Second-order derivatives are useful for detecting edges (when combined with thresholding and non-maximum suppression). However, they are more commonly used for feature detection, a topic that will be covered in Chapter 6. 4.1 Laplacian The Laplacian operator calculates the divergence of the gradient of the image, effectively highlighting regions where there is a rapid change in intensity. void Laplacian(CImg<> &imageIn) { CImg<> laplacian = imageIn.get_laplacian(); laplacian.normalize(0, 255).save(\"./results/lighthouse_laplacian.png\"); } Mathematically, it is represented as: \\nabla^2 f = \\frac{{\\partial^2 f}}{{\\partial x^2}} + \\frac{{\\partial^2 f}}{{\\partial y^2}} 4.2 Hessian The Hessian matrix consists of the second-order partial derivatives of the image. void Hessian(CImg<> &imageIn) { CImg<> Ixx = imageIn.get_hessian(\"xx\")[0]; // ... rest of the code ... } It is mathematically expressed as: \\mathbf{H} = \\begin{bmatrix} \\frac{{\\partial^2 f}}{{\\partial x^2}} & \\frac{{\\partial^2 f}}{{\\partial x \\partial y}} \\\\ \\frac{{\\partial^2 f}}{{\\partial y \\partial x}} & \\frac{{\\partial^2 f}}{{\\partial y^2}} \\end{bmatrix} Hessian XX Hessian YY Hessian XY 4.3 LoG (Laplacian of Gaussian) LoG combines Gaussian smoothing with the Laplacian operator. void LoG(CImg<> &imageIn) { CImg<> log = imageIn.get_blur(2).laplacian(); // ... rest of the code ... } The expression for LoG is: \\nabla^2 (G * f) = \\nabla^2 G * f where G is the Gaussian function. 4.4 DoG (Difference of Gaussian) DoG approximates the LoG by taking the difference between two blurred images with different standard deviations. void DoG(CImg<> &imageIn) { CImg<> gauss1 = imageIn.get_blur(1); CImg<> gauss2 = imageIn.get_blur(2); CImg<> dog = gauss1 - gauss2; // ... rest of the code ... } Mathematically, DoG is represented as: \\text{DoG} = (G_{\\sigma_1} * f) - (G_{\\sigma_2} * f) where G_{\\sigma_1} and G_{\\sigma_2} are Gaussian functions with standard deviations \\sigma_1 and \\sigma_2 , respectively. Summary 2nd-Order Derivative Applications Pros Cons Laplacian Edge Detection Sensitive to edges, Simple computation Noisy, Sensitive to noise Hessian Feature Detection Captures second-order information, Rich features Computationally expensive LoG (Laplacian of Gaussian) Edge Detection, Feature Detection Reduces noise, Effective edge detection Slower than DoG DoG (Difference of Gaussian) Edge Detection, Approximation of LoG Faster approximation of LoG Less accurate than LoG 5. Adaptive Filtering (Sigma Filter) Traditional smoothing filters often blur the edges along with reducing noise, causing a loss of important details. On the other hand, adaptive or other nonlinear filters like the sigma filter promise to reduce noise in images while preserving edges and contours. The method is based on the principle of weighting the influence of neighboring pixels based on the gradient, similar to some normalization or shunting mechanisms found in neuroscience. 1. Gradient Calculation First, the code calculates the gradient of the input image using the get_gradient() method: CImgList<> g = imgIn.get_gradient(); CImg<> grad = (g[0].get_sqr() + g[1].get_sqr()).sqrt(); The gradient, \\nabla f , quantifies the rate of change in pixel values across the image and is given by the formula: \\nabla f = \\sqrt{{\\left(\\frac{{\\partial f}}{{\\partial x}}\\right)}^2 + {\\left(\\frac{{\\partial f}}{{\\partial y}}\\right)}^2} 2. Sum of Gradients Next, the sum of gradients in a 3x3 neighborhood is computed: CImg<> Sgrad = grad.get_convolve(CImg<>(3, 3, 1, 1, 1)); This step convolves the gradient with a 3x3 filter, summing the neighboring gradients, effectively measuring local variations in pixel intensities. 3. Adaptive Weighting The code then applies adaptive weighting using the following lines: float epsilon = 100; CImg<> rap = imgIn.get_div(grad + epsilon); Here, the division acts as a weighting coefficient, with the epsilon term preventing division by zero. The weight of a pixel in the sum is inversely proportional to the local gradient: \\text{{weight}} = \\frac{{f}}{{\\nabla f + \\epsilon}} In rap , the pixels with high gradients are weighted less, but in this case, less actually means darker pixels! So, this code is emphasizing the edges and contours of the image. We started with this image: And ended up with this: 4. Smoothing Operation The smoothing operation is performed in the following lines: CImg_3x3(I, float); // declare Ipp, Ipc, etc. cimg_for3x3(rap, x, y, 0, 0, I, float) imgOut(x, y) = (Ipp + Ipc + Ipn + Icp + Icc + Icn + Inp + Inc + Inn) / (Sgrad(x, y) + epsilon); Here, CImg_3x3(I, float); declares variables like Ipp , Ipc , etc., representing the neighboring pixels. The cimg_for3x3 macro iterates through the image, applying the smoothing operation. The numerator is a simple average (actually sum) filter, and the bottom term is larger for pixels with high gradients. Therefore, we emphasize the edges and contours again by making those pixels darker. The final formula for smoothing is: \\text{{imgOut}}(x, y) = \\frac{{\\sum \\text{{neighboring pixels}}}}{{\\text{{Sgrad}}(x, y) + \\epsilon}} As you can see, the noise are amplified along with the edges are emphasized. This adaptive sigma filter may not be the best choice for this image. The somewhat cartoon-like appearance of the final output image is a common effect of adaptive smoothing techniques like the sigma filter. By emphasizing edges and smoothing uniform areas, the image can take on a more stylized or abstract appearance. Connection to Neuroscience The adaptive nature of this method is akin to the way some neurons modulate their response based on local activity. It aligns with principles observed in neuroscience where the influence of neighboring neurons is normalized or shunted based on the local context, allowing for a balance between sensitivity to stimuli and adaptation to the local environment. 6. Adaptive Window Filters Adaptive window filters are smart filters that change their behavior based on the characteristics of the area they are working on. They look at each pixel and decide the best way to smooth or sharpen it based on the pixels around it. There are different ways to do this, and here are three examples using the following noisy image (Gaussian noise \\sigma = 40 ): 6.1 Nagao Filter Imagine you have a small grid (usually 5x5) around a pixel in the middle. In this grid, you create 9 different windows (smaller groups of pixels), each containing 9 pixels. Here we first intialize the windows: CImgList<unsigned char> Nagao(9, 5, 5, 1, 1, 0); Nagao(0, 0, 0) = Nagao(0, 0, 1) = Nagao(0, 0, 2) = Nagao(0, 0, 3) = Nagao(0, 0, 4) = Nagao(0, 1, 1) = Nagao(0, 1, 2) = Nagao(0, 1, 3) = Nagao(0, 2, 2) = 1; for (int i = 1; i < 4; ++i) Nagao[i] = Nagao[0].get_rotate(i * 90); Nagao(4, 1, 1) = Nagao(4, 1, 2) = Nagao(4, 1, 3) = Nagao(4, 2, 1) = Nagao(4, 2, 2) = Nagao(4, 2, 3) = Nagao(4, 3, 1) = Nagao(4, 3, 2) = Nagao(4, 3, 3) = 1; Nagao(5, 0, 0) = Nagao(5, 0, 1) = Nagao(5, 0, 2) = Nagao(5, 1, 0) = Nagao(5, 1, 1) = Nagao(5, 1, 2) = Nagao(5, 2, 0) = Nagao(5, 2, 1) = Nagao(5, 2, 2) = 1; for (int i = 1; i < 4; ++i) Nagao[5 + i] = Nagao[5].get_rotate(i * 90); For each window: You calculate the average color (mean) and how much the colors vary (variance). You pick the window where the colors vary the least (smallest variance). You replace the middle pixel with the average color from that chosen window. Here is the code: CImg<> mu(9, 1, 1, 1, 0), sigma(9, 1, 1, 1, 0), st, N(5, 5); CImg<int> permutations; cimg_for5x5(imgIn, x, y, 0, 0, N, float) { CImgList<> res(9); for (int i = 0; i < 9; ++i) { res[i] = N.get_mul(Nagao[i]); st = res[i].get_stats(); mu[i] = st[2]; sigma[i] = st[3]; } // Searching minimal variance. sigma.sort(permutations); imgOut(x, y) = mu[permutations[0]]; } This method helps to keep the details in the image while reducing noise. 6.2 Kuwahara Filter The Kuwahara filter is like the Nagao filter, but only uses windows 5 - 8 of Nagao filter. It's just a variation that might work better on certain types of images. The basic idea of finding the least varying window and using its average color remains the same. 7. Deriche Recursive Filters John Canny's work on edge detection is not only confined to the Canny edge detector but also includes mathematical foundations for defining the criteria of an effective edge detector. Canny outlined three criteria: good detection, good localization, and a single response. The first two criteria can be combined to yield a so-called Canny criterion value , adding mathematical rigor to the field of edge detection. The Deriche recursive filter is an recursive solution to Canny's criteria. However, in the book, Deriche filter is presented as a family of recursive (and potentially more efficient) alternatives to smoothing operation (0-th order), gradient computation (1st order), and Laplacian computation (2nd order). The function CImg<T>& deriche(const float sigma, const unsigned int order=0, const char axis='x') filter applies in one direction at once. sigma is the standard deviation of the filter, while order is the order of the derivative to compute. axis is the axis along which the filter is applied. 7.1 Smoothing (0-th Order) CImg<> img_deriche0 = imgIn.get_deriche(SIGMA, 0, 'x'); img_deriche0.deriche(SIGMA, 0, 'y'); 7.2 Gradient Computation (1st Order) CImg<> img_deriche1 = imgIn.get_deriche(SIGMA, 1, 'x'); img_deriche1.deriche(SIGMA, 1, 'y'); CImg<> img_deriche1_norm = (img_deriche1.get_sqr() += img_deriche1.get_sqr()).sqrt(); 7.3 Laplacian Computation (2nd Order) CImg<> img_deriche2_x = imgIn.get_deriche(SIGMA, 2, 'x'); CImg<> img_deriche2_y = imgIn.get_deriche(SIGMA, 2, 'y'); CImg<> img_deriche2_laplacian = img_deriche2_x + img_deriche2_y; 8. Frequency Domain Filtering 8.1 Using CImg<>::FFT() Load the image and convert to grayscale CImg<unsigned char> img(\"../images/lighthouse.png\"); CImg<> lum = img.get_norm().blur(0.75f); Resize the image FFT requires dimensions to be a power of 2, so resize the image to meet this requirement. int width = 1 << static_cast<int>(std::ceil(std::log2(lum.width()))); int height = 1 << static_cast<int>(std::ceil(std::log2(lum.height()))); lum.resize(width, height, -100, -100, 0); Compute the FFT CImgList<> fft = lum.get_FFT(); Process Magnitude Take the logarithm of the magnitude part to better visualize it, and then shift the zero frequency component to the center of the spectrum. CImg<> magnitude(fft[0]); magnitude += 1; // Avoid log(0) magnitude.log(); magnitude.shift(magnitude.width() / 2, magnitude.height() / 2, 0, 0, 2); (Optional) Compute Inverse FFT Perform the inverse FFT to recover the original image. CImg<> img_ifft = fft.get_FFT(true)[0]; 8.2 Butterworth Filters The idea of frequency-domain filtering involves first transforming the image into the frequency domain, then multiplying the image with a mask, and finally transforming the image back to the spatial domain. Theoretically, this is equivalent to convolving the image with the mask in the spatial domain. It might sound like a lot of extra work, but the Fast Fourier Transform (FFT) makes it much faster. However, you need to be mindful of artifacts such as the Gibbs phenomenon, which can be reduced by using appropriate windowing functions. Butterworth filters are a family of filters characterized by a maximally flat frequency response. They can be applied in both analog and digital forms, and in both time and frequency domains. In the analog frequency domain, the 2D transfer function for a Butterworth low-pass filter can be represented as: H(u,v) = \\frac{1}{1 + \\left( \\frac{D(u,v)}{D_0} \\right)^{2n}} where D(u,v) = \\sqrt{u^2 + v^2} is the distance from the origin in the frequency domain, and D_0 is the cutoff frequency. The parameter n is the order of the filter. Increasing n will result in a steeper roll-off, but at the expense of increased complexity, potential instability, and possible phase distortion. In the spatial domain, you would have to use the inverse Fourier transform to obtain a corresponding difference equation. 8.3 Gaussian Filters The Gaussian filter can be implemented in both time and frequency domains Here's how to apply a Gaussian filter in the frequency domain: Perform the Fast Fourier Transform (FFT) First, compute the FFT of the input image. CImgList<> fImg = imgIn.get_FFT(); Create the Gaussian Mask Construct the frequency response of the filter using the Gaussian function. The Gaussian mask is defined in the frequency domain, and sigma is the standard deviation controlling the spread of the Gaussian function. In the frequency domain, the filter is described by the Gaussian function: H(u, v) = 2\\pi \\sigma^2 \\exp\\left(-2\\pi^2\\sigma^2\\left(\\left(\\frac{u}{W} - 0.5\\right)^2 + \\left(\\frac{v}{H} - 0.5\\right)^2\\right)\\right) Here, W and H are the width and height of the image, \\sigma^2 is the squared standard deviation, and (u,v) are the frequency coordinates. CImg<> gaussMask(imgIn.width(), imgIn.height()); float sigma2 = cimg::sqr(sigma); cimg_forXY(gaussMask, x, y) { float fx = x / (float)imgIn.width() - 0.5f, fx2 = cimg::sqr(fx), fy = y / (float)imgIn.height() - 0.5f, fy2 = cimg::sqr(fy); gaussMask(x, y) = 2 * cimg::PI * sigma2 * std::exp(-2 * cimg::sqr(cimg::PI) * sigma2 * (fx2 + fy2)); } Zero Shift the Gaussian Mask Shift the Gaussian mask by half its width and height to center the zero frequency. // Zero shift. gaussMask.shift(-imgIn.width() / 2, -imgIn.height() / 2, 0, 0, 2); Apply the Filter Perform the element-wise multiplication of the Fourier Transformed image and the Gaussian mask for both the magnitude and phase components. cimglist_for(fImg, k) fImg[k].mul(gaussMask); Inverse FFT and Normalize Transform back to the spatial domain via inverse FFT and normalize the result. // Inverse FFT and real part. return fImg.get_FFT(true)[0].normalize(0, 255); 9. Diffusion Filtering The term \"diffusion\" in the context of diffusion filtering is analogous to the physical process of diffusion, but with pixel values rather than physical particles. The idea is that the pixel values in an image will \"diffuse\" from areas of high intensity to areas of low intensity, smoothing the image. However, unlike simple linear filtering, diffusion filtering can be controlled to be anisotropic , meaning it can prefer certain directions over others. This allows the diffusion to be guided by the content of the image, smoothing some areas while preserving edges and details. The diffusion filter introduced in the book, Perona and Malik's algorithm, is nonlinear and adaptive. ( Note: nonlinearity does not necessarily guarantee adaptivity, nor the other way around. Filters can be simultaneously linear and adaptive, like the least mean squares (LMS) filter, or nonlinear but non-adaptive, like the median filter. )","title":"5. Filtering"},{"location":"05_filtering/#filtering-learning-reflection","text":"Author : Tony Fu Date : August 21, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub Reference : Chapter 5 Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library by Tschumperl\u00e9, Tilmant, Barra","title":"Filtering - Learning Reflection"},{"location":"05_filtering/#1-convolution","text":"CImg<> sobel(3, 3, 1, 1, 0); sobel(0, 0) = -1; sobel(0, 1) = -2; sobel(0, 2) = -1; sobel(1, 0) = 0; sobel(1, 1) = 0; sobel(1, 2) = 0; sobel(2, 0) = 1; sobel(2, 1) = 2; sobel(2, 2) = 1; imgIn.convolve(sobel); The above code snippet shows how to perform a convolution on an image with a 3x3 Sobel filter. The result is shown below: Original : Convolution :","title":"1. Convolution"},{"location":"05_filtering/#boundary-conditions","text":"Boundary conditions specify how to handle the edges and can be specified using the const usigned int boundary_conditions parameter of the convolve() method. The four boundary conditions provided by the CImg library have specific meanings: Dirichlet (0): The pixels outside the image boundaries are considered to be zero. This creates a sort of \"hard\" edge around the image and can lead to noticeable artifacts along the borders. Neumann (1) (default): The value of the border pixels is extended outside the image boundaries. Essentially, this reflects the gradient at the border, assuming that the intensity of the image doesn't change beyond the edge. This is the default boundary condition in CImg and tends to provide visually acceptable results. Periodic (2): The image is treated as if it were tiling the plane in a repeated pattern. This means that the pixels on the right edge of the image are used as the boundary condition for the left edge, and the pixels on the bottom are used for the top. This can create seamless transitions but can also lead to strange effects if the image does not naturally tile. Mirror (3): The pixels outside the image boundaries are determined by mirroring the pixels inside the boundaries. Imagine folding the image over at its edges, so the pixels just inside the border are duplicated just outside the border. This can create a more visually smooth transition at the edges but may not be appropriate for all types of images.","title":"Boundary Conditions"},{"location":"05_filtering/#2-median-fitler","text":"img.blur_median(3); The above code snippet shows how to perform a median filter on an image with a 3x3 window. The result is shown below: Origin : Median Filter :","title":"2. Median Fitler"},{"location":"05_filtering/#order-statistic-os-filter","text":"An OS filter (Bovik, Huang, and Munson, 1983) is a non-linear filter that computes a linear combination of these sorted values: F = w_1 \\cdot s_1 + w_2 \\cdot s_2 + \\ldots + w_n \\cdot s_n where w_i are the weights that define how much each ordered value contributes to the final result, and s_i are the sorted values of the neighborhood pixels.","title":"Order-Statistic (OS) Filter"},{"location":"05_filtering/#3-first-order-derivatives","text":"The get_gradient function computes the image gradient along specified axes using different numerical schemes.","title":"3. First-Order Derivatives"},{"location":"05_filtering/#parameters","text":"axes : Axes considered for the gradient computation (e.g., \"xy\"). scheme : Numerical scheme used for the gradient computation. Options are:","title":"Parameters"},{"location":"05_filtering/#schemes","text":"Backward Finite Differences ( scheme = -1 ) Computes the gradient using backward finite differences: \\text{grad}[pos] = \\text{data}[pos] - \\text{data}[pos - \\text{off}] Central Finite Differences ( scheme = 0 ) Computes the gradient using central finite differences: \\text{grad}[pos] = \\frac{\\text{data}[pos + \\text{off}] - \\text{data}[pos - \\text{off}]}{2} Forward Finite Differences ( scheme = 1 ) Computes the gradient using forward finite differences: \\text{grad}[pos] = \\text{data}[pos + \\text{off}] - \\text{data}[pos] Sobel Scheme ( scheme = 2 ) Utilizes Sobel operators to compute the gradient. Rotation Invariant Scheme ( scheme = 3 ) Uses a rotation-invariant kernels: \\text{Rotation-Invariant}_{x} = \\begin{bmatrix} -a & -b & -a \\\\ 0 & 0 & 0 \\\\ a & b & a \\end{bmatrix}\\\\ \\text{Rotation-Invariant}_{y} = \\text{Rotation-Invariant}_{x}^T where a = 0.25 \\times (2 - \\sqrt{2}) and b = 0.5 \\times (\\sqrt{2} - 1) . Deriche Recursive Filter : Introduced later. Van Vliet Recursive Filter : Introduced later. Scheme Applications Pros Cons Backward finite differences General-purpose Simple, easy to implement Less accurate, sensitive to noise Centered finite differences General-purpose More accurate than forward/backward Sensitive to noise Forward finite differences General-purpose Simple, easy to implement Less accurate, sensitive to noise Using Sobel kernels Edge detection Good at capturing edges, less noisy Can miss fine details Using rotation invariant Edge detection, texture analysis Rotation invariant, captures subtle edges More computationally expensive Using Deriche recursive filter Smoothing, edge detection Smooths noise, good edge detection Computationally expensive Using Van Vliet recursive filter Smoothing, edge detection Smooths noise, efficient computation Might blur some edges","title":"Schemes"},{"location":"05_filtering/#example","text":"// Gradient approximation using centered finite differences. CImgList<> grad = imageIn.get_gradient(); // Norm and phase of the gradient. CImg<> norm = (grad[0].get_sqr() + grad[1].get_sqr()).sqrt(), phi = grad[1].get_atan2(grad[0]); Original Gradient X Gradient Y Gradient Norm Gradient Phase","title":"Example"},{"location":"05_filtering/#4-second-order-derivatives","text":"Second-order derivatives are useful for detecting edges (when combined with thresholding and non-maximum suppression). However, they are more commonly used for feature detection, a topic that will be covered in Chapter 6.","title":"4. Second-Order Derivatives"},{"location":"05_filtering/#41-laplacian","text":"The Laplacian operator calculates the divergence of the gradient of the image, effectively highlighting regions where there is a rapid change in intensity. void Laplacian(CImg<> &imageIn) { CImg<> laplacian = imageIn.get_laplacian(); laplacian.normalize(0, 255).save(\"./results/lighthouse_laplacian.png\"); } Mathematically, it is represented as: \\nabla^2 f = \\frac{{\\partial^2 f}}{{\\partial x^2}} + \\frac{{\\partial^2 f}}{{\\partial y^2}}","title":"4.1 Laplacian"},{"location":"05_filtering/#42-hessian","text":"The Hessian matrix consists of the second-order partial derivatives of the image. void Hessian(CImg<> &imageIn) { CImg<> Ixx = imageIn.get_hessian(\"xx\")[0]; // ... rest of the code ... } It is mathematically expressed as: \\mathbf{H} = \\begin{bmatrix} \\frac{{\\partial^2 f}}{{\\partial x^2}} & \\frac{{\\partial^2 f}}{{\\partial x \\partial y}} \\\\ \\frac{{\\partial^2 f}}{{\\partial y \\partial x}} & \\frac{{\\partial^2 f}}{{\\partial y^2}} \\end{bmatrix} Hessian XX Hessian YY Hessian XY","title":"4.2 Hessian"},{"location":"05_filtering/#43-log-laplacian-of-gaussian","text":"LoG combines Gaussian smoothing with the Laplacian operator. void LoG(CImg<> &imageIn) { CImg<> log = imageIn.get_blur(2).laplacian(); // ... rest of the code ... } The expression for LoG is: \\nabla^2 (G * f) = \\nabla^2 G * f where G is the Gaussian function.","title":"4.3 LoG (Laplacian of Gaussian)"},{"location":"05_filtering/#44-dog-difference-of-gaussian","text":"DoG approximates the LoG by taking the difference between two blurred images with different standard deviations. void DoG(CImg<> &imageIn) { CImg<> gauss1 = imageIn.get_blur(1); CImg<> gauss2 = imageIn.get_blur(2); CImg<> dog = gauss1 - gauss2; // ... rest of the code ... } Mathematically, DoG is represented as: \\text{DoG} = (G_{\\sigma_1} * f) - (G_{\\sigma_2} * f) where G_{\\sigma_1} and G_{\\sigma_2} are Gaussian functions with standard deviations \\sigma_1 and \\sigma_2 , respectively.","title":"4.4 DoG (Difference of Gaussian)"},{"location":"05_filtering/#summary","text":"2nd-Order Derivative Applications Pros Cons Laplacian Edge Detection Sensitive to edges, Simple computation Noisy, Sensitive to noise Hessian Feature Detection Captures second-order information, Rich features Computationally expensive LoG (Laplacian of Gaussian) Edge Detection, Feature Detection Reduces noise, Effective edge detection Slower than DoG DoG (Difference of Gaussian) Edge Detection, Approximation of LoG Faster approximation of LoG Less accurate than LoG","title":"Summary"},{"location":"05_filtering/#5-adaptive-filtering-sigma-filter","text":"Traditional smoothing filters often blur the edges along with reducing noise, causing a loss of important details. On the other hand, adaptive or other nonlinear filters like the sigma filter promise to reduce noise in images while preserving edges and contours. The method is based on the principle of weighting the influence of neighboring pixels based on the gradient, similar to some normalization or shunting mechanisms found in neuroscience.","title":"5. Adaptive Filtering (Sigma Filter)"},{"location":"05_filtering/#1-gradient-calculation","text":"First, the code calculates the gradient of the input image using the get_gradient() method: CImgList<> g = imgIn.get_gradient(); CImg<> grad = (g[0].get_sqr() + g[1].get_sqr()).sqrt(); The gradient, \\nabla f , quantifies the rate of change in pixel values across the image and is given by the formula: \\nabla f = \\sqrt{{\\left(\\frac{{\\partial f}}{{\\partial x}}\\right)}^2 + {\\left(\\frac{{\\partial f}}{{\\partial y}}\\right)}^2}","title":"1. Gradient Calculation"},{"location":"05_filtering/#2-sum-of-gradients","text":"Next, the sum of gradients in a 3x3 neighborhood is computed: CImg<> Sgrad = grad.get_convolve(CImg<>(3, 3, 1, 1, 1)); This step convolves the gradient with a 3x3 filter, summing the neighboring gradients, effectively measuring local variations in pixel intensities.","title":"2. Sum of Gradients"},{"location":"05_filtering/#3-adaptive-weighting","text":"The code then applies adaptive weighting using the following lines: float epsilon = 100; CImg<> rap = imgIn.get_div(grad + epsilon); Here, the division acts as a weighting coefficient, with the epsilon term preventing division by zero. The weight of a pixel in the sum is inversely proportional to the local gradient: \\text{{weight}} = \\frac{{f}}{{\\nabla f + \\epsilon}} In rap , the pixels with high gradients are weighted less, but in this case, less actually means darker pixels! So, this code is emphasizing the edges and contours of the image. We started with this image: And ended up with this:","title":"3. Adaptive Weighting"},{"location":"05_filtering/#4-smoothing-operation","text":"The smoothing operation is performed in the following lines: CImg_3x3(I, float); // declare Ipp, Ipc, etc. cimg_for3x3(rap, x, y, 0, 0, I, float) imgOut(x, y) = (Ipp + Ipc + Ipn + Icp + Icc + Icn + Inp + Inc + Inn) / (Sgrad(x, y) + epsilon); Here, CImg_3x3(I, float); declares variables like Ipp , Ipc , etc., representing the neighboring pixels. The cimg_for3x3 macro iterates through the image, applying the smoothing operation. The numerator is a simple average (actually sum) filter, and the bottom term is larger for pixels with high gradients. Therefore, we emphasize the edges and contours again by making those pixels darker. The final formula for smoothing is: \\text{{imgOut}}(x, y) = \\frac{{\\sum \\text{{neighboring pixels}}}}{{\\text{{Sgrad}}(x, y) + \\epsilon}} As you can see, the noise are amplified along with the edges are emphasized. This adaptive sigma filter may not be the best choice for this image. The somewhat cartoon-like appearance of the final output image is a common effect of adaptive smoothing techniques like the sigma filter. By emphasizing edges and smoothing uniform areas, the image can take on a more stylized or abstract appearance.","title":"4. Smoothing Operation"},{"location":"05_filtering/#connection-to-neuroscience","text":"The adaptive nature of this method is akin to the way some neurons modulate their response based on local activity. It aligns with principles observed in neuroscience where the influence of neighboring neurons is normalized or shunted based on the local context, allowing for a balance between sensitivity to stimuli and adaptation to the local environment.","title":"Connection to Neuroscience"},{"location":"05_filtering/#6-adaptive-window-filters","text":"Adaptive window filters are smart filters that change their behavior based on the characteristics of the area they are working on. They look at each pixel and decide the best way to smooth or sharpen it based on the pixels around it. There are different ways to do this, and here are three examples using the following noisy image (Gaussian noise \\sigma = 40 ):","title":"6. Adaptive Window Filters"},{"location":"05_filtering/#61-nagao-filter","text":"Imagine you have a small grid (usually 5x5) around a pixel in the middle. In this grid, you create 9 different windows (smaller groups of pixels), each containing 9 pixels. Here we first intialize the windows: CImgList<unsigned char> Nagao(9, 5, 5, 1, 1, 0); Nagao(0, 0, 0) = Nagao(0, 0, 1) = Nagao(0, 0, 2) = Nagao(0, 0, 3) = Nagao(0, 0, 4) = Nagao(0, 1, 1) = Nagao(0, 1, 2) = Nagao(0, 1, 3) = Nagao(0, 2, 2) = 1; for (int i = 1; i < 4; ++i) Nagao[i] = Nagao[0].get_rotate(i * 90); Nagao(4, 1, 1) = Nagao(4, 1, 2) = Nagao(4, 1, 3) = Nagao(4, 2, 1) = Nagao(4, 2, 2) = Nagao(4, 2, 3) = Nagao(4, 3, 1) = Nagao(4, 3, 2) = Nagao(4, 3, 3) = 1; Nagao(5, 0, 0) = Nagao(5, 0, 1) = Nagao(5, 0, 2) = Nagao(5, 1, 0) = Nagao(5, 1, 1) = Nagao(5, 1, 2) = Nagao(5, 2, 0) = Nagao(5, 2, 1) = Nagao(5, 2, 2) = 1; for (int i = 1; i < 4; ++i) Nagao[5 + i] = Nagao[5].get_rotate(i * 90); For each window: You calculate the average color (mean) and how much the colors vary (variance). You pick the window where the colors vary the least (smallest variance). You replace the middle pixel with the average color from that chosen window. Here is the code: CImg<> mu(9, 1, 1, 1, 0), sigma(9, 1, 1, 1, 0), st, N(5, 5); CImg<int> permutations; cimg_for5x5(imgIn, x, y, 0, 0, N, float) { CImgList<> res(9); for (int i = 0; i < 9; ++i) { res[i] = N.get_mul(Nagao[i]); st = res[i].get_stats(); mu[i] = st[2]; sigma[i] = st[3]; } // Searching minimal variance. sigma.sort(permutations); imgOut(x, y) = mu[permutations[0]]; } This method helps to keep the details in the image while reducing noise.","title":"6.1 Nagao Filter"},{"location":"05_filtering/#62-kuwahara-filter","text":"The Kuwahara filter is like the Nagao filter, but only uses windows 5 - 8 of Nagao filter. It's just a variation that might work better on certain types of images. The basic idea of finding the least varying window and using its average color remains the same.","title":"6.2 Kuwahara Filter"},{"location":"05_filtering/#7-deriche-recursive-filters","text":"John Canny's work on edge detection is not only confined to the Canny edge detector but also includes mathematical foundations for defining the criteria of an effective edge detector. Canny outlined three criteria: good detection, good localization, and a single response. The first two criteria can be combined to yield a so-called Canny criterion value , adding mathematical rigor to the field of edge detection. The Deriche recursive filter is an recursive solution to Canny's criteria. However, in the book, Deriche filter is presented as a family of recursive (and potentially more efficient) alternatives to smoothing operation (0-th order), gradient computation (1st order), and Laplacian computation (2nd order). The function CImg<T>& deriche(const float sigma, const unsigned int order=0, const char axis='x') filter applies in one direction at once. sigma is the standard deviation of the filter, while order is the order of the derivative to compute. axis is the axis along which the filter is applied.","title":"7. Deriche Recursive Filters"},{"location":"05_filtering/#71-smoothing-0-th-order","text":"CImg<> img_deriche0 = imgIn.get_deriche(SIGMA, 0, 'x'); img_deriche0.deriche(SIGMA, 0, 'y');","title":"7.1 Smoothing (0-th Order)"},{"location":"05_filtering/#72-gradient-computation-1st-order","text":"CImg<> img_deriche1 = imgIn.get_deriche(SIGMA, 1, 'x'); img_deriche1.deriche(SIGMA, 1, 'y'); CImg<> img_deriche1_norm = (img_deriche1.get_sqr() += img_deriche1.get_sqr()).sqrt();","title":"7.2 Gradient Computation (1st Order)"},{"location":"05_filtering/#73-laplacian-computation-2nd-order","text":"CImg<> img_deriche2_x = imgIn.get_deriche(SIGMA, 2, 'x'); CImg<> img_deriche2_y = imgIn.get_deriche(SIGMA, 2, 'y'); CImg<> img_deriche2_laplacian = img_deriche2_x + img_deriche2_y;","title":"7.3 Laplacian Computation (2nd Order)"},{"location":"05_filtering/#8-frequency-domain-filtering","text":"","title":"8. Frequency Domain Filtering"},{"location":"05_filtering/#81-using-cimgfft","text":"","title":"8.1 Using CImg&lt;&gt;::FFT()"},{"location":"05_filtering/#load-the-image-and-convert-to-grayscale","text":"CImg<unsigned char> img(\"../images/lighthouse.png\"); CImg<> lum = img.get_norm().blur(0.75f);","title":"Load the image and convert to grayscale"},{"location":"05_filtering/#resize-the-image","text":"FFT requires dimensions to be a power of 2, so resize the image to meet this requirement. int width = 1 << static_cast<int>(std::ceil(std::log2(lum.width()))); int height = 1 << static_cast<int>(std::ceil(std::log2(lum.height()))); lum.resize(width, height, -100, -100, 0);","title":"Resize the image"},{"location":"05_filtering/#compute-the-fft","text":"CImgList<> fft = lum.get_FFT();","title":"Compute the FFT"},{"location":"05_filtering/#process-magnitude","text":"Take the logarithm of the magnitude part to better visualize it, and then shift the zero frequency component to the center of the spectrum. CImg<> magnitude(fft[0]); magnitude += 1; // Avoid log(0) magnitude.log(); magnitude.shift(magnitude.width() / 2, magnitude.height() / 2, 0, 0, 2);","title":"Process Magnitude"},{"location":"05_filtering/#optional-compute-inverse-fft","text":"Perform the inverse FFT to recover the original image. CImg<> img_ifft = fft.get_FFT(true)[0];","title":"(Optional) Compute Inverse FFT"},{"location":"05_filtering/#82-butterworth-filters","text":"The idea of frequency-domain filtering involves first transforming the image into the frequency domain, then multiplying the image with a mask, and finally transforming the image back to the spatial domain. Theoretically, this is equivalent to convolving the image with the mask in the spatial domain. It might sound like a lot of extra work, but the Fast Fourier Transform (FFT) makes it much faster. However, you need to be mindful of artifacts such as the Gibbs phenomenon, which can be reduced by using appropriate windowing functions. Butterworth filters are a family of filters characterized by a maximally flat frequency response. They can be applied in both analog and digital forms, and in both time and frequency domains. In the analog frequency domain, the 2D transfer function for a Butterworth low-pass filter can be represented as: H(u,v) = \\frac{1}{1 + \\left( \\frac{D(u,v)}{D_0} \\right)^{2n}} where D(u,v) = \\sqrt{u^2 + v^2} is the distance from the origin in the frequency domain, and D_0 is the cutoff frequency. The parameter n is the order of the filter. Increasing n will result in a steeper roll-off, but at the expense of increased complexity, potential instability, and possible phase distortion. In the spatial domain, you would have to use the inverse Fourier transform to obtain a corresponding difference equation.","title":"8.2 Butterworth Filters"},{"location":"05_filtering/#83-gaussian-filters","text":"The Gaussian filter can be implemented in both time and frequency domains Here's how to apply a Gaussian filter in the frequency domain:","title":"8.3 Gaussian Filters"},{"location":"05_filtering/#perform-the-fast-fourier-transform-fft","text":"First, compute the FFT of the input image. CImgList<> fImg = imgIn.get_FFT();","title":"Perform the Fast Fourier Transform (FFT)"},{"location":"05_filtering/#create-the-gaussian-mask","text":"Construct the frequency response of the filter using the Gaussian function. The Gaussian mask is defined in the frequency domain, and sigma is the standard deviation controlling the spread of the Gaussian function. In the frequency domain, the filter is described by the Gaussian function: H(u, v) = 2\\pi \\sigma^2 \\exp\\left(-2\\pi^2\\sigma^2\\left(\\left(\\frac{u}{W} - 0.5\\right)^2 + \\left(\\frac{v}{H} - 0.5\\right)^2\\right)\\right) Here, W and H are the width and height of the image, \\sigma^2 is the squared standard deviation, and (u,v) are the frequency coordinates. CImg<> gaussMask(imgIn.width(), imgIn.height()); float sigma2 = cimg::sqr(sigma); cimg_forXY(gaussMask, x, y) { float fx = x / (float)imgIn.width() - 0.5f, fx2 = cimg::sqr(fx), fy = y / (float)imgIn.height() - 0.5f, fy2 = cimg::sqr(fy); gaussMask(x, y) = 2 * cimg::PI * sigma2 * std::exp(-2 * cimg::sqr(cimg::PI) * sigma2 * (fx2 + fy2)); }","title":"Create the Gaussian Mask"},{"location":"05_filtering/#zero-shift-the-gaussian-mask","text":"Shift the Gaussian mask by half its width and height to center the zero frequency. // Zero shift. gaussMask.shift(-imgIn.width() / 2, -imgIn.height() / 2, 0, 0, 2);","title":"Zero Shift the Gaussian Mask"},{"location":"05_filtering/#apply-the-filter","text":"Perform the element-wise multiplication of the Fourier Transformed image and the Gaussian mask for both the magnitude and phase components. cimglist_for(fImg, k) fImg[k].mul(gaussMask);","title":"Apply the Filter"},{"location":"05_filtering/#inverse-fft-and-normalize","text":"Transform back to the spatial domain via inverse FFT and normalize the result. // Inverse FFT and real part. return fImg.get_FFT(true)[0].normalize(0, 255);","title":"Inverse FFT and Normalize"},{"location":"05_filtering/#9-diffusion-filtering","text":"The term \"diffusion\" in the context of diffusion filtering is analogous to the physical process of diffusion, but with pixel values rather than physical particles. The idea is that the pixel values in an image will \"diffuse\" from areas of high intensity to areas of low intensity, smoothing the image. However, unlike simple linear filtering, diffusion filtering can be controlled to be anisotropic , meaning it can prefer certain directions over others. This allows the diffusion to be guided by the content of the image, smoothing some areas while preserving edges and details. The diffusion filter introduced in the book, Perona and Malik's algorithm, is nonlinear and adaptive. ( Note: nonlinearity does not necessarily guarantee adaptivity, nor the other way around. Filters can be simultaneously linear and adaptive, like the least mean squares (LMS) filter, or nonlinear but non-adaptive, like the median filter. )","title":"9. Diffusion Filtering"},{"location":"appendix_1/","text":"Math Expressions in CImg's Fill Method Author : Tony Fu Date : August 19, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub This page provides a guide to using mathematical expressions within the CImg<unsigned char>::fill() method of the CImg library. Syntax and Operators Mathematical expressions can include the following: Basic arithmetic: + , - , * , / , % Trigonometric functions: sin , cos , tan , etc. Logarithmic functions: log , exp , etc. Variables I : Represents the current pixel value. For example, I*2 would double the intensity of every pixel. J(x, y) : Refers to the neighboring pixel values at relative coordinates (x, y) x , y , z , c : Represents the coordinates of the current pixel. Conditional Expressions You can use conditional expressions like condition ? value_if_true : value_if_false . Boolean Logic Use logical operators like && (AND), || (OR), and ! (NOT) for conditional logic, and comparison operators ( == , != , < , <= , > , >= ) are also available. Examples 0. Original Image 1. Doubling the Intensity of an Image CImg<unsigned char> img(\"image.jpg\"); img.fill(\"I*2\", true); 2. Inverting an Image CImg<unsigned char> img(\"image.jpg\"); img.fill(\"255-I\", true); 3. Spiral Effect CImg<unsigned char> img(\"image.jpg\"); img_spiral.fill(\"(x*y)%500\",true); 4. Conditional Operations CImg<unsigned char> img(\"image.jpg\"); img.fill(\"I*(I!=J(1,0) || I!=J(0,1)?0:1)\", true);","title":"Appendix 1 - Math Expressions in CImg's Fill Method"},{"location":"appendix_1/#math-expressions-in-cimgs-fill-method","text":"Author : Tony Fu Date : August 19, 2023 Device : MacBook Pro 16-inch, Late 2021 (M1 Pro) Code : GitHub This page provides a guide to using mathematical expressions within the CImg<unsigned char>::fill() method of the CImg library.","title":"Math Expressions in CImg's Fill Method"},{"location":"appendix_1/#syntax-and-operators","text":"Mathematical expressions can include the following: Basic arithmetic: + , - , * , / , % Trigonometric functions: sin , cos , tan , etc. Logarithmic functions: log , exp , etc.","title":"Syntax and Operators"},{"location":"appendix_1/#variables","text":"I : Represents the current pixel value. For example, I*2 would double the intensity of every pixel. J(x, y) : Refers to the neighboring pixel values at relative coordinates (x, y) x , y , z , c : Represents the coordinates of the current pixel.","title":"Variables"},{"location":"appendix_1/#conditional-expressions","text":"You can use conditional expressions like condition ? value_if_true : value_if_false .","title":"Conditional Expressions"},{"location":"appendix_1/#boolean-logic","text":"Use logical operators like && (AND), || (OR), and ! (NOT) for conditional logic, and comparison operators ( == , != , < , <= , > , >= ) are also available.","title":"Boolean Logic"},{"location":"appendix_1/#examples","text":"","title":"Examples"},{"location":"appendix_1/#0-original-image","text":"","title":"0. Original Image"},{"location":"appendix_1/#1-doubling-the-intensity-of-an-image","text":"CImg<unsigned char> img(\"image.jpg\"); img.fill(\"I*2\", true);","title":"1. Doubling the Intensity of an Image"},{"location":"appendix_1/#2-inverting-an-image","text":"CImg<unsigned char> img(\"image.jpg\"); img.fill(\"255-I\", true);","title":"2. Inverting an Image"},{"location":"appendix_1/#3-spiral-effect","text":"CImg<unsigned char> img(\"image.jpg\"); img_spiral.fill(\"(x*y)%500\",true);","title":"3. Spiral Effect"},{"location":"appendix_1/#4-conditional-operations","text":"CImg<unsigned char> img(\"image.jpg\"); img.fill(\"I*(I!=J(1,0) || I!=J(0,1)?0:1)\", true);","title":"4. Conditional Operations"}]}
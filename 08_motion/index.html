<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Tony Fu" /><link rel="canonical" href="https://tonyfu97.github.io/Digital-Image-Processing/08_motion/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>8. Motion Estimation - Digital Image Processing Notes</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "8. Motion Estimation";
        var mkdocs_page_input_path = "08_motion.md";
        var mkdocs_page_url = "/Digital-Image-Processing/08_motion/";
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-274394082');
      </script>
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Digital Image Processing Notes
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../01_getting_started/">1. Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../02_block_decomposition/">2. Block Decomposition</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../03_point_processing/">3. Point Processing Transformations</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../04_morphology/">4. Morphology</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../05_filtering/">5. Filtering</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../06_feature_extraction/">6. Feature Extraction</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../07_segmentation/">7. Segmentation</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">8. Motion Estimation</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#1-horn-schunck-optical-flow">1. Horn-Schunck Optical Flow</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#problem-formulation">Problem Formulation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#minimization-process">Minimization Process</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#2-multi-scale-optical-flow">2. Multi-Scale Optical Flow</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#leveraging-multi-scale-approaches-for-robust-optical-flow">Leveraging Multi-Scale Approaches for Robust Optical Flow</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example_1">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#3-lucas-kanade-optical-flow">3. Lucas-Kanade Optical Flow</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#problem-formulation_1">Problem Formulation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#minimization-process_1">Minimization Process</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#examples">Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#4-lucas-kanade-optical-flow-with-eigenelement-analysis">4. Lucas-Kanade Optical Flow with Eigenelement Analysis</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#why-eigenvalues">Why Eigenvalues?</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#examples_1">Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#5-object-tracking-by-cross-correlation">5. Object Tracking by Cross-Correlation</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#algorithm-overview">Algorithm Overview</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#examples_2">Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#6-object-tracking-by-phase-correlation">6. Object Tracking by Phase Correlation</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#theoretical-background">Theoretical Background</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#algorithm-overview_1">Algorithm Overview</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#examples_3">Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#7-improve-object-tracking-with-linear-kalman-filter">7. Improve Object Tracking with Linear Kalman Filter</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#algorithm-overview_2">Algorithm Overview</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#notations">Notations</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kalman-filter-equations">Kalman Filter Equations</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#prediction">Prediction</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#updatecorrection">Update/Correction</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#examples_4">Examples</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../09_multispectral/">9. Multispectral Image Processing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../10_3d/">10. 3D Visualization</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../appendix_1/">Appendix 1 - Math Expressions in CImg's Fill Method</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../appendix_2/">Appendix 2 - JPEG Compression</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../where_did_i_get_my_images/">Where Did I Get My Images?</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Digital Image Processing Notes</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
      <li>8. Motion Estimation</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/tonyfu97/Digital-Image-Processing/blob/master/docs/08_motion.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="motion-estimation-learning-reflection">Motion Estimation - Learning Reflection</h1>
<p><strong>Author</strong>: Tony Fu<br />
<strong>Date</strong>: August 27, 2023<br />
<strong>Device</strong>: MacBook Pro 16-inch, Late 2021 (M1 Pro)<br />
<strong>Code</strong>: <a href="https://github.com/tonyfu97/Digital-Image-Processing/tree/main/08_motion">GitHub</a><br />
<strong>Reference</strong>: Chapter 8 <a href="https://www.amazon.com/Digital-Image-Processing-Implementing-Algorithms/dp/1032347538"><em>Digital Image Processing with C++: Implementing Reference Algorithms with the CImg Library</em> by Tschumperl√©, Tilmant, Barra</a></p>
<p>I also recommend Professor Shree Nayar's <a href="https://youtube.com/playlist?list=PL2zRqk16wsdoYzrWStffqBAoUY8XdvatV&amp;si=mds5gPNjt4l43Pmw">lecture series</a> on the on Optical Flow.</p>
<h2 id="1-horn-schunck-optical-flow">1. Horn-Schunck Optical Flow</h2>
<h3 id="problem-formulation">Problem Formulation</h3>
<p>The Horn-Schunck method frames optical flow as an energy minimization problem by defining an energy function <script type="math/tex">E</script> that encapsulates two main terms:</p>
<ol>
<li>
<p><strong>Data Term</strong>: This measures how well the flow <script type="math/tex"> (u,v) </script> is consistent with the pixel intensities in the given images. It's based on the brightness constancy constraint: <script type="math/tex"> I(x,y,t) = I(x+u, y+v, t+1) </script>, which states that the intensity of a point in an image should remain constant over time.</p>
<p>
<script type="math/tex; mode=display">
E_{\text{data}} = \int \int (I_x u + I_y v + I_t)^2 \, dx \, dy
</script>
</p>
<p>The book also shows a variant of the data term (called the "direct method") that minize the squared difference between the shifted image and the original image.</p>
</li>
<li>
<p><strong>Smoothness Term</strong>: To encourage smoothness in the flow field, Horn and Schunck include a regularization term. This term imposes a penalty on abrupt changes in <script type="math/tex">u</script> and <script type="math/tex">v</script>.</p>
<p>
<script type="math/tex; mode=display">
E_{\text{smooth}} = \int \int (\nabla u)^2 + (\nabla v)^2 \, dx \, dy
</script>
</p>
</li>
</ol>
<p>The combined energy function <script type="math/tex"> E </script> to be minimized is:</p>
<p>
<script type="math/tex; mode=display">
E = E_{\text{data}} + \alpha E_{\text{smooth}} =
\int \int \left[ (I_x u + I_y v + I_t)^2 + \alpha ((\nabla u)^2 + (\nabla v)^2) \right] \, dx \, dy
</script>
</p>
<h3 id="minimization-process">Minimization Process</h3>
<p>To find the flow fields <script type="math/tex"> u </script> and <script type="math/tex"> v </script> that minimize this energy function, Horn and Schunck uses the Euler-Lagrange equations derived from <script type="math/tex"> E </script>.</p>
<ol>
<li>
<p>Take the first variation of <script type="math/tex"> E </script> with respect to <script type="math/tex"> u </script> and <script type="math/tex"> v </script> and set them to zero.</p>
<p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u} = 0 \quad \text{and} \quad \frac{\partial E}{\partial v} = 0
</script>
</p>
</li>
<li>
<p>This results in a set of PDEs that are solved iteratively:</p>
<p>
<script type="math/tex; mode=display">
I_x(I_x u + I_y v + I_t) + \alpha \Delta u = 0
</script>
</p>
<p>
<script type="math/tex; mode=display">
I_y(I_x u + I_y v + I_t) + \alpha \Delta v = 0
</script>
</p>
<p>In the original paper, Horn and Schunck approximated the Laplacians with:</p>
<p>
<script type="math/tex; mode=display">
\nabla u = 4 (\bar{u} - u) \quad \text{and} \quad \nabla v = 4 (\bar{v} - v)
</script>
</p>
<p>where <script type="math/tex"> \bar{u} </script> and <script type="math/tex"> \bar{v} </script> are the averages of <script type="math/tex"> u </script> and <script type="math/tex"> v </script> in the 3-neighborhood of the current pixel.</p>
</li>
<li>
<p>The PDEs are solved iteratively until convergence (in the code, I set the maximum number of iterations to 100). The iterative update equations are:</p>
<p>
<script type="math/tex; mode=display">
u = \bar{u} - \frac{I_x(I_x \bar{u} + I_y \bar{v} + I_t)}{4\alpha + I_x^2 + I_y^2}
\quad \text{and} \quad
v = \bar{v} - \frac{I_y(I_x \bar{u} + I_y \bar{v} + I_t)}{4\alpha + I_x^2 + I_y^2}
</script>
</p>
</li>
</ol>
<h3 id="example">Example</h3>
<p>I use two frames from the following GIF as input to the Horn-Schunck optical flow algorithm:</p>
<p><img alt="driveby_gif" src="../images/driveby.gif" /></p>
<p>The result optical flow is shown below:</p>
<p><img alt="horn_schunck" src="../results/08/horn_schunck.png" /></p>
<h2 id="2-multi-scale-optical-flow">2. Multi-Scale Optical Flow</h2>
<h3 id="leveraging-multi-scale-approaches-for-robust-optical-flow">Leveraging Multi-Scale Approaches for Robust Optical Flow</h3>
<p>Optical flow estimation involves capturing pixel-level movement between consecutive images. However, real-world scenarios often include varied and complex motions which may not be accurately captured at just a single scale. This is where our multi-scale approach comes in.</p>
<p>An example implementation of the algorithm scales down the image iteratively by factors of 2, beginning with the coarsest scale and moving towards the finest. At each scale, the Horn-Schunck algorithm is applied to estimate optical flow. The algorithm first captures larger motion patterns at these coarser scales and then refines these estimates as it proceeds to finer scales.</p>
<h3 id="example_1">Example</h3>
<p><img alt="horn_schunck_multiscale" src="../results/08/horn_schunck_multiscale.png" /></p>
<p>Notice that most arrows, which represent the optical flow, are concentrated on the moving car. However, you might also see that the arrow directions are not entirely accurate.</p>
<h2 id="3-lucas-kanade-optical-flow">3. Lucas-Kanade Optical Flow</h2>
<p>In the Horn-Schunck method, the energy function <script type="math/tex">E</script> is minimized globally over the entire image. This makes the problem severely under-determined, as there are two unknowns <script type="math/tex">u</script> and <script type="math/tex">v</script> for each pixel. To overcome this limitation, Lucas and Kanade proposed a local method that minimizes the energy function within a local window <script type="math/tex">W</script> around each pixel.</p>
<h3 id="problem-formulation_1">Problem Formulation</h3>
<p>Before introducing the window <script type="math/tex">W</script>, let's first take a closer look at the <strong>data term</strong> used in the Horn-Schunck method. It is based on the brightness constancy constraint: <script type="math/tex"> I(x,y,t) = I(x+u, y+v, t+1) </script>, which states that the intensity of a point in an image should remain constant over time. Here, we take some steps to derive the optical flow equation. This derivation is also covered by Professor Shree Nayar in his <a href="https://youtu.be/IjPLZ3hjU1A?si=vFxhDuyotewubWS6">lecture</a>.</p>
<ol>
<li>
<p><strong>First-Order Taylor Series Expansion</strong>: We approximate the right-hand side of the equation around <script type="math/tex">(x, y, t)</script>:</p>
<p>
<script type="math/tex; mode=display">
I(x+dx, y+dy, t+dt) \approx I(x, y, t) + \frac{\partial I}{\partial x} dx + \frac{\partial I}{\partial y} dy + \frac{\partial I}{\partial t} dt
</script>
</p>
</li>
<li>
<p><strong>Combining Equations</strong>: Using the initial assumption <script type="math/tex">I(x, y, t) = I(x+dx, y+dy, t+dt)</script>, we get:</p>
<p>
<script type="math/tex; mode=display">
I(x, y, t) + I_x dx + I_y dy + I_t dt = I(x, y, t)
</script>
</p>
</li>
<li>
<p><strong>Simplification</strong>: We subtract <script type="math/tex">I(x, y, t)</script> from both sides:</p>
<p>
<script type="math/tex; mode=display">
I_x dx + I_y dy + I_t dt = 0
</script>
</p>
</li>
<li>
<p><strong>Optical Flow Variables</strong>: Finally, by dividing by <script type="math/tex">dt</script> and introducing <script type="math/tex">u = dx/dt</script> and <script type="math/tex">v = dy/dt</script>, we arrive at the familiar optical flow equation:</p>
<p>
<script type="math/tex; mode=display">
I_x u + I_y v + I_t = 0
</script>
</p>
</li>
</ol>
<h3 id="minimization-process_1">Minimization Process</h3>
<p>The brightness constancy assumption, upon which the optical flow equation <script type="math/tex">I_x u + I_y v + I_t = 0</script> is based, may not hold true in all real-world scenarios. Therefore, it is unlikely that the equation will be exactly zero. Instead, we minimize the squared error term <script type="math/tex">E(u, v)</script> to find the best possible solution.</p>
<p>As alluded to above, the equation provides only a single constraint for the two unknowns <script type="math/tex">u</script> and <script type="math/tex">v</script>, making the problem under-determined. Lucas-Kanade overcomes this limitation by applying this constraint within a local window <script type="math/tex">W</script> of pixels. We sum up the squared differences in brightness to form the error term <script type="math/tex">E(u, v)</script>, making the problem mathematically tractable by providing enough equations to solve for the unknowns. This also improves the robustness of the optical flow estimate by averaging out inconsistencies and noise in the local region.</p>
<p>
<script type="math/tex; mode=display">
E(u, v) = \sum_{W} (I_x u + I_y v + I_t)^2
</script>
</p>
<p>We can further expand the equation to:</p>
<p>
<script type="math/tex; mode=display">
E(u, v) = \sum_{W} I_x^2 u^2 + 2 I_x I_y u v + I_y^2 v^2 + 2 I_x I_t u + 2 I_y I_t v + I_t^2
</script>
</p>
<p>To minimize <script type="math/tex">E(u, v)</script>, we set its partial derivatives with respect to <script type="math/tex">u</script> and <script type="math/tex">v</script> to zero:</p>
<p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u} = 2 \sum_{W} (I_x^2 u + I_x I_y v + I_x I_t) = 0
</script>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial v} = 2 \sum_{W} (I_x I_y u + I_y^2 v + I_y I_t) = 0
</script>
</p>
<p>Rewriting these equations in matrix form, we get:</p>
<p>
<script type="math/tex; mode=display">
A \mathbf{V} = \mathbf{b}
</script>
</p>
<p>Where:</p>
<p>
<script type="math/tex; mode=display">
A = \begin{bmatrix}
\sum_{W} I_x^2 & \sum_{W} I_x I_y \\
\sum_{W} I_x I_y & \sum_{W} I_y^2
\end{bmatrix}
</script>
</p>
<p>
<script type="math/tex; mode=display">
\mathbf{V} = \begin{bmatrix}
u \\
v
\end{bmatrix}
</script>
</p>
<p>
<script type="math/tex; mode=display">
\mathbf{b} = \begin{bmatrix}
-\sum_{W} I_x I_t \\
-\sum_{W} I_y I_t
\end{bmatrix}
</script>
</p>
<p>To find the velocities <script type="math/tex">u</script> and <script type="math/tex">v</script>, we typically solve the equation <script type="math/tex">A \mathbf{V} = \mathbf{b}</script> by computing <script type="math/tex">\mathbf{V} = A^{-1} \mathbf{b}</script>. But wait, <script type="math/tex">A</script> is most likely non-square (and therefore non-invertible), so we can't compute its inverse. In such cases, we aim to find <script type="math/tex">\mathbf{V}</script> that minimizes the residual <script type="math/tex">||A \mathbf{V} - \mathbf{b}||^2</script>. This <em>least squares</em> technique is commonly used in optimization problems:</p>
<p>
<script type="math/tex; mode=display">
A^T A \mathbf{V} = A^T \mathbf{b}
</script>
</p>
<p>Here, <script type="math/tex">A^T</script> is the transpose of <script type="math/tex">A</script>, and <script type="math/tex">A^T A</script> becomes a square matrix, making it possible to find an exact solution. The resulting <script type="math/tex">\mathbf{V}</script> is the least squares solution to the original equation.</p>
<h3 id="examples">Examples</h3>
<p>Applying the Lucas-Kanade optical flow algorithm to the same two frames from the previous section, we get the following result:</p>
<p><img alt="lucas_kanade" src="../results/08/lucas_kanade.png" /></p>
<p>Here is another example:</p>
<p><img alt="shuffleboard" src="../images/shuffleboard.gif" /></p>
<p>And the result:</p>
<p><img alt="lucas_kanade" src="../results/08/lucas_kanade_shuffleboard.png" /></p>
<h2 id="4-lucas-kanade-optical-flow-with-eigenelement-analysis">4. Lucas-Kanade Optical Flow with Eigenelement Analysis</h2>
<h3 id="why-eigenvalues">Why Eigenvalues?</h3>
<p>Recall that in the <a href="../06_feature_extraction/#1-harris-stephens-corner-detector">Harris-Stephen corner detection</a> section, we introduced the concept of a structure tensor, denoted <script type="math/tex"> M </script>, to capture local image structures. Similarly, in the Lucas-Kanade method, we utilize another structure tensor <script type="math/tex">A^T A</script>. This <script type="math/tex">2 \times 2</script> matrix serves to encapsulate the local texture around a pixel.</p>
<p>The eigenvalues of this structure tensor guide us in determining how to process each pixel. The following table outlines the actions based on different conditions:</p>
<table>
<thead>
<tr>
<th>Condition</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td>Both eigenvalues <script type="math/tex"> \geq \tau_D </script>
</td>
<td>Compute the velocity using <script type="math/tex"> \text{localVelocity} = -A^T A \times A^T b </script>.</td>
</tr>
<tr>
<td>One eigenvalue <script type="math/tex"> \geq \tau_D </script>
</td>
<td>Project the estimated velocity onto the direction corresponding to the large eigenvalue.</td>
</tr>
<tr>
<td>Both eigenvalues <script type="math/tex"> < \tau_D </script>
</td>
<td>Ignore the pixel, as it is too smooth or noisy to provide reliable motion information.</td>
</tr>
</tbody>
</table>
<p>For a deeper dive into eigenvalue analysis, you can refer to Professor Shree Nayar's <a href="https://youtu.be/6wMoHgpVUn8?si=J_qQ42REAsAvxOdh&amp;t=243">lecture</a>.</p>
<h3 id="examples_1">Examples</h3>
<p>To demonstrate the benefits of incorporating eigenvalue analysis, let's apply the Lucas-Kanade optical flow algorithm to the same sets of frames we used in previous sections. Below are the results:</p>
<p><img alt="lucas_kanade_eigen_driveby" src="../results/08/lucas_kanade_eigen_driveby.png" /></p>
<p><img alt="lucas_kanade_eigen_shuffleboard" src="../results/08/lucas_kanade_eigen_shuffleboard.png" /></p>
<p>I find the output to be cleaner compared to earlier methods.</p>
<h2 id="5-object-tracking-by-cross-correlation">5. Object Tracking by Cross-Correlation</h2>
<h3 id="algorithm-overview">Algorithm Overview</h3>
<p>Object tracking in a sequence of images can be accomplished by first identifying the object in the initial frame and creating a "template" of it. In subsequent frames, we cross-correlate this template with a local neighborhood of pixels to find the location that yields the highest correlation. This newly identified location represents the object's new position. This method is considered a form of sparse motion estimation because it doesn't involve comparing the entire image, but rather a small localized area.</p>
<h3 id="examples_2">Examples</h3>
<p>In the following image, we first identify the object in motion and enclose it within a box to create a template:</p>
<p><img alt="cross_correlation_driveby_input" src="../results/08/cross_correlation_driveby_input.png" /></p>
<p>We then apply the cross-correlation algorithm to subsequent frames to track the object:</p>
<p><img alt="cross_correlation_driveby_output" src="../results/08/cross_correlation_driveby_output.png" /></p>
<p>Here's another example:</p>
<p><img alt="cross_correlation_shuffleboard_input" src="../results/08/cross_correlation_shuffleboard_input.png" /></p>
<p>And the tracking result:</p>
<p><img alt="cross_correlation_shuffleboard_output" src="../results/08/cross_correlation_shuffleboard_output.png" /></p>
<h2 id="6-object-tracking-by-phase-correlation">6. Object Tracking by Phase Correlation</h2>
<h3 id="theoretical-background">Theoretical Background</h3>
<p>Assume that two images differ only by a translation:</p>
<p>
<script type="math/tex; mode=display">
I_2(x,y) = I_1(x + \Delta x, y + \Delta y)
</script>
</p>
<p>This can be viewed as the convolution of the first image with a delta function:</p>
<p>
<script type="math/tex; mode=display">
I_2(x,y) = I_1(x,y) \ast \delta(x + \Delta x, y + \Delta y)
</script>
</p>
<p>Applying the Fourier transform to both sides gives:</p>
<p>
<script type="math/tex; mode=display">
F_2(f_x, f_y) = F_1(f_x, f_y) \times e^{-j2\pi(f_x \Delta x + f_y \Delta y)}
</script>
</p>
<p>This equation will be crucial for deriving <script type="math/tex">\Delta x</script> and <script type="math/tex">\Delta y</script>, the translational offsets between the images. We will later see how to go from <script type="math/tex">F_2(f_x, f_y)</script> and <script type="math/tex">F_1(f_x, f_y)</script> to <script type="math/tex">e^{-j2\pi(f_x \Delta x + f_y \Delta y)}</script>.</p>
<h3 id="algorithm-overview_1">Algorithm Overview</h3>
<p>The phase correlation approach is similar to cross-correlation. It involves creating an object template in the initial frame and then locating this object in subsequent frames. Instead of using the cross-correlation function, the method employs frequency domain transformations:</p>
<p>
<script type="math/tex; mode=display">
F_1(f_x, f_y) = \mathcal{F}(I_1(x,y)) \quad \text{and} \quad F_2(f_x, f_y) = \mathcal{F}(I_2(x,y))
</script>
</p>
<p>The cross-power spectrum (<script type="math/tex">CPS</script> is formulated as:</p>
<p>
<script type="math/tex; mode=display">
CPS
=
\frac{F_1 \overline{F_2}}{|F_1 \overline{F_2}|}
=
\frac{F_1 \overline{F_1 \times  e^{-j2\pi(f_x \Delta x + f_y \Delta y)}}}{|F_1 \overline{F_1 \times  e^{-j2\pi(f_x \Delta x + f_y \Delta y)}}|}
=\ldots =
e^{-j2\pi(f_x \Delta x + f_y \Delta y)}
</script>
</p>
<p>The proof (see the book) show that this eventually simplifies to <script type="math/tex">e^{-j2\pi(f_x \Delta x + f_y \Delta y)}</script>, which can be inverse Fourier transformed to get <script type="math/tex">\delta(x + \Delta x, y + \Delta y)</script>. In the code, the real and imaginary parts of the cross-power spectrum are separately handled as follows:</p>
<p>
<script type="math/tex; mode=display">
CPS_{real} = F_{1real} F_{2real} + F_{1imag} F_{2imag}\\
CPS_{imag} = F_{1imag} F_{2real} - F_{1real} F_{2imag}
</script>
</p>
<p>
<script type="math/tex; mode=display">
CPS = \frac{CPS_{real} + jCPS_{imag}}{|CPS_{real} + jCPS_{imag}| + \epsilon}
</script>
</p>
<p>The <script type="math/tex">\epsilon</script> term is added to avoid division by zero.</p>
<h3 id="examples_3">Examples</h3>
<p>We apply the phase correlation algorithm to the same frame sets used previously. The results demonstrate that it effectively tracks moving objects. </p>
<p><img alt="driveby_subplots" src="../results/08/phase_correlation_driveby_subplots.png" /></p>
<p>However, the method is less reliable when the object lacks a distinct texture. For example, phase correlation fails in the following example:</p>
<p><img alt="shuffleboard_subplots" src="../results/08/phase_correlation_shuffleboard_subplots.png" /></p>
<h2 id="7-improve-object-tracking-with-linear-kalman-filter">7. Improve Object Tracking with Linear Kalman Filter</h2>
<h3 id="algorithm-overview_2">Algorithm Overview</h3>
<p>The idea of using phase correlation over spatial correlation comes from the fact that phase captures translation information across the entire region of interest. So why not extend this temporally? Instead of relying solely on the current frame to track an object, we could use previous frames to predict the object's location in the current one. This is the underlying concept of the Kalman filter.</p>
<p>The Kalman filter is a recursive algorithm designed to estimate the state of a system using a series of noisy measurements. It's a staple in control systems where the objective is to predict the system's future state based on past measurements. In our case, the system is the moving object, and the measurements are the object's positions in each frame.</p>
<p>Before diving deeper, let's first familiarize ourselves with the notations used in the Kalman filter:</p>
<h3 id="notations">Notations</h3>
<ul>
<li>
<p><strong>State <script type="math/tex">\mathbf{s_i}</script></strong>:
    <script type="math/tex; mode=display">
    \mathbf{s_i} = \begin{bmatrix} x_i, y_i, u_i, v_i \end{bmatrix}^T
    </script>
    Here, <script type="math/tex">x_i</script> and <script type="math/tex">y_i</script> represent the object's coordinates in the <script type="math/tex">i</script>-th frame, and <script type="math/tex">u_i</script> and <script type="math/tex">v_i</script> signify its velocities.</p>
</li>
<li>
<p><strong>Estimated Position <script type="math/tex">\mathbf{z}_i</script></strong>: This is calculated using phase correlation and represents the object's estimated location in the <script type="math/tex">i</script>-th frame.</p>
</li>
<li>
<p><strong>State Transition Model <script type="math/tex">\mathbf{D}_i</script></strong>: This matrix describes how the state evolves over time. In our implementation, it's a constant matrix <script type="math/tex"> \mathbf{D} </script>:
    <script type="math/tex; mode=display">
    \mathbf{D} = \begin{bmatrix}
    1 & 0 & 1 & 0 \\
    0 & 1 & 0 & 1 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
    \end{bmatrix}
    </script>
</p>
</li>
<li>
<p><strong>Observation Model <script type="math/tex">\mathbf{M}_i</script></strong>: It translates the state into position. Our observation model is simply the constant matrix <script type="math/tex"> \mathbf{M} </script>, meaning that it simply extracts <script type="math/tex">x_i</script> and <script type="math/tex">y_i</script> from <script type="math/tex">\mathbf{s}_i</script>:
    <script type="math/tex; mode=display">
    \mathbf{M} = \begin{bmatrix}
    1 & 0 & 0 & 0\\
    0 & 1 & 0 & 0
    \end{bmatrix}
    </script>
</p>
</li>
<li>
<p><strong>Model Covariance <script type="math/tex">\Sigma_{d, i}</script></strong> and <strong>Measurement Covariance <script type="math/tex">\Sigma_{m, i}</script></strong>: These signify the uncertainty in the model and the measurements, respectively.</p>
<p>
<script type="math/tex; mode=display">
\Sigma_D = \begin{pmatrix}
5 & 0 & 0 & 0 \\
0 & 5 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}
</script>
<script type="math/tex; mode=display">
\Sigma_M = \begin{pmatrix}
50 & 0 \\
0 & 50
\end{pmatrix}
</script>
</p>
<p>The values are set to represent our belief about the model and measurement noise. Essentially, larger values indicate greater uncertainty. They serve to balance the model prediction against new observations. </p>
</li>
<li>
<p><strong>State Covariance <script type="math/tex">\Sigma_i</script></strong>: This measures the uncertainty in our state estimate.</p>
</li>
<li>
<p><strong>Kalman Gain <script type="math/tex">\mathbf{K}_i</script></strong>: This weighting factor decides how much we should trust the new measurements relative to our model prediction.</p>
</li>
</ul>
<h3 id="kalman-filter-equations">Kalman Filter Equations</h3>
<h4 id="prediction">Prediction</h4>
<p>Here we predict the state and its uncertainty.
<script type="math/tex; mode=display">
\mathbf{s}_i = \mathbf{D} \mathbf{s}_{i-1} \\
\Sigma_i = \Sigma_{d, i} + \mathbf{D} \Sigma_{i-1} \mathbf{D}^T
</script>
</p>
<h4 id="updatecorrection">Update/Correction</h4>
<p>In this phase, we update the predicted state based on new measurements.
<script type="math/tex; mode=display">
\mathbf{K}_i = \Sigma_i \mathbf{M}^T (\mathbf{M} \Sigma_i \mathbf{M}^T + \Sigma_{m, i})^{-1} \\
\mathbf{s}_i = \mathbf{s}_i + \mathbf{K}_i (\mathbf{z}_i - \mathbf{M} \mathbf{s}_i) \\
\Sigma_i = (I - \mathbf{K}_i \mathbf{M}) \Sigma_i
</script>
</p>
<p>Notice that the larger the <script type="math/tex">\Sigma_{m, i}</script>, the smaller the Kalman gain <script type="math/tex">\mathbf{K}_i</script>. This means that we trust the model more than the measurements. On the other hand, if the measurement noise is small, the Kalman gain will be large, and we will trust the measurements more than the model.</p>
<h3 id="examples_4">Examples</h3>
<p>When the Kalman filter is combined with phase correlation, tracking may initially seem sluggish but eventually catches up. This adaptation period is common as the Kalman filter attempts to balance noisy measurements from the phase correlation with its own predictions.</p>
<p><img alt="kalman_driveby_subplots" src="../results/08/kalman_driveby_subplots.png" /></p>
<p>As expected, when phase correlation fails in the shuffleboard example, the Kalman filter also fails to track the object.</p>
<p><img alt="kalman_shuffleboard_subplots" src="../results/08/kalman_shuffleboard_subplots.png" /></p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../07_segmentation/" class="btn btn-neutral float-left" title="7. Segmentation"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../09_multispectral/" class="btn btn-neutral float-right" title="9. Multispectral Image Processing">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
      <p>Copyright &copy; 2023 Tony Fu</p>
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/tonyfu97/Digital-Image-Processing" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../07_segmentation/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../09_multispectral/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
